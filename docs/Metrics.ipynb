{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c0a8f3",
   "metadata": {},
   "source": [
    "## Reference free evaluation\n",
    "\n",
    "In a Retrival augmented generation system it can be time consuming to create ground truth for every set of sample to be analyzed and evaluated. Moreover intelligent insights about the performance of RAG pipeline can be derived with using questions, retrived contexts and generated answer. **Reference free evaluation metrics are the set of metrics can be used without the need of a annotated ground truth**.These metrics can be used against any set of random sample drawn out of a RAG pipeline. \n",
    "\n",
    "\n",
    "\n",
    "### Faithfulness \n",
    "\n",
    "\n",
    "This measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. The answer is scaled to (0,1) range. Higher the better.\n",
    "\n",
    "The generated answer is regarded as faithful if all the claims that are made in the answer can be inferred from the given context. To calculate this a set of claims from the generated answer is first identified. Then each one of these claims are cross checked with given context to determine if it can be inferred from given context or not. The faithfulness score is given by  divided by \n",
    "\n",
    "$$\n",
    "\\text{Faithfulness score} = {|\\text{Number of claims that can be inferred from given context}| \\over |\\text{Total number of claims in the generated answer}|}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Example usage\n",
    "\n",
    "```python\n",
    "\n",
    "from ragas.metrics.faithfulness import Faithfulness\n",
    "faithfulness = Faithfulness()\n",
    "\n",
    "# Dataset({\n",
    "#     features: ['question','contexts','answer'],\n",
    "#     num_rows: 25\n",
    "# })\n",
    "dataset: Dataset\n",
    "\n",
    "results = faithfulness.score(dataset)\n",
    "```\n",
    "\n",
    "#### Example\n",
    "\n",
    "**High faithfulness**\n",
    "\n",
    "**Low faithfulness**\n",
    "\n",
    "\n",
    "\n",
    "### Answer Relevancy \n",
    "\n",
    "This measures how relevant is the generated answer to the prompt. If the generated answer is incomplete or contains redundant information the score will be low. It is calculated from question and answer. Values range (0,1), higher the better.\n",
    "\n",
    "The generated answer is considered as relevant if it directly addresses the question in an appropriate way. In particular, our assessment of answer relevance does not take into account factuality, but penalises cases where the answer is incomplete or where it contains redundant information. It is calculated by first prompting the LLM to generate appropriate question for the generated answer n times and then measuring the mean cosine similarity of the generated questions with the actual question. The key idea here is that if the generated answer adresses the original question the LLM should be able to generate question from the generated answer correctly.\n",
    "\n",
    "### Example usage\n",
    "\n",
    "```python\n",
    "\n",
    "from ragas.metrics import AnswerRelevancy\n",
    "answer_relevancy = AnswerRelevancy()\n",
    "\n",
    "# init_model to load models used\n",
    "answer_relevancy.init_model()\n",
    "\n",
    "# Dataset({\n",
    "#     features: ['question','answer'],\n",
    "#     num_rows: 25\n",
    "# })\n",
    "dataset: Dataset\n",
    "\n",
    "results = answer_relevancy.score(dataset)\n",
    "\n",
    "```\n",
    "\n",
    "#### Example\n",
    "\n",
    "**High answer relevancy**\n",
    "\n",
    "**Low answer relevancy**\n",
    "\n",
    "\n",
    "\n",
    "### Context Precision\n",
    "\n",
    "Measures the precision of retrived context. It is calculated from `question` and `contexts`. Values range (0,1), higher the better.\n",
    "\n",
    "The retrieved context should ideally only contain neccessary information to answer the given query. To calculate this, first we estimate $|S|$ by identifying sentences from the retrived context that can be used to answer the given question. The final score is given by\n",
    "\n",
    "$$\n",
    "\\text{context precision} = {|S| \\over |\\text{Total number of sentences in retrived context}|}\n",
    "$$\n",
    "\n",
    "### Example usage\n",
    "\n",
    "```python\n",
    "\n",
    "from ragas.metrics import ContextPrecision\n",
    "context_precision = ContextPrecision(strictness=3)\n",
    "\n",
    "# run init models to load the models used\n",
    "context_precision.init_model()\n",
    "\n",
    "# Dataset({\n",
    "#     features: ['question','contexts'],\n",
    "#     num_rows: 25\n",
    "# })\n",
    "dataset: Dataset\n",
    "\n",
    "results = context_precision.score(dataset)\n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "\n",
    "## Aspect critique \n",
    "\n",
    "Designed to judge the submission against defined aspects like harmlessness, correctness, etc. You can also define your own aspect and validate the submission against your desired aspect. The output of aspect critiques is always binary. It is calculated from `answer`.\n",
    "\n",
    "Critiques are LLM evaluators that evaluate the your submission using the provided aspect. There are several aspects like correctness, harmfulness,etc (Check SUPPORTED_ASPECTS to see full list) that comes predefined with Ragas Critiques. If you wish to define your own aspect you can also do this. The strictness parameter is used to ensure a level of self consistency in prediction (ideal range 2-4). The output of aspect critiques is always binary indicating whether the submission adhered to the given aspect definition or not. It is calculated from answer. These scores will not be considered for the final ragas_score due to it's non-continuous nature.\n",
    "\n",
    "```python\n",
    "\n",
    "## check predefined aspects\n",
    "from ragas.metrics.critique import SUPPORTED_ASPECTS\n",
    "print(SUPPORTED_ASPECTS)\n",
    "\n",
    "from ragas.metrics.critique import conciseness\n",
    "# Dataset({\n",
    "#     features: ['question','answer'],\n",
    "#     num_rows: 25\n",
    "# })\n",
    "dataset: Dataset\n",
    "\n",
    "results = conciseness.score(dataset)\n",
    "\n",
    "\n",
    "## Define your critique\n",
    "from ragas.metrics.critique import AspectCritique\n",
    "mycritique = AspectCritique(name=\"my-critique\", definition=\"Is the submission safe to children?\", strictness=2)\n",
    "\n",
    "results = mycritique.score(dataset)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Grouth truth\n",
    "\n",
    "Some performance of some components in a RAG system can only be fully determined using a evaluation dataset that contains annotated ground truth. TODO: Give reference to testset generation\n",
    "\n",
    "### Context Recall\n",
    "\n",
    "measures the recall of the retrieved context using annotated answer as ground truth. It is calculated from `ground truth` and `retrieved context`. Values range from (0,1), higher the better. \n",
    "\n",
    "To estimate context recall from ground truth answer each sentence from ground truth answer is analyzes to verify if can be attributed to retrived context or not. In ideal scenario, all the sentences in ground truth answer can be attributed to retrived context. \n",
    "\n",
    "$$\n",
    "\\text{context recall} = {|\\text{GT sentences that can be attributed to context}| \\over |\\text{Number of sentences in GT}|}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Example usage\n",
    "\n",
    "```python\n",
    "from ragas.metrics import ContextRecall\n",
    "context_recall = ContextRecall()\n",
    "# Dataset({\n",
    "#     features: ['contexts','ground_truths'],\n",
    "#     num_rows: 25\n",
    "# })\n",
    "dataset: Dataset\n",
    "\n",
    "results = context_recall.score(dataset)\n",
    "\n",
    "```\n",
    "\n",
    "### Answer semantic similarity\n",
    "\n",
    "measures the semantic similarity of generated answer against ground truth. It is calculated from `ground truth` and ` answer`. Values range from (0,1), higher the better. \n",
    "\n",
    "Semantic similarity between answers can provide insights about the quality of generated answer. It is calculated using cross-encoder model. \n",
    "\n",
    "#### Example usage\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "### Answer correctness\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Test set generation \n",
    "TODO: Discuss the idea of continual testing\n",
    "\n",
    "\n",
    "### Pyest\n",
    "\n",
    "## Ragas LLM\n",
    "\n",
    "\n",
    "## Integrations\n",
    "\n",
    "### Langchain\n",
    "\n",
    "### Llama-index\n",
    "\n",
    "### Langsmith\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f42b887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This context is too brief and doesn't delve into or explain any concepts. It's a simple statement without any supporting information or explanation. Therefore, it gets a low score. Score: 2.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83ceb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = text.find(\"Score:\") + len(\"Score:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b684eabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval('2. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89a347b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "ragas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
