{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0174eb96",
   "metadata": {},
   "source": [
    "# Using different Embedding Models\n",
    "\n",
    "Ragas allows users to change the default embedding model used in the evaluation task.\n",
    "\n",
    "This guide will show you how to use different embedding models for evaluation in Ragas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0f9b9",
   "metadata": {},
   "source": [
    "## Evaluating with Open AI Embeddings\n",
    "\n",
    "Ragas uses BAAI/bge-small-en-v1.5 by default. In this example we can use Open AI Embeddings from langchain which uses text-embedding-ada-002. We will be using gpt4 as the llm for evaluation and `AnswerSimilarity` as the metric\n",
    "\n",
    "To start-off, we initialise the gpt4 `chat_model` from langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6d96660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have you OpenAI API key ready\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6906a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from ragas.llms import LangchainLLM\n",
    "\n",
    "gpt4 = ChatOpenAI(model_name=\"gpt-4\")\n",
    "gpt4_wrapper = LangchainLLM(llm=gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fdb48b",
   "metadata": {},
   "source": [
    "In order to use the Open AI embedding, we have to instantiate an object of the `OpenAIEmbeddings` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c72521-3372-4663-81e4-c159b0edde40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.embeddings import OpenAIEmbeddings\n",
    "\n",
    "open_ai_embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62645da8-6a52-4cbb-bec7-59f7e153cd38",
   "metadata": {},
   "source": [
    "To use the open_ai_embeddings with the AnswerSimilarity metric, create an object by passing the open_ai_embedding and llm as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "307321ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import AnswerSimilarity\n",
    "\n",
    "answer_similarity = AnswerSimilarity(llm=gpt4_wrapper, embeddings=open_ai_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1930dd49",
   "metadata": {},
   "source": [
    "That's it! answer_similarity will now be using OpenAIEmbeddings under the hood for evaluations.\n",
    "\n",
    "Now lets run the evaluations using the example from [quickstart](../../getstarted/evaluation.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62c0eadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    baseline: Dataset({\n",
       "        features: ['question', 'ground_truths', 'answer', 'contexts'],\n",
       "        num_rows: 30\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data\n",
    "from datasets import load_dataset\n",
    "\n",
    "fiqa_eval = load_dataset(\"explodinggradients/fiqa\", \"ragas_eval\")\n",
    "fiqa_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4396f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_similarity]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer_similarity': 0.8877}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate\n",
    "from ragas import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    fiqa_eval[\"baseline\"].select(range(5)),  # showing only 5 for demonstration\n",
    "    metrics=[answer_similarity]\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f490031e-fb73-4170-8762-61cadb4031e6",
   "metadata": {},
   "source": [
    "## Evaluating with FastEmbed Embeddings\n",
    "\n",
    "`FastEmbed` is a Python library built for embedding generation and has support for popular text models. Ragas has integration with FastEmbed and can be used by instantiating an object of the FastEmbedEmbeddings class. More information regarding FastEmbed and supported models can be found [here](https://github.com/qdrant/fastembed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85e313f2-e45c-4551-ab20-4e526e098740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252M/252M [00:07<00:00, 32.2MiB/s] \n"
     ]
    }
   ],
   "source": [
    "from ragas.embeddings import FastEmbedEmbeddings\n",
    "\n",
    "fast_embeddings = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ddf74a-9830-4e1a-a4dd-7e5ec17a71e4",
   "metadata": {},
   "source": [
    "Now lets create the metric object for AnswerSimilarity by passing the llm and embedding as the `FastEmbedEmbeddings` object that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fd4adf3-db15-4c95-bf7c-407266517214",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_similarity2 = AnswerSimilarity(llm=gpt4_wrapper, embeddings=fast_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a610f2-19e5-40ec-bb7d-760c1d608a85",
   "metadata": {},
   "source": [
    "Now you can run the evaluations with and analyse the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20882d05-1b54-4d17-88a0-f7ada2d6a576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_similarity]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.32s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer_similarity': 0.8938}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = evaluate(\n",
    "    fiqa_eval[\"baseline\"].select(range(5)),  # showing only 5 for demonstration\n",
    "    metrics=[answer_similarity2],\n",
    ")\n",
    "\n",
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating with HuggingFace Embeddings\n",
    "\n",
    "Ragas has support for using embedding models using HuggingFace. Using the `HuggingfaceEmbeddings` class in Ragas, the embedding models supported by HuggingFace can directly be used for the evaluation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use embedding models from HuggingFace, you need to install the following\n",
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.embeddings import HuggingfaceEmbeddings\n",
    "\n",
    "hf_embeddings = HuggingfaceEmbeddings(model_name=\"BAAI/bge-small-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we follow the same steps as above to use the HuggingFace Embeddings in the ragas metrics and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_similarity3 = AnswerSimilarity(llm=gpt4_wrapper, embeddings=hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_similarity]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer_similarity': 0.9156}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result3 = evaluate(\n",
    "    fiqa_eval[\"baseline\"].select(range(5)),  # showing only 5 for demonstration\n",
    "    metrics=[answer_similarity3],\n",
    ")\n",
    "\n",
    "result3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
