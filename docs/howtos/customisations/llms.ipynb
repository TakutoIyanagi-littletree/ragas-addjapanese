{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0174eb96",
   "metadata": {},
   "source": [
    "# Bring your own LLMs\n",
    "\n",
    "Ragas uses langchain under the hood for connecting to LLMs for metrics that require them. This means you can swap out the default LLM we use (`gpt-3.5-turbo-16k`) with any 100s of API supported out of the box by langchain:\n",
    "\n",
    "- [Completion LLMs Supported](https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.llms)\n",
    "- [Chat based LLMs Supported](https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.chat_models)\n",
    "\n",
    "This guide will show you how to use another LLM API for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b57fcd-5f3f-4dc5-9ba1-c3b152c501cc",
   "metadata": {},
   "source": [
    ":::{Note}\n",
    "If your looking to use Azure OpenAI for evaluation checkout [this guide](./azure-openai.ipynb)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0f9b9",
   "metadata": {},
   "source": [
    "## Evaluating with GPT4\n",
    "\n",
    "Ragas uses gpt3.5 by default but using gpt4 for evaluation can improve the results so lets use that for the `Faithfulness` metric.\n",
    "\n",
    "To start-off, we initialise the gpt4 `chat_model` from langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6d96660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have you OpenAI API key ready\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6906a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "gpt4 = ChatOpenAI(model_name=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fdb48b",
   "metadata": {},
   "source": [
    "`RagasLLM` wrapper is required to use the LangchainLLM Wrapper. This helps the Ragas library specify the interfaces that will be used internally by the metrics and what is exposed via the Langchain library. \n",
    "\n",
    "You can also use other LLM APIs from tools like LlamaIndex or LiteLLM but that requires creating your own implementation of `RagasLLM` that supports it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25c72521-3372-4663-81e4-c159b0edde40",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AzureOpenAIEmbeddings' from 'langchain.embeddings' (/opt/anaconda3/envs/ragas-3.8/lib/python3.8/site-packages/langchain/embeddings/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LangchainLLMWrapper\n\u001b[1;32m      3\u001b[0m gpt4_wrapper \u001b[38;5;241m=\u001b[39m LangchainLLMWrapper(gpt4)\n",
      "File \u001b[0;32m~/belar/src/ragas/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version \u001b[38;5;28;01mas\u001b[39;00m __version__\n",
      "File \u001b[0;32m~/belar/src/ragas/evaluation.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, concatenate_datasets\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_analytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationEvent, track\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Metric\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcritique\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AspectCritique\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     remap_column_names,\n\u001b[1;32m     14\u001b[0m     validate_column_dtypes,\n\u001b[1;32m     15\u001b[0m     validate_evaluation_modes,\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/belar/src/ragas/metrics/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_answer_correctness\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnswerCorrectness, answer_correctness\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_answer_relevance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnswerRelevancy, answer_relevancy\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_answer_similarity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnswerSimilarity, answer_similarity\n",
      "File \u001b[0;32m~/belar/src/ragas/metrics/_answer_correctness.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_answer_similarity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnswerSimilarity\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_faithfulness\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Faithfulness\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationMode, MetricWithLLM\n",
      "File \u001b[0;32m~/belar/src/ragas/metrics/_answer_similarity.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     HuggingfaceEmbeddings,\n\u001b[1;32m     11\u001b[0m     OpenAIEmbeddings,\n\u001b[1;32m     12\u001b[0m     embedding_factory,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIKeyNotFound\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationMode, MetricWithLLM\n",
      "File \u001b[0;32m~/belar/src/ragas/embeddings/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     AzureOpenAIEmbeddings,\n\u001b[1;32m      3\u001b[0m     HuggingfaceEmbeddings,\n\u001b[1;32m      4\u001b[0m     OpenAIEmbeddings,\n\u001b[1;32m      5\u001b[0m     RagasEmbeddings,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingfaceEmbeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAIEmbeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzureOpenAIEmbeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRagasEmbeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m ]\n",
      "File \u001b[0;32m~/belar/src/ragas/embeddings/base.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIEmbeddings \u001b[38;5;28;01mas\u001b[39;00m BaseAzureOpenAIEmbeddings\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings \u001b[38;5;28;01mas\u001b[39;00m BaseOpenAIEmbeddings\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embeddings\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AzureOpenAIEmbeddings' from 'langchain.embeddings' (/opt/anaconda3/envs/ragas-3.8/lib/python3.8/site-packages/langchain/embeddings/__init__.py)"
     ]
    }
   ],
   "source": [
    "from ragas.llms.base import LangchainLLMWrapper\n",
    "\n",
    "gpt4_wrapper = LangchainLLMWrapper(gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62645da8-6a52-4cbb-bec7-59f7e153cd38",
   "metadata": {},
   "source": [
    "Substitute the `llm` in `Metric` instance with the newly create GPT4 model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307321ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import faithfulness\n",
    "\n",
    "faithfulness.llm = gpt4_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1930dd49",
   "metadata": {},
   "source": [
    "That's it! `Faithfulness` will now be using GPT-4 under the hood for evaluations.\n",
    "\n",
    "Now lets run the evaluations using the example from [quickstart](../quickstart.ipnb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62c0eadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'ground_truths', 'answer', 'contexts'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data\n",
    "from datasets import load_dataset\n",
    "\n",
    "fiqa_eval = load_dataset(\"explodinggradients/amnesty_qa\", \"english\")\n",
    "fiqa_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4396f6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AzureOpenAIEmbeddings' from 'langchain.embeddings' (/opt/anaconda3/envs/ragas-3.8/lib/python3.8/site-packages/langchain/embeddings/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# evaluate\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[1;32m      4\u001b[0m result \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m      5\u001b[0m     fiqa_eval[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)),  \u001b[38;5;66;03m# showing only 5 for demonstration\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[faithfulness],\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m result\n",
      "File \u001b[0;32m~/belar/src/ragas/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version \u001b[38;5;28;01mas\u001b[39;00m __version__\n",
      "File \u001b[0;32m~/belar/src/ragas/evaluation.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, concatenate_datasets\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_analytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationEvent, track\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Metric\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcritique\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AspectCritique\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     remap_column_names,\n\u001b[1;32m     14\u001b[0m     validate_column_dtypes,\n\u001b[1;32m     15\u001b[0m     validate_evaluation_modes,\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/belar/src/ragas/metrics/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_answer_correctness\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnswerCorrectness, answer_correctness\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_answer_relevance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnswerRelevancy, answer_relevancy\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_answer_similarity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnswerSimilarity, answer_similarity\n",
      "File \u001b[0;32m~/belar/src/ragas/metrics/_answer_correctness.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_answer_similarity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnswerSimilarity\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_faithfulness\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Faithfulness\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationMode, MetricWithLLM\n",
      "File \u001b[0;32m~/belar/src/ragas/metrics/_answer_similarity.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     HuggingfaceEmbeddings,\n\u001b[1;32m     11\u001b[0m     OpenAIEmbeddings,\n\u001b[1;32m     12\u001b[0m     embedding_factory,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIKeyNotFound\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationMode, MetricWithLLM\n",
      "File \u001b[0;32m~/belar/src/ragas/embeddings/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     AzureOpenAIEmbeddings,\n\u001b[1;32m      3\u001b[0m     HuggingfaceEmbeddings,\n\u001b[1;32m      4\u001b[0m     OpenAIEmbeddings,\n\u001b[1;32m      5\u001b[0m     RagasEmbeddings,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingfaceEmbeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAIEmbeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzureOpenAIEmbeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRagasEmbeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m ]\n",
      "File \u001b[0;32m~/belar/src/ragas/embeddings/base.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIEmbeddings \u001b[38;5;28;01mas\u001b[39;00m BaseAzureOpenAIEmbeddings\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings \u001b[38;5;28;01mas\u001b[39;00m BaseOpenAIEmbeddings\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embeddings\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AzureOpenAIEmbeddings' from 'langchain.embeddings' (/opt/anaconda3/envs/ragas-3.8/lib/python3.8/site-packages/langchain/embeddings/__init__.py)"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "from ragas import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    fiqa_eval[\"train\"].select(range(5)),  # showing only 5 for demonstration\n",
    "    metrics=[faithfulness],\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f490031e-fb73-4170-8762-61cadb4031e6",
   "metadata": {},
   "source": [
    "## Evaluating with Open-Source LLMs\n",
    "\n",
    "You can also use any of the Open-Source LLM for evaluation. Ragas support most the the deployment methods like [HuggingFace TGI](https://python.langchain.com/docs/integrations/llms/huggingface_textgen_inference), [Anyscale](https://python.langchain.com/docs/integrations/llms/anyscale), [vLLM](https://python.langchain.com/docs/integrations/llms/vllm) and many [more](https://python.langchain.com/docs/integrations/llms/) through Langchain. \n",
    "\n",
    "When it comes to selecting open-source language models, there are some rules of thumb to follow, given that the quality of evaluation metrics depends heavily on the model's quality:\n",
    "\n",
    "1. Opt for models with more than 7 billion parameters. This choice ensures a minimum level of quality in the results for ragas metrics. Models like Llama-2 or Mistral can be an excellent starting point.\n",
    "2. Always prioritize finetuned models over base models. Finetuned models tend to follow instructions more effectively, which can significantly improve their performance.\n",
    "3. If your project focuses on a specific domain, such as science or finance, prioritize models that have been pre-trained on a larger volume of tokens from your domain of interest. For instance, if you are working with research data, consider models pre-trained on a substantial number of tokens from platforms like arXiv or Semantic Scholar.\n",
    "\n",
    ":::{note}\n",
    "Choosing the right Open-Source LLM for evaluation can by tricky. You can also fine-tune these models to get even better performance on Ragas meterics. If you need some help/advice on that feel free to [talk to us](https://calendly.com/shahules/30min)\n",
    ":::\n",
    "\n",
    "In this example we are going to use [vLLM](https://github.com/vllm-project/vllm) for hosting a `HuggingFaceH4/zephyr-7b-alpha`. Checkout the [quickstart](https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html) for more details on how to get started with vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e313f2-e45c-4551-ab20-4e526e098740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the vLLM server\n",
    "!python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model HuggingFaceH4/zephyr-7b-alpha \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ddf74a-9830-4e1a-a4dd-7e5ec17a71e4",
   "metadata": {},
   "source": [
    "Now lets create an Langchain llm instance and wrap it with `RagasLLM` class. Because vLLM can run in OpenAI compatibilitiy mode, we can use the `ChatOpenAI` class as it is with small tweaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd4adf3-db15-4c95-bf7c-407266517214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from ragas.llms.base import LangchainLLMWrapper\n",
    "\n",
    "inference_server_url = \"http://localhost:8080/v1\"\n",
    "\n",
    "# create vLLM Langchain instance\n",
    "chat = ChatOpenAI(\n",
    "    model=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "    openai_api_key=\"no-key\",\n",
    "    openai_api_base=inference_server_url,\n",
    "    max_tokens=5,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# use the Ragas LangchainLLMWrapper wrapper to create a RagasLLM instance\n",
    "vllm = LangchainLLMWrapper(chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd7932a-7933-4de8-a6af-2830457e02a0",
   "metadata": {},
   "source": [
    "Now lets import all the metrics you want to use and change the llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20882d05-1b54-4d17-88a0-f7ada2d6a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    context_precision,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    ")\n",
    "from ragas.metrics.critique import harmfulness\n",
    "\n",
    "# change the LLM\n",
    "\n",
    "faithfulness.llm = vllm\n",
    "context_precision.llm = vllm\n",
    "context_recall.llm = vllm\n",
    "harmfulness.llm = vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a610f2-19e5-40ec-bb7d-760c1d608a85",
   "metadata": {},
   "source": [
    "Now you can run the evaluations with `HuggingFaceH4/zephyr-7b-alpha` and analyse the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8858300-7985-4c79-8d03-c671afd645ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 1/1 [06:25<00:00, 385.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.7167}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate\n",
    "from ragas import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    fiqa_eval[\"baseline\"].select(range(5)),  # showing only 5 for demonstration\n",
    "    metrics=[faithfulness],\n",
    ")\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
