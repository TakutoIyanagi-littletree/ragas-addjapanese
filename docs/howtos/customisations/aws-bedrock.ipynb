{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c249b40",
   "metadata": {},
   "source": [
    "# Using Amazon Bedrock\n",
    "\n",
    "Amazon Bedrock is a fully managed service that makes FMs from leading AI startups and Amazon available via an API, so you can choose from a wide range of FMs to find the model that is best suited for your use case.\n",
    "\n",
    "This tutorial will show you how to use Amazon Bedrock endpoints and LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e63f667",
   "metadata": {},
   "source": [
    ":::{Note}\n",
    "this guide is for folks who are using the Amazon Bedrock endpoints. Check the [evaluation guide](../../getstarted/evaluation.md) if your using OpenAI endpoints.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f29e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ragas langchain pydantic pandas --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54b5e01",
   "metadata": {},
   "source": [
    "### Load sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b658e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "from datasets import load_dataset\n",
    "\n",
    "amnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v2\")\n",
    "amnesty_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1201199",
   "metadata": {},
   "source": [
    "Now let us use the llm from Bedrock using `Bedrock` class from Langchain. \n",
    "Init a new instance of `Bedrock` with the `model_id` of the model you want to use. \n",
    "You will also have to init `BedrockEmbeddings` for evaluate function with the metrics that use embeddings (context_precision and context_recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40406a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "config = {\n",
    "    \"credentials_profile_name\": \"default\",  # E.g \"default\"\n",
    "    \"region_name\": \"us-east-1\",  # E.g. \"us-east-1\"\n",
    "    \"model_id\": \"cohere.command-text-v14\",  # E.g \"anthropic.claude-v2\"\n",
    "    \"model_kwargs\": {\"temperature\": 0.4},  # Your model parameters\n",
    "    \"embeddings_model\": \"cohere.embed-english-v3\",  # your embeddings model\n",
    "}\n",
    "\n",
    "bedrock_model = Bedrock(\n",
    "    credentials_profile_name=config[\"credentials_profile_name\"],\n",
    "    model_id=config[\"model_id\"],\n",
    "    model_kwargs=config[\"model_kwargs\"],\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "\n",
    "# init the embeddings\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    credentials_profile_name=config[\"credentials_profile_name\"],\n",
    "    model_id=config[\"embeddings_model\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44641e41",
   "metadata": {},
   "source": [
    "Now we can use the llm and embeddings with `Bedrock` by passing it in the evaluate function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6ecd5a",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Running the evalutation is as simple as calling evaluate on the `Dataset` with the metrics of your choice.\n",
    "\n",
    "First Let us import metrics that we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17bcf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    context_precision,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    answer_relevancy,\n",
    ")\n",
    "from ragas.metrics.critique import harmfulness\n",
    "\n",
    "# list of metrics we're going to use\n",
    "metrics = [\n",
    "    context_precision,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    answer_relevancy,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523885f4",
   "metadata": {},
   "source": [
    "Let us wrap the bedrock LLM's as LangchainLLMWrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings.base import LangchainEmbeddingsWrapper\n",
    "\n",
    "# Wrap the LLM's as ragas Langchain LLM's\n",
    "ragas_critic_llm = LangchainLLMWrapper(bedrock_model)\n",
    "# wrap the embeddings model\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(bedrock_embeddings)\n",
    "\n",
    "for m in metrics:\n",
    "    # change LLM for metric\n",
    "    m.__setattr__(\"llm\", ragas_critic_llm)\n",
    "\n",
    "    # check if this metric needs embeddings\n",
    "    if hasattr(m, \"embeddings\"):\n",
    "        # if so change the embeddings model\n",
    "        m.__setattr__(\"embeddings\", ragas_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa9aee9",
   "metadata": {},
   "source": [
    "Now you call evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "import nest_asyncio  # CHECK NOTES\n",
    "\n",
    "# NOTES: Only used when running on a jupyter notebook, otherwise comment or remove this function.\n",
    "nest_asyncio.apply()\n",
    "\n",
    "result = evaluate(\n",
    "    amnesty_qa[\"eval\"].select(range(3)),\n",
    "    metrics=metrics,\n",
    "    llm=ragas_critic_llm,\n",
    "    embeddings=ragas_embeddings,\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc0ec2",
   "metadata": {},
   "source": [
    "and there you have the it, all the scores you need.\n",
    "\n",
    "now if we want to dig into the results and figure out examples where your pipeline performed worse or really good you can easily convert it into a pandas array and use your standard analytics tools too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8686bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f668fce1",
   "metadata": {},
   "source": [
    "And thats it!\n",
    "\n",
    "if you have any suggestion/feedbacks/things your not happy about, please do share it in the [issue section](https://github.com/explodinggradients/ragas/issues). We love hearing from you üòÅ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
