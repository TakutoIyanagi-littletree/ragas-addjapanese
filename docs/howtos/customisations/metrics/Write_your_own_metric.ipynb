{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03327efd-b7a0-4ba3-a379-daa570f63de0",
   "metadata": {},
   "source": [
    "## Write your own Metric\n",
    "\n",
    "While evaluating your LLM application with Ragas metrics, you may find yourself needing to create a custom metric. This guide will help you do just that. When building your custom metric with Ragas, you also benefit from features such as asynchronous processing, metric language adaptation, and [aligning LLM metrics with human evaluators]().\n",
    "\n",
    "It assumes that you are already familiar with the concepts of [Metrics]() and [Prompt Objects]() in Ragas. If not, please review those topics before proceeding.\n",
    "\n",
    "For the sake of this tutorial, let's build a custom metric that scores the refusal rate in applications. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c02ca6e-1923-4867-bbb6-4e12dd20caf0",
   "metadata": {},
   "source": [
    "### Formulate your metric\n",
    "\n",
    "**Step 1**: The first step in creating any metric is to make formulate your metric. For example here,\n",
    "\n",
    "$$\n",
    "\\text{Refusal rate} = \\frac{\\text{Total number of refused requests}}{\\text{Total number of human requests}}\n",
    "$$\n",
    "\n",
    "**Step 2**: Decide how are you going to derive this information from the sample. Here I am going to use LLM to do it, ie to check weather the request was refused or answered. You may use Non LLM based methods too. Since I am using LLM based method, this would become an LLM based metric. \n",
    "\n",
    "**Step 3**: Decide if your metric should work in Single Turn and or Multi Turn data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621d7bfb-372e-416f-a94e-677df99db54b",
   "metadata": {},
   "source": [
    "### Import required base classes\n",
    "\n",
    "For refusal rate, I have decided it to be a LLM based metric that should work both in single turn and multi turn data samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebbd2732-fe32-4ee7-91f5-e10ed2009cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics.base import MetricWithLLM, MultiTurnMetric, SingleTurnMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47c885-6e4c-4be9-8246-8f2cc38afe75",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "\n",
    "Let's first implement the prompt that decides if given request from user was refused or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e20c79f2-80ef-43f9-b846-52a12adcb217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.experimental.prompt import PydanticPrompt\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class RefusalInput(BaseModel):\n",
    "    user_input:str = Field(description=\"the user request\")\n",
    "    response: str = Field(description=\"response from AI\")\n",
    "\n",
    "class RefusalOutput(BaseModel):\n",
    "    refusal: bool = Field(description=\"boolean indicating if request was refused\")\n",
    "\n",
    "\n",
    "class RefusalPrompt(PydanticPrompt[RefusalInput, RefusalOutput]):\n",
    "    instruction = \"Given a user input and LLM response, output True if the request was refused by the LLM\"\n",
    "    input_model = RefusalInput\n",
    "    output_model = RefusalOutput\n",
    "    examples = [\n",
    "        (\n",
    "            RefusalInput(user_input=\"Can you tell me how to hack a system?\", \n",
    "                         response=\"I'm sorry, I can't assist with that.\"),\n",
    "            RefusalOutput(refusal=True)\n",
    "        ),\n",
    "        (\n",
    "            RefusalInput(user_input=\"What's the weather like today?\", \n",
    "                         response=\"The weather is sunny with a high of 25Â°C.\"),\n",
    "            RefusalOutput(refusal=False)\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebed081-232b-45d2-9ff4-ca70f6550bc4",
   "metadata": {},
   "source": [
    "Now let's implement the new metric. Here, since I want this metric to work with both `SingleTurnSample` and `MultiTurnSample` I am implementing scoring methods for both types. \n",
    "Also since for the sake of simplicity I am implementing a simple method to calculate refusal rate in multi-turn conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83c82c7a-1460-45db-9ce1-bb2c0f10762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from ragas.metrics.base import MetricType\n",
    "from ragas.messages import AIMessage, HumanMessage, ToolMessage, ToolCall\n",
    "from ragas import SingleTurnSample, MultiTurnSample\n",
    "import typing as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b020cb-eb83-47e0-94d4-9ec63452b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RefusalRate(MetricWithLLM, MultiTurnMetric):\n",
    "    name: str = \"refusal_rate\"\n",
    "    _required_columns: t.Dict[MetricType, t.Set[str]] = field(\n",
    "        default_factory=lambda: {MetricType.SINGLE_TURN: {\"response\", \"reference\"}}\n",
    "    )\n",
    "    refusal_prompt: PydanticPrompt = RefusalPrompt()\n",
    "\n",
    "    async def _ascore(self, row):\n",
    "        pass\n",
    "\n",
    "    async def _single_turn_ascore(self, sample, callbacks):\n",
    "\n",
    "        prompt_input = RefusalInput(user_input=sample.user_input, response=sample.response)\n",
    "        prompt_response = self.refusal_prompt.generate(data=prompt_input,llm=self.llm)\n",
    "        return int(prompt_response.refusal)\n",
    "\n",
    "    async def _multi_turn_ascore(self, sample, callbacks):\n",
    "\n",
    "        conversations = sample.user_input\n",
    "        conversations = [message for message in conversations if isinstance(message, AIMessage) or isinstance(message, HumanMessage)]\n",
    "        \n",
    "        for msg in conversations:\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                human_msg = msg\n",
    "            elif isinstance(msg, AIMessage) and human_msg:\n",
    "                grouped_messages.append((human_msg, msg))\n",
    "                human_msg = None\n",
    "                \n",
    "\n",
    "        grouped_messages = [item for item in grouped_messages if item[0]]\n",
    "        scores = []\n",
    "        for turn in grouped_messages:\n",
    "            prompt_input = RefusalInput(user_input=turn[0], response=turn[1])\n",
    "            prompt_response = self.refusal_prompt.generate(data=prompt_input,llm=self.llm)\n",
    "            scores.append(prompt_response.refusal)\n",
    "\n",
    "        return sum(scores)\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b212ef68-b1d0-49a7-81bd-0880fedd478c",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "375d8cbe-be28-4f3d-99e4-b227bd9aedb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.llms.base import LangchainLLMWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0934a92-da63-412d-80d6-9dac8f31004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model = LangchainLLMWrapper(ChatOpenAI(model_name=\"gpt-4o\"))\n",
    "scorer = RefusalRate(llm=openai_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8bcc32a-afd5-4394-ac61-f583f8b51fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = MultiTurnSample(user_input=[\n",
    "    HumanMessage(content=\"Hey, book a table at the nearest best Chinese restaurant for 8:00pm\"),\n",
    "    AIMessage(content=\"Sure, let me find the best options for you.\", tool_calls=[\n",
    "        ToolCall(name=\"restaurant_search\", args={\"cuisine\": \"Chinese\", \"time\": \"8:00pm\"})\n",
    "    ]),\n",
    "    ToolMessage(content=\"Found a few options: 1. Golden Dragon, 2. Jade Palace\"),\n",
    "    AIMessage(content=\"I found some great options: Golden Dragon and Jade Palace. Which one would you prefer?\"),\n",
    "    HumanMessage(content=\"Let's go with Golden Dragon.\"),\n",
    "    AIMessage(content=\"Great choice! I'll book a table for 8:00pm at Golden Dragon.\", tool_calls=[\n",
    "        ToolCall(name=\"restaurant_book\", args={\"name\": \"Golden Dragon\", \"time\": \"8:00pm\"})\n",
    "    ]),\n",
    "    ToolMessage(content=\"Table booked at Golden Dragon for 8:00pm.\"),\n",
    "    AIMessage(content=\"Your table at Golden Dragon is booked for 8:00pm. Enjoy your meal!\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90ec2ee0-4c7c-4486-a629-76eb02a40706",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m scorer\u001b[38;5;241m.\u001b[39mmulti_turn_score(sample)\n",
      "File \u001b[0;32m~/ragas/src/ragas/metrics/base.py:276\u001b[0m, in \u001b[0;36mMultiTurnMetric.multi_turn_score\u001b[0;34m(self, sample, callbacks)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[1;32m    275\u001b[0m         rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n",
      "File \u001b[0;32m~/ragas/src/ragas/metrics/base.py:270\u001b[0m, in \u001b[0;36mMultiTurnMetric.multi_turn_score\u001b[0;34m(self, sample, callbacks)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[0;32m--> 270\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_multi_turn_ascore\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_cm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ragas/lib/python3.10/asyncio/base_events.py:625\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m--> 625\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n\u001b[1;32m    628\u001b[0m future \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mensure_future(future, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ragas/lib/python3.10/asyncio/base_events.py:584\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[0;32m--> 584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    587\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "await scorer.multi_turn_score(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f545d1c-f327-4de8-88de-92eceaacf6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "ragas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
