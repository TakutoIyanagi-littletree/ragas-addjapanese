{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61c367aa-e0a3-4116-bda7-7b81404211fd",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- [Introduction]()\n",
    "- [Testset generation]()\n",
    "- [Build RAG with llama-index]()\n",
    "- [Tracing using Phoenix]()\n",
    "- [Evaluation]()\n",
    "- [Embedding analysis]()\n",
    "- [Conclusion]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baf25a1-02bc-43c7-82e9-93e362485b74",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4899e7a-43ef-4ae7-8f12-0024037a0b43",
   "metadata": {},
   "source": [
    "In this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d18e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ragas pypdf arize-phoenix \"openinference-instrumentation-llama-index<1.0.0\" \"llama-index<0.10.0\" pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02304338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display the complete contents of dataframe cells.\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a8385c",
   "metadata": {},
   "source": [
    "## Configure Your OpenAI API Key\n",
    "\n",
    "Set your OpenAI API key if it is not already set as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534f85a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f707d3-e921-4f81-bbfb-a2ddb917c79d",
   "metadata": {},
   "source": [
    "## Generate Your Synthetic Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4ce7f",
   "metadata": {},
   "source": [
    "Follow the instructions [here](https://docs.github.com/en/repositories/working-with-files/managing-large-files/installing-git-large-file-storage) to install `git-lfs`. Then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a0aba-a055-4262-8bd2-ee9e11cfd3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://huggingface.co/datasets/explodinggradients/prompt-engineering-papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f764f32",
   "metadata": {},
   "source": [
    "We'll build an index over a dataset of prompt engineering papers in PDF format. Download the dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5e2125-3d3a-4a09-b307-24ab443087d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "dir_path = \"./prompt-engineering-papers\"\n",
    "reader = SimpleDirectoryReader(dir_path, num_files_limit=2)\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be8124",
   "metadata": {},
   "source": [
    "Use Ragas to generate a synthetic test set for evaluating your LLM application. Take a look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d7e1d0-4c6e-4fd8-bfb8-be7b42d3de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "TEST_SIZE = 100\n",
    "\n",
    "# generator with openai models\n",
    "generator = TestsetGenerator.with_openai()\n",
    "\n",
    "# set question type distribution\n",
    "distribution = {simple: 0.5, reasoning: 0.25, multi_context: 0.25}\n",
    "\n",
    "# generate testset\n",
    "testset = generator.generate_with_llamaindex_docs(\n",
    "    documents, test_size=TEST_SIZE, distributions=distribution\n",
    ")\n",
    "test_df = testset.to_pandas()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded50764-cd14-402b-93fd-0e8377b88ddd",
   "metadata": {},
   "source": [
    "## Build a RAG Application With LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd489694",
   "metadata": {},
   "source": [
    "Launch Phoenix in the background and instrument your LlamaIndex application so that your OpenInference spans and traces are sent to and collected by Phoenix. [OpenInference](https://github.com/Arize-ai/openinference/tree/main/spec) is an open standard built atop OpenTelemetry that captures and stores LLM application executions. It is designed to be a category of telemetry data that is used to understand the execution of LLMs and the surrounding application context, such as retrieval from vector stores and the usage of external tools such as search engines or APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f31213-78b2-47cc-8e60-5e7b3a94319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from llama_index import set_global_handler\n",
    "\n",
    "set_global_handler(\"arize_phoenix\")\n",
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70249df",
   "metadata": {},
   "source": [
    "Build your query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eba224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "\n",
    "\n",
    "def build_query_engine(documents):\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        service_context=ServiceContext.from_defaults(chunk_size=512),\n",
    "        embed_model=OpenAIEmbedding(),\n",
    "    )\n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k=2)\n",
    "    return query_engine\n",
    "\n",
    "\n",
    "query_engine = build_query_engine(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a10b4",
   "metadata": {},
   "source": [
    "If you check Phoenix, you should see embedding spans from when your corpus data was indexed. Export and save those embeddings into a dataframe for visualization later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c6e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.trace.dsl.helpers import SpanQuery\n",
    "\n",
    "client = px.Client()\n",
    "corpus_df = px.Client().query_spans(\n",
    "    SpanQuery().explode(\n",
    "        \"embedding.embeddings\",\n",
    "        text=\"embedding.text\",\n",
    "        vector=\"embedding.vector\",\n",
    "    )\n",
    ")\n",
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca64bc",
   "metadata": {},
   "source": [
    "Relaunch Phoenix to clear the accumulated traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a9366",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.close_app()\n",
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e745b4",
   "metadata": {},
   "source": [
    "## Evaluate Your LLM Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f0204",
   "metadata": {},
   "source": [
    "Use Ragas to evaluate your LlamaIndex application. This will invoke your instrumented LlamaIndex application on the test dataset generated previously and compare the generated answers with the human reference answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2597314-d6de-412d-b00c-3e00297746e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generate_response(query_engine, question):\n",
    "    response = query_engine.query(question)\n",
    "    return {\n",
    "        \"answer\": response.response,\n",
    "        \"contexts\": [c.node.get_content() for c in response.source_nodes],\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_ragas_dataset(query_engine, test_df):\n",
    "    test_questions = test_df[\"question\"].values\n",
    "    responses = [generate_response(query_engine, q) for q in tqdm(test_questions)]\n",
    "\n",
    "    dataset_dict = {\n",
    "        \"question\": test_questions,\n",
    "        \"answer\": [response[\"answer\"] for response in responses],\n",
    "        \"contexts\": [response[\"contexts\"] for response in responses],\n",
    "        \"ground_truth\": test_df[\"ground_truth\"].values.tolist(),\n",
    "    }\n",
    "    ds = Dataset.from_dict(dataset_dict)\n",
    "    return ds\n",
    "\n",
    "\n",
    "ragas_eval_dataset = generate_ragas_dataset(query_engine, test_df)\n",
    "ragas_evals_df = pd.DataFrame(ragas_eval_dataset)\n",
    "ragas_evals_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a671393",
   "metadata": {},
   "source": [
    "Checkout Phoenix to view your LlamaIndex application traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c843f75d",
   "metadata": {},
   "source": [
    "We'll save out a couple of dataframes, one containing embedding data that we'll visualize later, and another containing our exported traces and spans that we'll evaluate using Ragas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset containing embeddings for visualization\n",
    "query_embeddings_df = px.Client().query_spans(\n",
    "    SpanQuery().explode(\n",
    "        \"embedding.embeddings\", text=\"embedding.text\", vector=\"embedding.vector\"\n",
    "    )\n",
    ")\n",
    "query_embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.session.evaluation import get_qa_with_reference\n",
    "\n",
    "# dataset containing span data for evaluation with Ragas\n",
    "spans_dataframe = get_qa_with_reference(client)\n",
    "spans_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b96c87",
   "metadata": {},
   "source": [
    "Ragas uses LangChain to evaluate your LLM application data. Let's instrument LangChain with OpenInference so we can see what's going on under the hood when we evaluate our LLM application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24fae83-66e6-419d-a669-f491cef87935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "\n",
    "LangChainInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc94272",
   "metadata": {},
   "source": [
    "Evaluate your LLM traces and view the evaluation scores in dataframe format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5bf278-b3ea-4e2a-9653-f724f41c067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "evaluation_result = evaluate(\n",
    "    dataset=ragas_eval_dataset,\n",
    "    metrics=[faithfulness, answer_correctness, context_recall, context_precision],\n",
    ")\n",
    "eval_scores_df = pd.DataFrame(evaluation_result.scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eae5015",
   "metadata": {},
   "source": [
    "Submit your evaluations to Phoenix so they are visible as annotations on your spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1610a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.trace import SpanEvaluations\n",
    "\n",
    "# Assign span ids to your ragas evaluation scores (needed so Phoenix knows where to attach the spans).\n",
    "eval_data_df = pd.DataFrame(evaluation_result.dataset)\n",
    "assert eval_data_df.question.to_list() == list(\n",
    "    reversed(spans_dataframe.input.to_list())  # The spans are in reverse order.\n",
    "), \"Phoenix spans are in an unexpected order. Re-start the notebook and try again.\"\n",
    "eval_scores_df.index = pd.Index(\n",
    "    list(reversed(spans_dataframe.index.to_list())), name=spans_dataframe.index.name\n",
    ")\n",
    "\n",
    "# Log the evaluations to Phoenix.\n",
    "for eval_name in eval_scores_df.columns:\n",
    "    evals_df = eval_scores_df[[eval_name]].rename(columns={eval_name: \"score\"})\n",
    "    evals = SpanEvaluations(eval_name, evals_df)\n",
    "    px.log_evaluations(evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f44224",
   "metadata": {},
   "source": [
    "If you check out Phoenix, you'll see your Ragas evaluations as annotations on your application spans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a6c9e9",
   "metadata": {},
   "source": [
    "## Visualize and Analyze Your Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb964b4",
   "metadata": {},
   "source": [
    "Lastly, visualize your corpus and query embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e3e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings_df = query_embeddings_df.iloc[::-1]\n",
    "assert ragas_evals_df.question.tolist() == query_embeddings_df.text.tolist()\n",
    "assert test_df.question.tolist() == ragas_evals_df.question.tolist()\n",
    "query_df = pd.concat(\n",
    "    [\n",
    "        ragas_evals_df[[\"question\", \"answer\", \"ground_truth\"]].reset_index(drop=True),\n",
    "        query_embeddings_df[[\"vector\"]].reset_index(drop=True),\n",
    "        test_df[[\"evolution_type\"]],\n",
    "        eval_scores_df.reset_index(drop=True),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7992b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_schema = px.Schema(\n",
    "    prompt_column_names=px.EmbeddingColumnNames(\n",
    "        raw_data_column_name=\"question\", vector_column_name=\"vector\"\n",
    "    ),\n",
    "    response_column_names=\"answer\",\n",
    ")\n",
    "corpus_schema = px.Schema(\n",
    "    prompt_column_names=px.EmbeddingColumnNames(\n",
    "        raw_data_column_name=\"text\", vector_column_name=\"vector\"\n",
    "    )\n",
    ")\n",
    "# relaunch phoenix with a primary and corpus dataset to view embeddings\n",
    "px.close_app()\n",
    "session = px.launch_app(\n",
    "    primary=px.Dataset(query_df, query_schema, \"query\"),\n",
    "    corpus=px.Dataset(corpus_df.reset_index(drop=True), corpus_schema, \"corpus\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd6196",
   "metadata": {},
   "source": [
    "Explore your embeddings to surface clusters of problematic data.\n",
    "\n",
    "- Select the `vector` embedding.\n",
    "- Inspect your clusters to find patterns in your data.\n",
    "- Select the metric of your choice from the `metric` dropdown to view aggregate metrics on a per-cluster basis.\n",
    "- Select `Color By > dimension` and then the dimension of your choice to color your data by a particular field, for example, by Ragas evaluation scores such as faithfulness or answer correctness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
