{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586226e7",
   "metadata": {},
   "source": [
    "# Evaluating Langchain QA Chains\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. It can also be used to create RAG systems (or QA systems as they are reffered to in langchain). If you want to know more about creating RAG systems with langchain you can check the [docs](https://python.langchain.com/docs/use_cases/question_answering/).\n",
    "\n",
    "With this integration you can easily evaluate your QA chains with the metrics offered in ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "387a93b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f763e7",
   "metadata": {},
   "source": [
    "First lets load the dataset. We are going to build a generic QA system over the [NYC wikipedia page](https://en.wikipedia.org/wiki/New_York_City). Load the dataset and create the `VectorstoreIndex` and the `RetrievalQA` from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa9a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "loader = TextLoader(\"./nyc_wikipedia/nyc_text.txt\")\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=index.vectorstore.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c07292b",
   "metadata": {},
   "source": [
    "Testing it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ebdf8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New York City was named in 1664 in honor of the Duke of York, who later became King James II of England. It was named after him because King Charles II of England appointed the Duke as proprietor of the former territory of New Netherland, including the city of New Amsterdam, which was seized from Dutch control by England.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "question = \"How did New York City get its name?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e85b26",
   "metadata": {},
   "source": [
    "Now in order to evaluate the qa system we generated a few relevant questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afee47d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = [\n",
    "    'What is the population of New York City as of 2020?',\n",
    "    'Which borough of New York City has the highest population?',\n",
    "    'What is the economic significance of New York City?',\n",
    "    'How did New York City get its name?',\n",
    "    'What is the significance of the Statue of Liberty in New York City?'\n",
    "]\n",
    "\n",
    "queries = [{\"query\": q} for q in eval_questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73201ba8",
   "metadata": {},
   "source": [
    "### Introducing `RagasEvaluatorChain`\n",
    "\n",
    "Now comes the fun part. In order to evaluate the QA chains build with langchain, ragas provides you with a `RagasEvaluatorChain`. The `RagasEvaluatorChain` takes in any `Metric` in ragas and make a evaluation chain out of it.\n",
    "\n",
    "The evaluator chain has the following APIs\n",
    "\n",
    "- `__call__()`\n",
    "- `evaluate()`\n",
    "- `evaluate_run()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d9266d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.langchain.evalchain import RagasEvaluatorChain\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_relevancy\n",
    "\n",
    "# create evaluation chains\n",
    "faithfulness_chain = RagasEvaluatorChain(metric=faithfulness)\n",
    "answer_rel_chain = RagasEvaluatorChain(metric=answer_relevancy)\n",
    "context_rel_chain = RagasEvaluatorChain(metric=context_relevancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d11005",
   "metadata": {},
   "source": [
    "`__call__()`\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ede32cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result = faithfulness_chain(result)\n",
    "eval_result['faithfulness_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68470d",
   "metadata": {},
   "source": [
    "`evaluate()`\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ce7bff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 1/1 [00:28<00:00, 28.98s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'faithfulness_score': 1.0},\n",
       " {'faithfulness_score': 0.0},\n",
       " {'faithfulness_score': 1.0},\n",
       " {'faithfulness_score': 1.0},\n",
       " {'faithfulness_score': 1.0}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the queries as a batch for efficiency\n",
    "predictions = qa_chain.batch(queries)\n",
    "\n",
    "# evaluate\n",
    "print(\"evaluating...\")\n",
    "r = faithfulness_chain.evaluate(queries, predictions)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc71587",
   "metadata": {},
   "source": [
    "## Evaluate with langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e75144c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using existing dataset:  NYC test\n"
     ]
    }
   ],
   "source": [
    "# dataset creation\n",
    "\n",
    "from langsmith import Client\n",
    "from langsmith.utils import LangSmithError\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"NYC test\"\n",
    "\n",
    "try:\n",
    "    # check if dataset exists\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    print(\"using existing dataset: \", dataset.name) \n",
    "except LangSmithError:\n",
    "    # if not create a new one with the generated query examples\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name, description=\"NYC test dataset\"\n",
    "    )\n",
    "    for q in eval_questions:\n",
    "        client.create_example(\n",
    "            inputs={\"query\": q},\n",
    "            dataset_id = dataset.id,\n",
    "        )\n",
    "    \n",
    "    print(\"Created a new dataset: \", dataset.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dedaf27",
   "metadata": {},
   "source": [
    "![](./assets/langsmith-dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a6decc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_chain(return_context=True):\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=index.vectorstore.as_retriever(),\n",
    "        return_source_documents=return_context\n",
    "    )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25f7992f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '2023-08-18-23-24-58-RetrievalQA' at:\n",
      "https://smith.langchain.com/projects/p/838cb050-9f13-408e-8fd8-82cb43dd1e03?eval=true\n"
     ]
    }
   ],
   "source": [
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    custom_evaluators = [\n",
    "        faithfulness_chain,\n",
    "        answer_rel_chain,\n",
    "        context_rel_chain\n",
    "    ],\n",
    "    prediction_key=\"result\"\n",
    ")\n",
    "\n",
    "result = run_on_dataset(\n",
    "    client,\n",
    "    dataset_name,\n",
    "    create_qa_chain,\n",
    "    evaluation=evaluation_config,\n",
    "    input_mapper=lambda x: x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ddd67c",
   "metadata": {},
   "source": [
    "![](./assets/langsmith-evaluation.png)\n",
    "\n",
    "Evaluation Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe1516",
   "metadata": {},
   "source": [
    "Now if you want to dive more into the reasons for the scores and how to improve them\n",
    "\n",
    "![](./assets/langsmith-feedback.png)\n",
    "\n",
    "\n",
    "![](./assets/langsmith-ragas-chain-trace.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06480935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"./assets/langsmith-dataset.png\"\n",
    "\"./assets/langsmith-evaluation.png\"\n",
    "\"./assets/langsmith-feedback.png\"\n",
    "\"./assets/langsmith-ragas-chain-trace.png\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
