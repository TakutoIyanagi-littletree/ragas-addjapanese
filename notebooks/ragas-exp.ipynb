{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac87dd7e",
   "metadata": {},
   "source": [
    "# Vision\n",
    "**Develop Unsupervised model assisted evaluation methods**\n",
    "\n",
    "**Factual consistency**\n",
    "- NLI\n",
    "- QAQG\n",
    "\n",
    "**Relevance**\n",
    "- Prompt based scoring and normalisation\n",
    "\n",
    "**Retriever score**\n",
    "- Crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d3ef4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49dd39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_KEY =  json.load(open('/Users/shahules/openai-key.json'))[\"key\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed0ece",
   "metadata": {},
   "source": [
    "## OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "64f77eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.Completion.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "93407fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = OPENAI_KEY\n",
    "def llm(prompt,**kwargs):\n",
    "    response = openai.Completion.create(\n",
    "      model=kwargs.get(\"model\",\"text-davinci-003\"),\n",
    "      prompt=prompt,\n",
    "      temperature=kwargs.get(\"temperature\",0),\n",
    "      top_p=kwargs.get(\"top_p\",1),\n",
    "      frequency_penalty=kwargs.get(\"frequency_penalty\",0.0),\n",
    "      presence_penalty=kwargs.get(\"presence_penalty\",0.0),\n",
    "      max_tokens=kwargs.get(\"max_tokens\",500),\n",
    "      logprobs=kwargs.get(\"logprobs\",1),\n",
    "      n=kwargs.get(\"n\",1),\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047b046f",
   "metadata": {},
   "source": [
    "## NLI paradigm\n",
    "Aim is to find contradicting statements in `generated_answer`.\n",
    "1. Given `generated answer`, generate set of statements from it.\n",
    "2. Verify each of these statements against given `context` to find contradictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "f3f2304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_ANSWER_STMNT = \"\"\"Given a question and answer, create a statement.\n",
    "question: Who is the president of India?\n",
    "answer: Narendra Modi\n",
    "statement: Narendara Modi is the president of India.\n",
    "question: Which magazine was started first Arthur's Magazine or Women's Magazine?\n",
    "answer: Arthur's Magazine\n",
    "statement: Arthur's Magazine started before Women's magazine. \n",
    "question: Cadmium Chloride is slightly soluble in this chemical, it is also called what?\n",
    "answer: alochol\n",
    "statement: Cadmium Chloride is slightly soluble in alcohol.\n",
    "question: {}\n",
    "answer: {}\n",
    "statemtent:\"\"\"\n",
    "\n",
    "ANSWER_STMNT = \"\"\"\n",
    "Generate statements from given text.\n",
    "text: Albert Einstein was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. Best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\n",
    "statements: Albert Einstein was born in Germany.\\n\\nAlbert Einstein was best known for his theory of relativity.\n",
    "text: {}\n",
    "statements:\n",
    "\"\"\"\n",
    "\n",
    "VERIFY = \"\"\"\n",
    "Given a context and set of statements separated by '.', Answer YES for each statement if it is supported by context and NO if not.\n",
    "context: Albert Einstein was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. Best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\n",
    "statements: Albert Einstein was born in India. Albert Einstein was best known for his theory of relativity.\n",
    "answer: NO. YES. \n",
    "context: {}\n",
    "statements: {}\n",
    "answer:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "6623ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT = {\"YES\":0,\"NO\":1}\n",
    "\n",
    "def NLI(question,context,answer):\n",
    "    \n",
    "    \"\"\"\n",
    "    return number of contradicting statements.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## single phrase answer\n",
    "    if (len(answer.split()) < 4) or (len(answer.split('.'))==1):\n",
    "        \n",
    "        prompt = QUESTION_ANSWER_STMNT.format(question,answer)\n",
    "        response = llm(prompt)\n",
    "        statements = response[\"choices\"][0][\"text\"]\n",
    "        \n",
    "     \n",
    "    ## long form\n",
    "    else:\n",
    "        prompt = ANSWER_STMNT.format(answer)\n",
    "        response = llm(prompt)\n",
    "        statements = response[\"choices\"][0][\"text\"].split(\"\\n\\n\")\n",
    "\n",
    "    print(statements)\n",
    "    ## verify\n",
    "    prompt = VERIFY.format(context,statements)\n",
    "    output = llm(prompt)\n",
    "    score = sum([DICT[key.strip()] for key in output['choices'][0]['text'].split('.') if key!=''])\n",
    "        \n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "49794d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Shahul was the king of kengeri city. He was a smart man and had many coutiers. He owned 20 horses and 44 elephants.\"\n",
    "question = \"How many horses did king of kengeri own?\"\n",
    "answer = \"19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "c076bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"The actress who played Lolita, Sue Lyon, was 17 at the time of filming.\"\n",
    "context = \"\"\"\n",
    "Lolita is a 1962 psychological comedy-drama film[5] directed by Stanley Kubrick and based on the 1955 novel of the same title by Vladimir Nabokov, who is also credited with writing the screenplay. The film follows Humbert Humbert, a middle-aged literature lecturer who becomes sexually infatuated with Dolores Haze (nicknamed \"Lolita\"), a young adolescent girl. It stars James Mason, Shelley Winters, Peter Sellers and, as the titular character, Sue Lyon.\n",
    "\n",
    "Owing to restrictions imposed by the Motion Picture Production Code, the film toned down the most provocative aspects of the novel, sometimes leaving much to the audience's imagination. The actress who played Lolita, Sue Lyon, was 14 at the time of filming.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "2be64e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLI(question,context,answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81d54fc",
   "metadata": {},
   "source": [
    "## QA-QG paradigm\n",
    "- Generate question and answer pair from `generated answer`.\n",
    "- Given `context`, ask these questions\n",
    "- Verify answer correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d56b78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Question_generation = \"\"\"Given a text, extract {} noun phrases and create questions for each based on given text.\n",
    "text: Albert Einstein was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. Best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\n",
    "A: Germany\n",
    "Q: Where was Albert Einstein born?\n",
    "A: theory of relativity\n",
    "Q: What is Albert Einstein best known for?\n",
    "text: {}\n",
    "\"\"\"\n",
    "\n",
    "Question_answering = \"\"\"Given a text and set of questions, answer the questions\n",
    "text: Albert Einstein was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. Best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\n",
    "questions: Where was Albert Einstein born?\\n\\nWhat is Albert Einstein best known for?\n",
    "answers:Germany\\n\\ntheory of relativity\n",
    "text: {}\n",
    "questions:{}\n",
    "answers:\n",
    "\"\"\"\n",
    "\n",
    "Answer_verification = \"\"\"Given a set of questions, correct answer and student's answer return the number of questions incorrectly answered by student.\n",
    "Where was Albert Einstein born?\\nCorrect answer: Germany\\nStudent answer:India\\n\\n\n",
    "What is Albert Einstein best known for?\\nCorrect answer:  theory of relativity\\nStudent answer: theory of relativity\\n\\n\n",
    "score:1\n",
    "{}\n",
    "score:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2c041d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QAQG_fun(question,context,answer):\n",
    "    \n",
    "    \"\"\"\n",
    "    returns number of factual inconsistencies.\n",
    "    \"\"\"\n",
    "    def answer_ver(qstn,answer,cand):\n",
    "        \n",
    "        return f\"{qstn}\\nCorrect answer: {answer}\\nStudent answer: {cand}\"\n",
    "    \n",
    "    num = 2\n",
    "    prompt = Question_generation.format(num,answer)\n",
    "    output = llm(prompt)\n",
    "    qa_pairs = [re.sub(r'A:|Q:','',x).strip() for item in output['choices'][0]['text'].split(\"\\n\\n\") for x in item.split('\\n')]\n",
    "    qa_pairs = [tuple(qa_pairs[i:i+2]) for i in range(0,len(qa_pairs),2)]\n",
    "    \n",
    "    questions = \"\\n\\n\".join([qstn for ans,qstn in qa_pairs])\n",
    "    prompt = Question_answering.format(context,questions)\n",
    "    answers = llm(prompt)['choices'][0]['text'].split('\\n\\n')\n",
    "    \n",
    "    prompt = \"\\n\\n\".join([answer_ver(qstn,ans,cand) for (ans,qstn),cand in zip(qa_pairs,answers)])\n",
    "    output = llm(Answer_verification.format(prompt))['choices'][0]['text'].strip()\n",
    "    return int(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5ef32a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"The actress who played Lolita, Sue Lyon, was 14 at the time of filming.\"\n",
    "context = \"\"\"\n",
    "Lolita is a 1962 psychological comedy-drama film[5] directed by Stanley Kubrick and based on the 1955 novel of the same title by Vladimir Nabokov, who is also credited with writing the screenplay. The film follows Humbert Humbert, a middle-aged literature lecturer who becomes sexually infatuated with Dolores Haze (nicknamed \"Lolita\"), a young adolescent girl. It stars James Mason, Shelley Winters, Peter Sellers and, as the titular character, Sue Lyon.\n",
    "\n",
    "Owing to restrictions imposed by the Motion Picture Production Code, the film toned down the most provocative aspects of the novel, sometimes leaving much to the audience's imagination. The actress who played Lolita, Sue Lyon, was 14 at the time of filming.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1990cfc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QAQG_fun(\"\",context,answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88585eae",
   "metadata": {},
   "source": [
    "## G-Eval\n",
    "- Define criterions to evaluate model.\n",
    "- Normalize `score = prob(s) * s`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "21cdf794",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevence = \"\"\"\n",
    "Evaluation Criteria.\\n\n",
    "Relevance (1-5) - how relevant is the reply to the given question.\n",
    "1. Read the reply and compare it to the question. Check if the given reply\n",
    "actually answers the question, and if it presents them in a clear and logical order.\n",
    "2. The reply should include only required information to answer the question.\n",
    "3. Penalize replies that contain redundancies and excess information.\n",
    "4. Assign a score for Relevance on a scale of 1 to 5, where 1 is the lowest and\n",
    "5 is the highest based on the Evaluation Criteria.\n",
    "\n",
    "question:{}\n",
    "reply:{}\n",
    "score:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "469a4f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "26febfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_eval(question,context,answer):\n",
    "    \n",
    "    prompt = relevence.format(question,answer)\n",
    "    output = llm(prompt)[\"choices\"][0]\n",
    "    prob = np.exp(sum(output[\"logprobs\"][\"token_logprobs\"]))\n",
    "    score = int(output[\"text\"].strip())\n",
    "    return prob * score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "69cac7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which year did Lolita release?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "57c84197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2831655698335416"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_eval(question,context,answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1bad3e",
   "metadata": {},
   "source": [
    "## retrieval score\n",
    "- Scores `retrieved passages` according to `question`\n",
    "- Score is lower the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "fd90471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer,GPTNeoForCausalLM,GPTNe\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "670b3cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model_name):\n",
    "    \n",
    "    model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    return model,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "9c9bcf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model,tokenizer = load(\"EleutherAI/gpt-neo-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "46d87dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreival_score(question,context,):\n",
    "    \n",
    "    \"\"\"\n",
    "    Retriver score\n",
    "    lower the better.\n",
    "    \"\"\"\n",
    "    \n",
    "    qstn_template = \"Please write a question based on this passage.\"\n",
    "    prompt = qstn_template + context\n",
    "    \n",
    "    inputs = tokenizer.encode(prompt)\n",
    "    outputs = tokenizer.encode(question)\n",
    "    input_ids = inputs + outputs\n",
    "    output_ids = inputs + outputs\n",
    "    output_ids[:len(inputs)] = [-100]*len(inputs)\n",
    "    input_ids,output_ids = torch.LongTensor(input_ids),torch.LongTensor(output_ids)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        loss = model(input_ids=input_ids,\n",
    "             labels=output_ids,\n",
    "             output_hidden_states=False).loss\n",
    "    \n",
    "    return loss.item()\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "869c1ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Shahul was the king of kengeri city. He was a smart man and had many coutiers. He owned 20 horses and 44 elephants.\"\n",
    "question = \"How many horses and elephants did king of kengeri own?\"\n",
    "answer = \"19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "49ad5cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([53]) torch.Size([53])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.766173839569092"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retreival_score(question,context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20bddec",
   "metadata": {},
   "source": [
    "## Dataset playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "353d82a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset hotpot_qa (/Users/shahules/.cache/huggingface/datasets/hotpot_qa/distractor/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5)\n"
     ]
    }
   ],
   "source": [
    "hotpot_qa = load_dataset(\"hotpot_qa\",\"distractor\",split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "506bc233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "5b4ea165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3641b172",
   "metadata": {},
   "source": [
    "## NLI on HotpostQA\n",
    "- iterate on samples and pass wrong answers on random instances\n",
    "- Pass question,context,answer\n",
    "- Check if NLI score reflects when wrong answer is passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "202f33da",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_answer = \"\"\"Given a question and correct answer, generate a plausible wrong answer\n",
    "question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
    "correct answer: yes\n",
    "answer: no\n",
    "question: {}\n",
    "correct answer: {}\n",
    "answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "78dcc2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hotpot_qa_nli():\n",
    "    \n",
    "    for item in hotpot_qa.shuffle().select(range(0,10)):\n",
    "        answer_correct = True\n",
    "        question = item['question']\n",
    "        answer  = item['answer']\n",
    "        if random.randint(0,10)>=5:\n",
    "            answer = llm(wrong_answer.format(question,answer))['choices'][0]['text'].strip()\n",
    "            answer_correct = False\n",
    "        titles,ids = item['supporting_facts'].values()\n",
    "        sentence_ids = [item['context']['title'].index(i) for i in titles]\n",
    "        sentences = [item['context']['sentences'][i][k] for i,k in zip(sentence_ids,ids)]\n",
    "        context = ' '.join(sentences)\n",
    "        print(\"question:\",question)\n",
    "        print(\"context:\",context)\n",
    "        print(\"answer:\",answer)\n",
    "        print(\"Correctness\",answer_correct)\n",
    "        nli = NLI(question,context,answer)\n",
    "        print(\"NLI Score\",nli)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "d1f0366d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Which Australian professional women's basketball team has an American playing in it?\n",
      "context: Colleen Planeta (born September 3, 1988) is an American professional basketball player.  She currently plays for the Adelaide Lightning in the WNBL. The Adelaide Lightning are an Australian professional women's basketball team competing in the Women's National Basketball League (WNBL).\n",
      "answer: Melbourne Boomers\n",
      "Correctness False\n",
      " An American is playing for the Melbourne Boomers, an Australian professional women's basketball team.\n",
      "NLI Score 1\n",
      "\n",
      "\n",
      "question: The Company They Keep is a book written by Diana Pavlac Glyer, who is a professor at a university in Azusa, California, that was founded in 1899, and is under the auspices of what religion?\n",
      "context: The Company They Keep: C. S. Lewis and J. R. R. Tolkien as Writers in Community (2007) is a non-fiction book written by Diana Pavlac Glyer, an Inklings scholar and English professor at Azusa Pacific University.  \"The Company They Keep\" challenges the commonly held belief that the Inklings did not influence each other through a detailed and engaging examination of both published and unpublished works, papers, and letters written by J.R.R. Tolkien, C.S. Lewis, Charles Williams, Owen Barfield, Warren Lewis and the lesser-known writers who comprised the Inklings. Azusa Pacific University (APU) is a private, evangelical Christian university located near Los Angeles in suburban Azusa, California, United States.  The university was founded in 1899, with classes opening on March 3, 1900, in Whittier, California, and began offering degrees in 1939.\n",
      "answer: evangelical Christian\n",
      "Correctness True\n",
      " Diana Pavlac Glyer's book The Company They Keep was written while she was a professor at an evangelical Christian university in Azusa, California founded in 1899.\n",
      "NLI Score 0\n",
      "\n",
      "\n",
      "question: What notable radio personality was born in North Bend, Oregon?\n",
      "context:  Notable radio personalities include pop music radio hosts Martin Block, Alan Freed, Dick Clark, Delilah Luke, Ameen Sayani, Wolfman Jack, and Casey Kasem, shock jocks such as Don Imus and Howard Stern, as well as sports talk hosts such as Mike Francesa and political talk hosts such as Rush Limbaugh. Delilah Rene (born February 15, 1960, North Bend, Oregon) is an American radio personality, author, and songwriter, best known as the host of a nationally syndicated nightly U.S. radio song request and dedication program, with an estimated 8 million listeners.\n",
      "answer: Howard Stern\n",
      "Correctness False\n",
      " Howard Stern was born in North Bend, Oregon.\n",
      "NLI Score 1\n",
      "\n",
      "\n",
      "question: Mr. & Mrs. Mxyzptlk is an episode of the superhero TV series that originally aired on what network?\n",
      "context: \"Mr. & Mrs. Mxyzptlk\" is the thirteenth episode of the second season from The CW television series \"Supergirl\", which aired on February 20, 2017. Supergirl is an American superhero action-adventure television series developed by Ali Adler, Greg Berlanti and Andrew Kreisberg (the latter two having previously created \"Arrow\" and \"The Flash\") that originally aired on CBS and premiered on October 26, 2015.\n",
      "answer: CBS\n",
      "Correctness True\n",
      " Mr. & Mrs. Mxyzptlk originally aired on CBS as an episode of the superhero TV series.\n",
      "NLI Score 0\n",
      "\n",
      "\n",
      "question: Which country refrained from participating in the 1991 Baltic Cup though it had participated in previous Baltic Cup competitions?\n",
      "context: The 1991 season was the 71st season of competitive football (soccer) in Estonia.  During the Soviet era, when all the nations were part of the Soviet Union, Belarus also took part in some of the competitions.\n",
      "answer: Ukraine\n",
      "Correctness False\n",
      " Ukraine refrained from participating in the 1991 Baltic Cup despite having participated in previous Baltic Cup competitions.\n",
      "NLI Score 1\n",
      "\n",
      "\n",
      "question: In which year was the King who made the 1925 Birthday Honours born?\n",
      "context: The 1925 Birthday Honours were appointments by King George V to various orders and honours to reward and highlight good works by citizens of the British Empire. George V (George Frederick Ernest Albert; 3 June 1865 – 20 January 1936) was King of the United Kingdom and the British Dominions, and Emperor of India, from 6 May 1910 until his death in 1936.\n",
      "answer: 1875\n",
      "Correctness False\n",
      " The King who made the 1925 Birthday Honours was born in 1875.\n",
      "NLI Score 1\n",
      "\n",
      "\n",
      "question: The Westchester Knicks are a basketball team in a minor league basketball organization that was known as what from 2001 to 2005?\n",
      "context: The Westchester Knicks are an American professional basketball team of the NBA G League and is an affiliate of the New York Knicks in the National Basketball Association (NBA). The NBA G League is the National Basketball Association's official minor league basketball organization.  The league was known as the National Basketball Development League (NBDL) from 2001 to 2005, and the NBA Development League (NBA D-League) from 2005 until 2017.\n",
      "answer: American Basketball Association\n",
      "Correctness False\n",
      " The Westchester Knicks were part of the American Basketball Association from 2001 to 2005.\n",
      "NLI Score 1\n",
      "\n",
      "\n",
      "question: Between the two operas, Mosè in Egitto and Lucrezia Borgia, which one has more acts?\n",
      "context: Mosè in Egitto (\"Moses in Egypt)\" (] ) is a three-act opera written by Gioachino Rossini to an Italian libretto by Andrea Leone Tottola, which was based on a 1760 play by Francesco Ringhieri, \"L'Osiride\". Lucrezia Borgia is a melodramatic opera in a prologue and two acts by Gaetano Donizetti.\n",
      "answer: Lucrezia Borgia\n",
      "Correctness False\n",
      " Lucrezia Borgia has more acts than Mosè in Egitto.\n",
      "NLI Score 1\n",
      "\n",
      "\n",
      "question: Where are the main administrative offices of the institute located where K. S. Babu earned his PhD in 1986?\n",
      "context:  He received his PhD in 1986 from the University of Hawaii, under the supervision of Ernest Ma.  The U.H. system's main administrative offices are located on the property of the University of Hawaiʻi at Mānoa in Honolulu CDP.\n",
      "answer: Tokyo\n",
      "Correctness False\n",
      " The main administrative offices of the institute where K. S. Babu earned his PhD in 1986 are located in Tokyo.\n",
      "NLI Score 1\n",
      "\n",
      "\n",
      "question: What year did a director of a North Korean cinema film get kidnapped?\n",
      "context:  Director Shin had been kidnapped in 1978 by North Korean intelligence on the orders of Kim Jong-il, son of the then-ruling Kim Il-sung.  Outsider appraisal of North Korean cinema is often condescending, while statements from official North Korean sources include claims like, \"In recent years our film art has created an unprecedented sensation in the world's filmdom... The revolutionary people of the world are unstinting in their praise of this feature film and other monumental works, calling them 'the first-class films by international standards', 'the most wonderful movies ever produced' and 'immortal revolutionary and popular films'.\"\n",
      "answer: 1978\n",
      "Correctness True\n",
      " A director of a North Korean cinema film was kidnapped in 1978.\n",
      "NLI Score 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hotpot_qa_nli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "c686e09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brown State Fishing Lake is in a country that has a population of how many inhabitants ?'"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotpot_qa[15]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "6d41a0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotpot_qa[15]['context']['title'].index('Brown State Fishing Lake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2613da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alerts",
   "language": "python",
   "name": "alerts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
