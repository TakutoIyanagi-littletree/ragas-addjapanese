{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5be51b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34105067-eed8-4f09-913f-7043ff77cb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ragas/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b1990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader, SimpleDirectoryReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "321dbebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7457125d",
   "metadata": {},
   "source": [
    "## Assessing current state of test date generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ce32d303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shahules/belar/experimental\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bc5970e-0873-4e06-9b68-3dda31b1d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ArxivReader = download_loader(\"ArxivReader\")\n",
    "\n",
    "loader = ArxivReader() #uses simpledirectory reader under the hood, hence need modification to laod pages properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46684465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = loader.load_data(\"retrieval augmented generation AND large language model\",max_results=20,papers_dir='./arxiv-papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62b52b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(\"./arxiv-papers/\",num_files_limit=25)\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc3d9cf",
   "metadata": {},
   "source": [
    "## SimpleDirectoryReader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aab30508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b28f535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(documents[14].get_metadata_str())\n",
    "# print(\"CONTENT\")\n",
    "# print(documents[14].get_content())\n",
    "# print(len(documents[14].get_content().split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4139e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5.,  3.,  6., 13., 28., 21., 39., 10.,  5.,  1.]),\n",
       " array([  10. ,  113.1,  216.2,  319.3,  422.4,  525.5,  628.6,  731.7,\n",
       "         834.8,  937.9, 1041. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(doc.get_content().split()) for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df051faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dist =  {\n",
    "    \"simple\": 0.3,\n",
    "    \"reasoning\": 0.25,\n",
    "    \"multi_context\": .2,\n",
    "    \"conditional\": 0.25,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f9c0891",
   "metadata": {},
   "outputs": [],
   "source": [
    "testsetgenerator = TestsetGenerator.from_default(testset_distribution=test_dist,chunk_size=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "553ac808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shahules/belar/experimental\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "977ab453",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 644\n",
      "Len of text chunks 4 998\n",
      "Len of text chunks 2 767\n",
      "Len of text chunks 3 957\n",
      "seed question What is an example of a situation where you have to delegate tasks?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▋                                                                               | 1/30 [00:35<17:21, 35.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 704\n",
      "seed question What is the accuracy of I TER-RETGEN compared to DSP in multi-hop question answering?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▏                                                                         | 3/30 [01:21<11:50, 26.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 757\n",
      "seed question What is the most common positive score and negative score given by the user in their past reviews?\n",
      "{'reason': \"The question mentions 'the user' which is not specified, making it unclear without extra information\", 'verdict': 'No'}\n",
      "rewritten question What is the most common positive score and negative score given by the user in their past reviews according to the results and discussion in the study?\n",
      "{'reason': \"The question mentions a 'study' in it which makes it unclear without it\", 'verdict': 'No'}\n",
      "Len of text chunks 3 965\n",
      "seed question What is the number of train samples in the QReCC dataset for Conversational Search?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▍                                                                 | 6/30 [01:54<06:44, 16.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 698\n",
      "seed question What is the impact of combining language models and imperfect retrievers on the performance of retriever-then-read models?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███████████████████████████                                                      | 10/30 [02:45<04:52, 14.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 455\n",
      "seed question How many generations were sampled for the evaluation of text fluency and coherence?\n",
      "{'reason': \"The question mentions an 'evaluation' but doesn't specify which one, making it unclear without additional context\", 'verdict': 'No'}\n",
      "rewritten question How many generations were sampled for the evaluation of text fluency and coherence in the study?\n",
      "{'reason': \"The question mentions 'the study' but doesn't specify which one, making it unclear without it\", 'verdict': 'No'}\n",
      "Len of text chunks 4 980\n",
      "seed question What is the purpose of the SELF-RAG framework?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|████████████████████████████████████████▌                                        | 15/30 [03:26<02:52, 11.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 580\n",
      "seed question What is the average improvement in performance on the TopiOCQA dataset when using the Vicuna generator with different retrievers?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|████████████████████████████████████████████████████████▋                        | 21/30 [03:51<01:13,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 616\n",
      "Len of text chunks 2 728\n",
      "Len of text chunks 2 724\n",
      "Len of text chunks 3 1018\n",
      "seed question Which film came out first, Blind Shaft or The Mask Of Fu Manchu?\n",
      "{'reason': 'The question is clear', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|███████████████████████████████████████████████████████████████████████████▌     | 28/30 [04:10<00:11,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 615\n",
      "seed question What is the purpose of the CORE framework?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "36it [04:25,  4.19s/it]                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 1 556\n",
      "seed question What is the proposed framework in this research paper?\n",
      "{'reason': \"The question mentions a 'research paper' in it which makes it unclear without it\", 'verdict': 'No'}\n",
      "rewritten question What is the proposed framework in the research paper RETRIEVAL-GENERATION SYNERGY AUGMENTED LARGE LANGUAGE MODELS?\n",
      "{'reason': 'The question mentions a specific research paper and its focus, which makes it clear without extra information', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "45it [05:02,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 766\n",
      "seed question Did Spiderman fight against Falcon in the MCU?\n",
      "Answer: No, Spiderman did not fight against Falcon in the MCU.\n",
      "{'reason': 'The question is clear', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "55it [05:23,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 1 545\n",
      "seed question What is the impact of using Hindi retrieval data on the model's ability to retrieve \"Neutral\" content in the mBERT model?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "66it [05:39,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 668\n",
      "seed question What is the Personalized Accuracy of the GPT-3.5 Summ. model for Task LaMP-1 with k=1?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "78it [06:26,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 835\n",
      "Len of text chunks 4 1004\n",
      "Len of text chunks 2 533\n",
      "Len of text chunks 4 958\n",
      "Len of text chunks 3 785\n",
      "seed question What is the architecture of the backbone LM used in the RETRO model?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "91it [07:15,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 1 481\n",
      "seed question What is the proposed method for large-scale retrieval in zero-shot scenarios?\n",
      "{'reason': \"The question mentions a 'proposed method' but doesn't specify where it's proposed from\", 'verdict': 'No'}\n",
      "rewritten question What is the proposed method for large-scale retrieval in zero-shot scenarios in the paper \"Large Language Models are Strong Zero-Shot Retriever\" by Tao Shen, Guodong Long, Xiubo Geng, Chongyang Tao, Tianyi Zhou, and Daxin Jiang?\n",
      "{'reason': 'The question is clear', 'verdict': 'Yes'}\n",
      "{'reason': \"The question mentions a specific 'paper' and its content, which makes it unclear without it\", 'verdict': 'No'}\n",
      "Len of text chunks 3 906\n",
      "seed question What was the accuracy of ChatGPT in noise robustness under a noise ratio of 0.6?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "105it [08:08,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 1 606\n",
      "seed question What is the purpose of the fact checking module in RETA-LLM?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "120it [08:31,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 817\n",
      "seed question What is the purpose of the study mentioned in the given context?\n",
      "{'reason': \"The question refers to a 'given context' which is not provided\", 'verdict': 'No'}\n",
      "rewritten question What is the purpose of the study mentioned in the given context?\n",
      "{'reason': \"The question refers to a 'given context' which is not provided\", 'verdict': 'No'}\n",
      "Len of text chunks 2 705\n",
      "seed question What is the number of test samples in the QReCC dataset?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "136it [09:01,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 1015\n",
      "seed question What is the effect of presence or absence of evidence and/or dialog history on the value of the tradeoff?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "153it [09:10,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 782\n",
      "Len of text chunks 3 915\n",
      "seed question What is the toxicity probability for RETRO (top-N= 2, top-K= 2) Pretraining in the Full set of prompts?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': \"The question mentions 'experimental results' but doesn't specify which experiment it's referring to\", 'verdict': 'No'}\n",
      "Len of text chunks 1 555\n",
      "seed question What is the proposed framework for enhancing the performance of small language models on edge devices?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "171it [10:31,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 843\n",
      "seed question What are the limitations of LLMs in terms of summarizing information from multiple documents?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "190it [10:41,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 4 865\n",
      "Len of text chunks 2 879\n",
      "seed question What are the different aspects of Carlos Moedas' biography, early life, and political career?\n",
      "{'reason': 'The question is clear', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "210it [11:07,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 5 620\n",
      "Len of text chunks 3 887\n",
      "Len of text chunks 2 665\n",
      "seed question What is the effect of changing the weighting term for ISSUP on the citation precision of the models?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "231it [11:29,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 796\n",
      "Len of text chunks 2 716\n",
      "seed question What is the overall performance of retriever-augmented language models in the reasoning LM task?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "253it [11:57,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 998\n",
      "seed question What is the accuracy of the KAR approach in information retrieval based on Table 1?\n",
      "{'reason': \"The question mentions 'Table 1' which is not provided, making it unclear\", 'verdict': 'No'}\n",
      "rewritten question What is the accuracy of the KAR approach in information retrieval based on Table 1 in the context of the Speech models integration?\n",
      "{'reason': \"The question mentions 'Table 1' and 'the context of the Speech models integration' which are not provided, making it unclear\", 'verdict': 'No'}\n",
      "Len of text chunks 3 736\n",
      "seed question What is the best value for the interpolation hyperparameter in the kNN-LM experiments?\n",
      "{'reason': \"The question mentions 'kNN-LM experiments' which makes it unclear without it\", 'verdict': 'No'}\n",
      "rewritten question What is the best value for the interpolation hyperparameter in the kNN-LM experiments in the paper 'Exploring the limits of transfer learning with a unified text-to-text transformer' by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu?\n",
      "{'reason': 'The question is clear', 'verdict': 'Yes'}\n",
      "{'reason': \"The question mentions a 'paper' in it which makes it unclear without it\", 'verdict': 'No'}\n",
      "Len of text chunks 3 804\n",
      "seed question What is the purpose of the rewriter optimization in reinforcement learning?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "276it [12:54,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 603\n",
      "seed question What is the purpose of SELF-RAG?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "300it [13:09,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 630\n",
      "seed question What are the hyperparameter settings used in the experiments?\n",
      "{'reason': \"The question mentions 'the experiments' which makes it unclear without it\", 'verdict': 'No'}\n",
      "rewritten question What is the strategy introduced to estimate Rt in the PRCA?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "325it [13:37,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 979\n",
      "seed question What is the impact of using Hindi retrieval data on the model's ability to retrieve \"Neutral\" content in the mBERT model?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "351it [13:52,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 751\n",
      "seed question What is the performance of FLARE with respect to the masking threshold β on 2WikiMultihopQA?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "378it [14:34,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 615\n",
      "seed question What techniques have been proposed for the automatic generation of counterfactual data or contrast sets?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "406it [15:03,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 563\n",
      "seed question What is the proposed method for fitting black-box Large Language Models for retrieval question answering in the PRCA framework?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "435it [15:34,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 736\n",
      "seed question What are the limitations of retrievers in reasoning tasks?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "465it [15:46,  2.04s/it]\n"
     ]
    }
   ],
   "source": [
    "test_df = testsetgenerator.generate(documents,test_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0b3b9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed_question</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_type</th>\n",
       "      <th>episode_done</th>\n",
       "      <th>evolution_elimination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is an example of a situation where you ha...</td>\n",
       "      <td>When should a manager pass on tasks to their t...</td>\n",
       "      <td>- Delegation–Delegation is the assignment of a...</td>\n",
       "      <td>A manager should pass on tasks to their team f...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the accuracy of I TER-RETGEN compared ...</td>\n",
       "      <td>How well do I TER-RETGEN and DSP work in answe...</td>\n",
       "      <td>- \"When increasing the number of iterations fo...</td>\n",
       "      <td>ITER-RETGEN works well in answering multi-hop ...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the number of train samples in the QRe...</td>\n",
       "      <td>How many train samples and how big is the QReC...</td>\n",
       "      <td>QReCC dataset [3] has 29,596 train samples.</td>\n",
       "      <td>The QReCC dataset for Conversational Search ha...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the impact of combining language model...</td>\n",
       "      <td>What happens when you mix imperfect retrievers...</td>\n",
       "      <td>- \"This section explores how the combination o...</td>\n",
       "      <td>When you mix imperfect retrievers and language...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the purpose of the SELF-RAG framework?</td>\n",
       "      <td>What does the SELF-RAG framework help LLMs with?</td>\n",
       "      <td>This work introduces SELF-RAG, a new framework...</td>\n",
       "      <td>The SELF-RAG framework helps LLMs (Language Mo...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the average improvement in performance...</td>\n",
       "      <td>What does PRCA do to retrievers and generators...</td>\n",
       "      <td>The inclusion of PRCA improves performance in ...</td>\n",
       "      <td>PRCA improves the performance of retrievers an...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Which film came out first, Blind Shaft or The ...</td>\n",
       "      <td>Which movie was released first, Blind Shaft or...</td>\n",
       "      <td>1. Blind Shaft came out in 2003.\\n2. The Mask ...</td>\n",
       "      <td>The Mask Of Fu Manchu was released first.</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the purpose of the CORE framework?</td>\n",
       "      <td>What's the point of the CORE framework?</td>\n",
       "      <td>- Counterfactual data augmentation (CDA) helps...</td>\n",
       "      <td>The point of the CORE framework is to create d...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the proposed framework in the research...</td>\n",
       "      <td>When and where was Emilie HeghArntzen's mom born?</td>\n",
       "      <td>Emilie HeghArntzen was born on January 1, 1994...</td>\n",
       "      <td>The information from the given context does no...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Did Spiderman fight against Falcon in the MCU?...</td>\n",
       "      <td>Did Spiderman and Falcon ever meet in the Marv...</td>\n",
       "      <td>No relevant sentences were provided in the con...</td>\n",
       "      <td>Based on the given context, it is not possible...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is the impact of using Hindi retrieval da...</td>\n",
       "      <td>How does using Hindi retrieval data affect the...</td>\n",
       "      <td>Comparing the results in Table 7 with the prev...</td>\n",
       "      <td>Using Hindi retrieval data generally improves ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is the Personalized Accuracy of the GPT-3...</td>\n",
       "      <td>How accurate is the GPT-3.5 Summ. model compar...</td>\n",
       "      <td>Table 3: Results for FlanT5-base model fine-tu...</td>\n",
       "      <td>The GPT-3.5 Summ. model is more accurate than ...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is the architecture of the backbone LM us...</td>\n",
       "      <td>Can you break down how the backbone LM is stru...</td>\n",
       "      <td>Specifically, we propose a variant of the mode...</td>\n",
       "      <td>In the RETRO model for open-domain QA, the bac...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What was the accuracy of ChatGPT in noise robu...</td>\n",
       "      <td>How well does ChatGPT perform in noisy environ...</td>\n",
       "      <td>ChatGPT performs at 90.00% accuracy in noisy e...</td>\n",
       "      <td>ChatGPT performs at 90.00% accuracy in noisy e...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What is the purpose of the fact checking modul...</td>\n",
       "      <td>What does the fact checking module in RETA-LLM...</td>\n",
       "      <td>Finally, RETA-LLM uses the fact checking modul...</td>\n",
       "      <td>The fact checking module in RETA-LLM uses the ...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What is the number of test samples in the QReC...</td>\n",
       "      <td>How big are the sample sizes in the QReCC and ...</td>\n",
       "      <td>QReCC has a sample size of 29596.\\nToolBench h...</td>\n",
       "      <td>The sample size in the QReCC dataset is 29596,...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is the effect of presence or absence of e...</td>\n",
       "      <td>How does having evidence or a history of conve...</td>\n",
       "      <td>- Presence or absence of the evidence and/or d...</td>\n",
       "      <td>Having evidence or a history of conversation h...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What is the proposed framework for enhancing t...</td>\n",
       "      <td>So, what's the plan for making small language ...</td>\n",
       "      <td>- \"To overcome this limitation, we propose the...</td>\n",
       "      <td>The plan for making small language models bett...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What are the limitations of LLMs in terms of s...</td>\n",
       "      <td>What can LLMs not do when it comes to summariz...</td>\n",
       "      <td>Furthermore, LLMs lack the ability to summariz...</td>\n",
       "      <td>LLMs cannot summarize information from multipl...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What are the different aspects of Carlos Moeda...</td>\n",
       "      <td>Can you tell me about Carlos Moedas' backgroun...</td>\n",
       "      <td>Carlos Moedas is a Portuguese politician and e...</td>\n",
       "      <td>Carlos Moedas is a Portuguese politician and e...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>What is the effect of changing the weighting t...</td>\n",
       "      <td>What happens if we change the weighting term f...</td>\n",
       "      <td>- \"We analyze the effects of different paramet...</td>\n",
       "      <td>If we change the weighting term for ISSUP on t...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What is the overall performance of retriever-a...</td>\n",
       "      <td>How well do retriever-augmented language model...</td>\n",
       "      <td>- We keep the data samples that include at lea...</td>\n",
       "      <td>Retriever-augmented language models perform di...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What is the purpose of the rewriter optimizati...</td>\n",
       "      <td>What does the rewriter optimization do in rein...</td>\n",
       "      <td>The rewriter optimization in reinforcement lea...</td>\n",
       "      <td>The rewriter optimization in reinforcement lea...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What is the purpose of SELF-RAG?</td>\n",
       "      <td>What does SELF-RAG actually do?</td>\n",
       "      <td>SELF-RAG learns to retrieve, critique, and gen...</td>\n",
       "      <td>SELF-RAG is a model that learns to retrieve, c...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What is the strategy introduced to estimate Rt...</td>\n",
       "      <td>What's the new strategy for estimating Rt in t...</td>\n",
       "      <td>- Our objective is twofold: maximizing generat...</td>\n",
       "      <td>The new strategy for estimating Rt in the PRCA...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What is the impact of using Hindi retrieval da...</td>\n",
       "      <td>How does using Hindi retrieval data affect the...</td>\n",
       "      <td>Comparing the results in Table 7 with the prev...</td>\n",
       "      <td>Using Hindi retrieval data generally improves ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>What is the performance of FLARE with respect ...</td>\n",
       "      <td>How does the retrieval percentage affect FLARE...</td>\n",
       "      <td>- \"On 2WikiMultihopQA, the performance plateau...</td>\n",
       "      <td>The retrieval percentage affects FLARE perform...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>What techniques have been proposed for the aut...</td>\n",
       "      <td>How does CORE come up with alternative edits?</td>\n",
       "      <td>- Retrieval-augmented models learn to search o...</td>\n",
       "      <td>CORE comes up with alternative edits by using ...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What is the proposed method for fitting black-...</td>\n",
       "      <td>How does the PRCA framework suggest fitting bl...</td>\n",
       "      <td>- \"To tackle this issue and further improve Re...</td>\n",
       "      <td>The PRCA framework suggests fitting black-box ...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>What are the limitations of retrievers in reas...</td>\n",
       "      <td>What can retrievers not do well when it comes ...</td>\n",
       "      <td>- In this task, the model should assign a high...</td>\n",
       "      <td>Retrievers cannot perform reasoning tasks well.</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        seed_question  \\\n",
       "0   What is an example of a situation where you ha...   \n",
       "1   What is the accuracy of I TER-RETGEN compared ...   \n",
       "2   What is the number of train samples in the QRe...   \n",
       "3   What is the impact of combining language model...   \n",
       "4      What is the purpose of the SELF-RAG framework?   \n",
       "5   What is the average improvement in performance...   \n",
       "6   Which film came out first, Blind Shaft or The ...   \n",
       "7          What is the purpose of the CORE framework?   \n",
       "8   What is the proposed framework in the research...   \n",
       "9   Did Spiderman fight against Falcon in the MCU?...   \n",
       "10  What is the impact of using Hindi retrieval da...   \n",
       "11  What is the Personalized Accuracy of the GPT-3...   \n",
       "12  What is the architecture of the backbone LM us...   \n",
       "13  What was the accuracy of ChatGPT in noise robu...   \n",
       "14  What is the purpose of the fact checking modul...   \n",
       "15  What is the number of test samples in the QReC...   \n",
       "16  What is the effect of presence or absence of e...   \n",
       "17  What is the proposed framework for enhancing t...   \n",
       "18  What are the limitations of LLMs in terms of s...   \n",
       "19  What are the different aspects of Carlos Moeda...   \n",
       "20  What is the effect of changing the weighting t...   \n",
       "21  What is the overall performance of retriever-a...   \n",
       "22  What is the purpose of the rewriter optimizati...   \n",
       "23                   What is the purpose of SELF-RAG?   \n",
       "24  What is the strategy introduced to estimate Rt...   \n",
       "25  What is the impact of using Hindi retrieval da...   \n",
       "26  What is the performance of FLARE with respect ...   \n",
       "27  What techniques have been proposed for the aut...   \n",
       "28  What is the proposed method for fitting black-...   \n",
       "29  What are the limitations of retrievers in reas...   \n",
       "\n",
       "                                             question  \\\n",
       "0   When should a manager pass on tasks to their t...   \n",
       "1   How well do I TER-RETGEN and DSP work in answe...   \n",
       "2   How many train samples and how big is the QReC...   \n",
       "3   What happens when you mix imperfect retrievers...   \n",
       "4    What does the SELF-RAG framework help LLMs with?   \n",
       "5   What does PRCA do to retrievers and generators...   \n",
       "6   Which movie was released first, Blind Shaft or...   \n",
       "7             What's the point of the CORE framework?   \n",
       "8   When and where was Emilie HeghArntzen's mom born?   \n",
       "9   Did Spiderman and Falcon ever meet in the Marv...   \n",
       "10  How does using Hindi retrieval data affect the...   \n",
       "11  How accurate is the GPT-3.5 Summ. model compar...   \n",
       "12  Can you break down how the backbone LM is stru...   \n",
       "13  How well does ChatGPT perform in noisy environ...   \n",
       "14  What does the fact checking module in RETA-LLM...   \n",
       "15  How big are the sample sizes in the QReCC and ...   \n",
       "16  How does having evidence or a history of conve...   \n",
       "17  So, what's the plan for making small language ...   \n",
       "18  What can LLMs not do when it comes to summariz...   \n",
       "19  Can you tell me about Carlos Moedas' backgroun...   \n",
       "20  What happens if we change the weighting term f...   \n",
       "21  How well do retriever-augmented language model...   \n",
       "22  What does the rewriter optimization do in rein...   \n",
       "23                    What does SELF-RAG actually do?   \n",
       "24  What's the new strategy for estimating Rt in t...   \n",
       "25  How does using Hindi retrieval data affect the...   \n",
       "26  How does the retrieval percentage affect FLARE...   \n",
       "27      How does CORE come up with alternative edits?   \n",
       "28  How does the PRCA framework suggest fitting bl...   \n",
       "29  What can retrievers not do well when it comes ...   \n",
       "\n",
       "                                              context  \\\n",
       "0   - Delegation–Delegation is the assignment of a...   \n",
       "1   - \"When increasing the number of iterations fo...   \n",
       "2         QReCC dataset [3] has 29,596 train samples.   \n",
       "3   - \"This section explores how the combination o...   \n",
       "4   This work introduces SELF-RAG, a new framework...   \n",
       "5   The inclusion of PRCA improves performance in ...   \n",
       "6   1. Blind Shaft came out in 2003.\\n2. The Mask ...   \n",
       "7   - Counterfactual data augmentation (CDA) helps...   \n",
       "8   Emilie HeghArntzen was born on January 1, 1994...   \n",
       "9   No relevant sentences were provided in the con...   \n",
       "10  Comparing the results in Table 7 with the prev...   \n",
       "11  Table 3: Results for FlanT5-base model fine-tu...   \n",
       "12  Specifically, we propose a variant of the mode...   \n",
       "13  ChatGPT performs at 90.00% accuracy in noisy e...   \n",
       "14  Finally, RETA-LLM uses the fact checking modul...   \n",
       "15  QReCC has a sample size of 29596.\\nToolBench h...   \n",
       "16  - Presence or absence of the evidence and/or d...   \n",
       "17  - \"To overcome this limitation, we propose the...   \n",
       "18  Furthermore, LLMs lack the ability to summariz...   \n",
       "19  Carlos Moedas is a Portuguese politician and e...   \n",
       "20  - \"We analyze the effects of different paramet...   \n",
       "21  - We keep the data samples that include at lea...   \n",
       "22  The rewriter optimization in reinforcement lea...   \n",
       "23  SELF-RAG learns to retrieve, critique, and gen...   \n",
       "24  - Our objective is twofold: maximizing generat...   \n",
       "25  Comparing the results in Table 7 with the prev...   \n",
       "26  - \"On 2WikiMultihopQA, the performance plateau...   \n",
       "27  - Retrieval-augmented models learn to search o...   \n",
       "28  - \"To tackle this issue and further improve Re...   \n",
       "29  - In this task, the model should assign a high...   \n",
       "\n",
       "                                               answer  question_type  \\\n",
       "0   A manager should pass on tasks to their team f...    conditional   \n",
       "1   ITER-RETGEN works well in answering multi-hop ...    conditional   \n",
       "2   The QReCC dataset for Conversational Search ha...    conditional   \n",
       "3   When you mix imperfect retrievers and language...      reasoning   \n",
       "4   The SELF-RAG framework helps LLMs (Language Mo...    conditional   \n",
       "5   PRCA improves the performance of retrievers an...    conditional   \n",
       "6           The Mask Of Fu Manchu was released first.         simple   \n",
       "7   The point of the CORE framework is to create d...         simple   \n",
       "8   The information from the given context does no...  multi_context   \n",
       "9   Based on the given context, it is not possible...      reasoning   \n",
       "10  Using Hindi retrieval data generally improves ...         simple   \n",
       "11  The GPT-3.5 Summ. model is more accurate than ...      reasoning   \n",
       "12  In the RETRO model for open-domain QA, the bac...    conditional   \n",
       "13  ChatGPT performs at 90.00% accuracy in noisy e...         simple   \n",
       "14  The fact checking module in RETA-LLM uses the ...    conditional   \n",
       "15  The sample size in the QReCC dataset is 29596,...      reasoning   \n",
       "16  Having evidence or a history of conversation h...         simple   \n",
       "17  The plan for making small language models bett...  multi_context   \n",
       "18  LLMs cannot summarize information from multipl...         simple   \n",
       "19  Carlos Moedas is a Portuguese politician and e...      reasoning   \n",
       "20  If we change the weighting term for ISSUP on t...         simple   \n",
       "21  Retriever-augmented language models perform di...         simple   \n",
       "22  The rewriter optimization in reinforcement lea...         simple   \n",
       "23  SELF-RAG is a model that learns to retrieve, c...         simple   \n",
       "24  The new strategy for estimating Rt in the PRCA...         simple   \n",
       "25  Using Hindi retrieval data generally improves ...         simple   \n",
       "26  The retrieval percentage affects FLARE perform...      reasoning   \n",
       "27  CORE comes up with alternative edits by using ...      reasoning   \n",
       "28  The PRCA framework suggests fitting black-box ...      reasoning   \n",
       "29    Retrievers cannot perform reasoning tasks well.         simple   \n",
       "\n",
       "    episode_done evolution_elimination  \n",
       "0           True                 False  \n",
       "1           True                  True  \n",
       "2           True                 False  \n",
       "3          False                  True  \n",
       "4           True                  True  \n",
       "5           True                 False  \n",
       "6           True                  None  \n",
       "7           True                  None  \n",
       "8           True                 False  \n",
       "9           True                 False  \n",
       "10          True                  None  \n",
       "11         False                 False  \n",
       "12         False                 False  \n",
       "13          True                  None  \n",
       "14          True                  True  \n",
       "15          True                 False  \n",
       "16          True                  None  \n",
       "17          True                  True  \n",
       "18          True                  None  \n",
       "19          True                  True  \n",
       "20          True                  None  \n",
       "21          True                  None  \n",
       "22          True                  None  \n",
       "23          True                  None  \n",
       "24          True                  None  \n",
       "25          True                  None  \n",
       "26          True                 False  \n",
       "27          True                 False  \n",
       "28          True                  True  \n",
       "29          True                  None  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d58feb8-8604-4854-9d1d-7aa27495f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_pandas().to_csv(\"arxiv_retrieval_v3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03897e",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Conditional question evol is working\n",
    "- reasoning/multi context are not working as expected\n",
    "- Almost all questions are closed endeded \n",
    "- Almost all questions start with \"What\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e52f7",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "144547a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file.markdown_reader import MarkdownReader\n",
    "from llama_index.schema import Document\n",
    "from typing import List, Dict, Optional\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1e30ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagasMdReader(MarkdownReader):\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_file_metadata(path):\n",
    "        \n",
    "        return {\"filename\":os.path.basename(path),\n",
    "                \"dirname\":os.path.dirname(path)}\n",
    "        \n",
    "    \n",
    "    def get_local_metadata(self, text):\n",
    "        \n",
    "\n",
    "        return_dict = {}\n",
    "        pattern = r'---\\s*title:\\s*(.+?)(?:\\s*description:\\s*\"(.*?)\")?\\s*---'\n",
    "        match = re.findall(pattern, text)\n",
    "        if match:\n",
    "            title,desc = match[0]\n",
    "            return_dict['title'] = title\n",
    "            return_dict['description'] = desc if desc else None\n",
    "            \n",
    "        return return_dict\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    def load_data(\n",
    "        self, file: Path, extra_info: Optional[Dict] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Parse file into string.\"\"\"\n",
    "        \n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        if self._remove_hyperlinks:\n",
    "            content = self.remove_hyperlinks(content)\n",
    "        if self._remove_images:\n",
    "            content = self.remove_images(content)\n",
    "            \n",
    "        local_metadata = self.get_local_metadata(content)\n",
    "\n",
    "        extra_info = dict(extra_info,**local_metadata) if local_metadata else {}\n",
    "        return [Document(text=content,metadata=extra_info)]\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f9c01902",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_reader = RagasMdReader(remove_hyperlinks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bca9d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm2(prompt, **kwargs):\n",
    "    response = client.chat.completions.create(\n",
    "        model=kwargs.get(\"model\", \"gpt-3.5-turbo\"),\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=kwargs.get(\"temperature\", 0),\n",
    "        top_p=kwargs.get(\"top_p\", 1),\n",
    "        frequency_penalty=kwargs.get(\"frequency_penalty\", 0.0),\n",
    "        presence_penalty=kwargs.get(\"presence_penalty\", 0.0),\n",
    "        max_tokens=kwargs.get(\"max_tokens\", 500),\n",
    "        n=kwargs.get(\"n\", 1),\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feec19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.prompts import SEED_QUESTION, EVOLUTION_ELIMINATION, REWRITE_QUESTION, FILTER_QUESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "95d00bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [\n",
    "    \"/Users/shahules/Myprojects/rag-experiments/gitlab-handbook/data/handbook/leadership/\",\n",
    "    \"/Users/shahules/Myprojects/rag-experiments/gitlab-handbook/data/handbook/company/\",\n",
    "]\n",
    "documents = []\n",
    "for dir_path in dirs:\n",
    "    loader = SimpleDirectoryReader(dir_path, \n",
    "                                   recursive=True,\n",
    "                                  file_extractor={\".md\":md_reader},\n",
    "                                file_metadata=RagasMdReader.get_file_metadata)\n",
    "    \n",
    "    documents.extend(loader.load_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "64e715b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding SOCIAL STYLES\n"
     ]
    }
   ],
   "source": [
    "print(documents[14].metadata['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "15e60505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "---\n",
    "title: \"Ask Me Anything\"\n",
    "description: \"Learn and ask questions at GitLab's Ask Me Anything (AMA) meetings\"\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "# Regular expression to capture title and description\n",
    "pattern_optional_description = '---\\s*title:\\s*(.+?)(?:\\s*description:\\s*\"(.*?)\")?\\s*---'\n",
    "\n",
    "# Finding matches\n",
    "matches = re.match(pattern_optional_description, text)\n",
    "\n",
    "matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "08037476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4cc57d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile('---\\s*title:\\s*(.+?)(?:\\s*description:\\s*\"(.*?)\")?\\s*---')\n",
    "[m.groupdict() for m in pattern.finditer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58b4cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "---\n",
    "title: \"Book clubs\"\n",
    "---\n",
    "\n",
    "From time to time, we run internal book clubs on a book from one of our resource lists. All are welcome! However,\n",
    "each club has a suggested audience to indicate roles to which the content is tailored.\n",
    "\n",
    "- [Leadership]({{< ref \"_index.md#books\" >}})\n",
    "- [Development](https://about.gitlab.com/handbook/engineering/development/#books)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3a953420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_optional_description = r'---\\s*title:\\s*\"(.+?)\"\\s*---'\n",
    "re.findall(pattern,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64f4ad42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b94cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4bc085c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testset = testsetgenerator.generate(documents, test_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26f40412",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.to_pandas().to_csv(\"gitlab_communication_company_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ae48ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92931c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1=\"How should I use my notification settings in Slack?\"\n",
    "q2=\"What's the best way to manage Slack notification settings for efficient communication and minimal disruptions?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1052549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = EVOLUTION_ELIMINATION.prompt.template.format(question1=q1, question2=q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37bedda",
   "metadata": {},
   "source": [
    "## Create multi context\n",
    "- find similar docs using metadata \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a0773b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index.indices.query.embedding_utils import get_top_k_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bac7c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "41792a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [doc.metadata.get('title').strip('\"') for doc in documents if doc.metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "165254ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.embed_documents(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "064ef79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "19d68a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, indices = get_top_k_embeddings(embeddings[k],embeddings,similarity_cutoff=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ac6d8408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seach query :Top Cross-Functional Initiatives\n",
      " Results: [['Top Cross-Functional Initiatives', 'Building High Performing Teams', 'Product Career Development Framework Working Group', 'Leadership', 'Single Codebase Working Group']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Seach query :{titles[k]}\\n Results: [{[titles[i] for i in indices[:5]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4c9938",
   "metadata": {},
   "source": [
    "## Merge documents based on meta-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0695effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import HumanMessagePromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2f28fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITE_QUESTION = HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "Rewrite the given question so that it can be answered without context.\n",
    "\n",
    "Question: When was he born?\n",
    "Context : Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time. He was born on 14 March 1879.\n",
    "Rewritten question: When was Albert Einstein born? \n",
    "\n",
    "Context: A clothes iron (also flatiron, smoothing iron, or simply iron) is a small appliance that, when heated, is used to press clothes to remove wrinkles and unwanted creases. Domestic irons generally range in operating temperature from between 121 °C (250 °F) to 182 °C (360 °F). It is named for the metal (iron) of which the device was historically made, and the use of it is generally called ironing, the final step in the process of laundering clothes.\n",
    "Question: What is temperate range of the device?\n",
    "Rewritten question: What is temperate range of clothes iron?\n",
    "\n",
    "\n",
    "Question:{question}\n",
    "Context: {context}\n",
    "Rewritten question:\n",
    "\"\"\")\n",
    "\n",
    "QUERY =  HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "Rewrite the question using given context so that it can be read and answered without any extra information.\n",
    "\n",
    "Question:{question}\n",
    "Context: {context}\n",
    "\"\"\")\n",
    "\n",
    "text = \"\"\"\n",
    "The Mona Lisa, painted by Leonardo da Vinci in the early 16th century, is one of the most famous and valuable paintings in the world.\n",
    "Known for its enigmatic expression and innovative use of sfumato, the painting has become a symbol of Renaissance art.\n",
    "The Mona Lisa is displayed in the Louvre Museum in Paris and attracts millions of visitors annually.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fc666f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(indices):\n",
    "    \n",
    "    return '\\n'.join([documents[idx].get_content() for idx in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d871744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "455779c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm2(SEED_QUESTION.format(context=get_content([49,50])).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3baa48ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the purpose of the GPT-3 Editor in the CORE framework?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = response.choices[0].message.content\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2839379b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rewrite the question using given context so that it can be read and answered without any extra information.\n",
      "\n",
      "Question:What is the impact of training data size on the model's performance?\n",
      "Context: Preprint.\n",
      "Table 2: Overall experiment results on six tasks. Bold numbers indicate the best performance among\n",
      "non-proprietary models, and gray-colored bold text indicates the best proprietary model when\n",
      "they outperforms all non-proprietary models.∗indicates concurrent or recent results reported by\n",
      "concurrent work. – indicates numbers that are not reported by the original papers or are not applicable.\n",
      "Models are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em,\n",
      "rouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\n",
      "Short-form Closed-set Long-form generations (with citations)\n",
      "PopQA TQA Pub ARC Bio ASQA\n",
      "LM (acc) (acc) (acc) (acc) (FS) (em) (rg) (mau) (pre) (rec)\n",
      "LMs with proprietary data\n",
      "Llama2-c 13B 20.0 59.3 49.4 38.4 55.9 22.4 29.6 28.6 – –\n",
      "Ret-Llama2-c 13B 51.8 59.8 52.1 37.9 79.9 32.8 34.8 43.8 19.8 36.1\n",
      "ChatGPT 29.3 74.3 70.1 75.3 71.8 35.3 36.2 68.8 – –\n",
      "Ret-ChatGPT 50.8 65.7 54.7 75.3 – 40.7 39.9 79.7 65.1 76.6\n",
      "Perplexity.ai – – – – 71.2 – – – – –\n",
      "Baselines without retrieval\n",
      "Llama2 7B 14.7 30.5 34.2 21.8 44.5 7.9 15.3 19.0 – –\n",
      "Alpaca 7B 23.6 54.5 49.8 45.0 45.8 18.8 29.4 61.7 – –\n",
      "Llama2 13B 14.7 38.5 29.4 29.4 53.4 7.2 12.4 16.0 – –\n",
      "Alpaca 13B 24.4 61.3 55.5 54.9 50.2 22.9 32.0 70.6 – –\n",
      "CoVE 65B* – – – – 71.2 – – – – –\n",
      "Baselines with retrieval\n",
      "Toolformer* 6B – 48.8 – – – – – – – –\n",
      "Llama2 7B 38.2 42.5 30.0 48.0 78.0 15.2 22.1 32.0 2.9 4.0\n",
      "Alpaca 7B 46.7 64.1 40.2 48.0 76.6 30.9 33.3 57.9 5.5 7.2\n",
      "Llama2-FT 7B 48.7 57.3 64.3 65.8 78.2 31.0 35.8 51.2 5.0 7.5\n",
      "SAIL* 7B – – 69.2 48.4 – – – – – –\n",
      "Llama2 13B 45.7 47.0 30.2 26.0 77.5 16.3 20.5 24.7 2.3 3.6\n",
      "Alpaca 13B 46.1 66.9 51.1 57.6 77.7 34.8 36.7 56.6 2.0 3.8\n",
      "Our SELF-RAG 7B 54.9 66.4 72.4 67.3 81.2 30.0 35.7 74.3 66.9 67.8\n",
      "Our SELF-RAG 13B 55.8 69.3 74.5 73.1 80.2 31.7 37.0 71.6 70.3 71.3\n",
      "5 R ESULTS AND ANALYSIS\n",
      "5.1 M AINRESULTS\n",
      "Comparison against baselines without retrieval. Table 2 (top) presents the baselines without\n",
      "retrieval. Our SELF-RAG(bottom two rows) demonstrates a substantial performance advantage\n",
      "over supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA,\n",
      "biography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms\n",
      "a concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation\n",
      "task, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which\n",
      "iteratively prompts Llama2 65Bto refine output.\n",
      "Comparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF-RAGalso\n",
      "outperforms existing RAG in many tasks, obtaining the best performance among non-proprietary\n",
      "LM-based models on all tasks. While our method outperforms other baselines, on PopQA or Bio,\n",
      "powerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca) show large gains from\n",
      "their non-retrieval baselines. However, we found that these baselines provide limited solutions for\n",
      "tasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth\n",
      "and ARC-Challenge, baselines with retrieval do not improve performance notably from their no-\n",
      "retrieval counterparts. We also observe that most baselines with retrieval struggle to improve citation\n",
      "accuracy. On ASQA, our model shows significantly higher citation precision and recall than all\n",
      "models except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits superior efficacy\n",
      "in this particular task, surpassing smaller LMs. Our SELF-RAGbridges this performance gap, even\n",
      "outperforming ChatGPT in citation precision, which measures whether the model-generated claim is\n",
      "fully supported by cited evidence. We also found that on the metrics for factual precision, SELF-RAG\n",
      "7B occasionally outperforms our 13B due to the tendency of smaller SELF-RAGto often generate\n",
      "8\n",
      "Preprint.\n",
      "PQA Med AS\n",
      "(acc) (acc) (em)\n",
      "SELF-RAG(50k) 45.5 73.5 32.1\n",
      "Training\n",
      "No Retriever R 43.6 67.8 31.0\n",
      "No Critic C 42.6 72.0 18.1\n",
      "Test\n",
      "No retrieval 24.7 73.0 –\n",
      "Hard constraints 28.3 72.6 –\n",
      "Retrieve top1 41.8 73.1 28.6\n",
      "Remove ISSUP 44.1 73.2 30.6\n",
      "(a) Ablation\n",
      "1 270.070.5Precision\n",
      "1 2\n",
      "Weight for IsSupport9095Mauve\n",
      " (b) Customization\n",
      "0.0 0.2 0.4 0.60.980.990.991.00Accuracy\n",
      "PubHealth\n",
      "0.0 0.2 0.4 0.6\n",
      "Retrieval Threshold0.60.81.0AccuracyPopQA0.00.51.0\n",
      "Frequency\n",
      "0.250.500.751.00\n",
      "Frequency\n",
      " (c) Retrieval\n",
      "Figure 3: Analysis on SELF-RAG:(a)Ablation studies for key components of SELF-RAGtraining\n",
      "and inference based on our 7B model. (b) Effects of soft weights on ASQA citation precision and\n",
      "Mauve (fluency). (c) Retrieval frequency andnormalized accuracy on PubHealth and PopQA.\n",
      "precisely grounded yet shorter outputs. Llama2-FT 7B, which is the baseline LM trained on the same\n",
      "instruction-output pairs as SELF-RAGwithout retrieval or self-reflection and is retrieval-augmented\n",
      "at test time only, lags behind S ELF-RAG. This result indicates S ELF-RAGgains are not solely from\n",
      "training data and demonstrate the effectiveness of S ELF-RAGframework.\n",
      "5.2 A NALYSIS\n",
      "Ablation studies. We conduct a set of ablations of our framework to identify which factors play\n",
      "key roles. We evaluate two model variants trained differently than our model: No Retriever trains an\n",
      "LM using the standard instruction-following method given instruction-output pairs, without retrieved\n",
      "passages; No Critic trains an LM trained with input-output pairs that are always augmented with the\n",
      "top one retrieved document without reflection tokens. This is similar to SAIL (Luo et al., 2023), and\n",
      "we use our instruction-output data instead of using the Alpaca dataset (Dubois et al., 2023), as in\n",
      "SAIL. We also conduct ablation on our inference-time algorithm, including No retrieval disables\n",
      "retrieval during inference; Hard constraints indicates the model performance that retrieves when\n",
      "Retrieve =Yes instead of using the adaptive threshold; Retrieve top 1 always retrieves and uses the\n",
      "top one document only, similar to standard RAG approaches; Remove ISSUPindicates the model\n",
      "performance that removes ISSUPscore only during critique-guided beam search in Eq. 4. In this\n",
      "ablation experiment, we use a training instance size of 50k for a more efficient exploration of training\n",
      "variations. Later in this section, we conduct an analysis of the effect of training data size. We conduct\n",
      "the ablation studies on three datasets, PopQA, PubHealth, and ASQA. On ASQA, we evaluate models\n",
      "on sampled 150 instances and exclude ablations involving adaptive or no retrieval processes.\n",
      "We show in Table 3a the ablation results. The top part of the table shows results for training ablations,\n",
      "and the bottom part is for inference ablations. We see that all components play important roles. We\n",
      "also observe a large performance gap between SELF-RAGand No Retriever or Critic baselines across\n",
      "tasks, indicating that training an LM with those models largely contributes to the performance gain of\n",
      "SELF-RAG. Using the top passages regardless of their relevance (Retrieve top 1) as in conventional\n",
      "RAG approaches causes a large drop in PopQA and ASQA, and removing ISSUPduring the beam\n",
      "search results hurts performance on ASQA. This demonstrates the effectiveness of SELF-RAG’s\n",
      "capabilities of carefully selecting generations based fine-grained multiple criterion, instead of naively\n",
      "using all of the top passages from the retrieval model or solely depending on relevance scores.\n",
      "Effects of inference-time customization. One key benefit of our proposed framework is that it\n",
      "enables us to control how much each critique type affects the final generation sampling. We analyze\n",
      "the effects of different parameter weights on the top of our 7B model during inference time on\n",
      "ASQA, where multiple evaluation aspects are considered. Figure 3b shows the effects of changing\n",
      "the weighting term for ISSUP, which criticizes how supported the output is by the text passage. As\n",
      "the figure shows, increasing the weight leads to positive effects on the models’ citation precision\n",
      "since this puts more emphasis on whether model generation is supported by the evidence. On the\n",
      "9\n",
      "Preprint.\n",
      "0 50 100 150\n",
      "Num of training (k)3540455055Perfomance\n",
      "(a) PopQA\n",
      "0 100\n",
      "Num of training (k)717273\n",
      " (b) PubHealth\n",
      "0 100\n",
      "Num of training (k)4060\n",
      " (c) ASQA (prec)Pop Bio.\n",
      "S & P 92.5 70.0\n",
      "ISREL 95.0 90.0\n",
      "ISSUP 90.0 85.0\n",
      "(d) Human evaluation on PopQA\n",
      "and Bio generation.\n",
      "Figure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect\n",
      "of the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)\n",
      "Human analysis on S ELF-RAGoutputs as well as reflection tokens.\n",
      "contrary, a larger weight results in lower MAUVE scores: when generation gets longer and more\n",
      "fluent, there are often more claims that are not fully supported by citations, consistent with findings\n",
      "by Liu et al. (2023a). Our framework lets practitioners choose and customize models’ behaviors at\n",
      "test time by adjusting such parameters without requiring additional training.\n",
      "Efficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval\n",
      "occurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects\n",
      "overall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers\n",
      "of threshold δ(larger δresults in less retrieval) on PubHealth and PopQA. Figure 3c shows that\n",
      "the model’s retrieval frequencies dramatically change on both datasets. as δvaries. On one hand,\n",
      "performance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.\n",
      "Effects of training data size. We conduct an analysis of how the data scale affects the model’s\n",
      "performance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original\n",
      "150k training instances, and fine-tune four SELF-RAG 7Bvariants on those subsets. Then, we compare\n",
      "the model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF-\n",
      "RAGtrained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models’\n",
      "performance trained on different amount of data. Across all datasets, increasing data size often shows\n",
      "upward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do\n",
      "not observed such significant improvements on Llama2-FT 7Bwhen increasing the training data from\n",
      "50k to 150k. These results also indicate that further expanding the training data of SELF-RAGmay\n",
      "lead to further improvements, although in this work we limit our training data size to 150k.\n",
      "Human evaluations. We conduct small human evaluations on SELF-RAGoutputs, as well as the\n",
      "reliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio\n",
      "results. Following Menick et al. (2022), human annotators evaluate S&P , which indicates whether\n",
      "the model output is plausible (i.e., the output is a reasonable and on-topic response to the question\n",
      "as if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to\n",
      "verify the validity of the answer). For S&P, we do not consider the instances where SELF-RAG\n",
      "predicts irrelevant orno support . We then ask our annotators whether the model-predicted\n",
      "reflection tokens about ISRELand ISSUPmatch their inspections (e.g., whether the fully supported\n",
      "output is supported by the cited evidence). Human annotators find SELF-RAGanswers are often\n",
      "plausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is\n",
      "consistent with Menick et al. (2022). Human annotators also find ISRELand ISSUPreflection token\n",
      "predictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated\n",
      "examples and explanations on assessments.\n",
      "6 C ONCLUSION\n",
      "This work introduces SELF-RAG, a new framework to enhance the quality and factuality of LLMs\n",
      "through retrieval on demand and self-reflection. SELF-RAGtrains an LM to learn to retrieve, generate,\n",
      "and critique text passages and its own generation by predicting the next tokens from its original\n",
      "vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables\n",
      "the tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on\n",
      "six tasks using multiple metrics demonstrate that SELF-RAGsignificantly outperforms LLMs with\n",
      "more parameters or with conventional retrieval-augmented generation approaches.\n",
      "10\n",
      "\n",
      "Rewritten question: How does the size of the training data affect the performance of the SELF-RAG model according to the experiment results?\n"
     ]
    }
   ],
   "source": [
    "print(QUERY.format(question=q, context=get_content([12,13,14])).content)\n",
    "print('Rewritten question: How does the size of the training data affect the performance of the SELF-RAG model according to the experiment results?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fd1f148b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1909"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_content([12,13,14]).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3bad307a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimuli BMS [41] Pan et al. [29] Salicon [15] ML-Net [9] Ours Ground Truth\n",
      "Figure 3: Qualitative performance comparison .\n",
      "SateliteJumbledSocial\n",
      "OutdoorNaturalArt\n",
      "InvertedFractalObjectCartoon\n",
      "OutdoorManMadeIndoorActionPatternNoisy\n",
      "Affective\n",
      "BlackWhiteRandom\n",
      "LowResolutionLineDrawingSketchSketchLineDrawingLowResolutionRandomBlackWhiteAffectiveNoisyPatternActionIndoorOutdoorManMadeCartoonObjectFractalInvertedArtOutdoorNaturalSocialJumbledSatelite\n",
      "0.00.20.40.60.81.0\n",
      "Figure 4: Classiﬁcation confusion matrix .\n",
      "bias. Location bias is an important component of human\n",
      "visual attention, and so should be included in any model.\n",
      "Table 2 shows the results of the 5-fold cross validation\n",
      "tests on the CAT2000 dataset. We achieve the best perfor-\n",
      "mance of the tested models. Table 3 shows the performance\n",
      "on the held out test images. Our model achieves the second\n",
      "best performance behind the DeepFix model [22]. Com-\n",
      "pared with DeepFix our network is shallower with less pa-\n",
      "rameters. The DeepFix model is not publicly available to\n",
      "test on the cross-validation data. Figure 3 shows example\n",
      "saliency maps generated by the tested models.\n",
      "Table 4 shows the results on the Salicon test set. Under\n",
      "this dataset our model does not have a performance advan-CC SAUC AUC Judd\n",
      "Our Model 0.730 0.771 0.861\n",
      "ML-Net [9] 0.7430 0.7680 0.8660\n",
      "Deep Convnet [29] 0.6220 0.7240 0.8580\n",
      "Shallow Convnet [29] 0.5957 0.6698 0.8364\n",
      "Rare 2012 Improved [31] 0.5108 0.6644 0.8148\n",
      "Baseline: BMS [41] 0.4268 0.6935 0.7899\n",
      "Baseline: GBVS [12] 0.4212 0.6303 0.7899\n",
      "Baseline: Itti [16] 0.2046 0.6101 0.6669\n",
      "Table 4: Salicon test set results.\n",
      "tage over ML-Net. This is largely because the dataset does\n",
      "not have diverse categories like the CAT-2000 dataset. Nev-\n",
      "ertheless, the performance of our model is roughly equiva-\n",
      "lent to the ML-net model.\n",
      "Comparison with averaging ensemble Given that the\n",
      "mixture model can be seen as an ensemble, it is important\n",
      "to compare the performance with a vanilla ensemble. We\n",
      "generate an ensemble of 5 networks where each network is\n",
      "trained on a random 80% of the training split. The saliency\n",
      "maps from each ensemble member are averaged to produce\n",
      "the ﬁnal saliency map. Our model still achieves superior\n",
      "performance compared with the ensemble (Table 5). Addi-\n",
      "tionally, our model is signiﬁcantly faster and requires less\n",
      "storage space due to the weight sharing.\n"
     ]
    }
   ],
   "source": [
    "print(get_content([14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8464da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser.simple import SimpleNodeParser\n",
    "node_parser = SimpleNodeParser.from_defaults(\n",
    "            chunk_size=1000, chunk_overlap=0, include_metadata=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec05e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = node_parser.get_nodes_from_documents(\n",
    "            documents=documents\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a6cd93d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '2',\n",
       " 'file_name': '00891b5a3dbbe4dae6b06432c3f335f8.pdf',\n",
       " 'file_path': 'arxiv-papers/00891b5a3dbbe4dae6b06432c3f335f8.pdf',\n",
       " 'creation_date': '2023-11-14',\n",
       " 'last_modified_date': '2023-11-14',\n",
       " 'last_accessed_date': '2023-11-14'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "03315c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0c61a824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fe3e30",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c5cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e60c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/shahules/Downloads/gitlab_communication_company_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bbae5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'seed_question', 'question', 'context', 'answer',\n",
       "       'question_type', 'episode_done', 'evolution_elimination'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a7cafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(['Unnamed: 0','seed_question','evolution_elimination'],axis=1)\n",
    "df['question'] = df['question'].apply(lambda x : x.replace(\"Casual Rewrite:\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d32ae2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"gitlab_company_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61f0af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6b58dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6fe6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What is the percentage of repetition in the generations produced by RETRO compared to GPT across different sizes?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b5ebe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm2(FILTER_QUESTION.format(question=question).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b62da0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8LW6aipBvq4TAhWnzMXKmova3FxRX', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='{\"reason\": \"The question mentions \\'RETO\\' and \\'GPT\\' which may require additional context to understand\", \"verdict\": \"No\"}', role='assistant', function_call=None, tool_calls=None))], created=1700138992, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=31, prompt_tokens=160, total_tokens=191))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5119d147",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "ragas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
