{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5be51b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34105067-eed8-4f09-913f-7043ff77cb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ragas/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b1990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader, SimpleDirectoryReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "321dbebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7457125d",
   "metadata": {},
   "source": [
    "## Assessing current state of test date generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ce32d303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shahules/belar/experimental\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bc5970e-0873-4e06-9b68-3dda31b1d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ArxivReader = download_loader(\"ArxivReader\")\n",
    "\n",
    "loader = ArxivReader() #uses simpledirectory reader under the hood, hence need modification to laod pages properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46684465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = loader.load_data(\"retrieval augmented generation AND large language model\",max_results=20,papers_dir='./arxiv-papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b52b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(\"/Users/shahules/Downloads/AMNESTY/\",num_files_limit=20)\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc3d9cf",
   "metadata": {},
   "source": [
    "## SimpleDirectoryReader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aab30508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4e6aa58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'page_label: I\\nfile_name: AFR4471572023ENGLISH.pdf\\nfile_path: /Users/shahules/Downloads/AMNESTY/AFR4471572023ENGLISH.pdf\\ncreation_date: 2023-11-30\\nlast_modified_date: 2023-11-30\\nlast_accessed_date: 2023-11-30'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].get_metadata_str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b28f535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(documents[14].get_metadata_str())\n",
    "# print(\"CONTENT\")\n",
    "# print(documents[14].get_content())\n",
    "# print(len(documents[14].get_content().split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4139e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5.,  3.,  6., 13., 28., 21., 39., 10.,  5.,  1.]),\n",
       " array([  10. ,  113.1,  216.2,  319.3,  422.4,  525.5,  628.6,  731.7,\n",
       "         834.8,  937.9, 1041. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(doc.get_content().split()) for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df051faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dist =  {\n",
    "    \"simple\": 0.3,\n",
    "    \"reasoning\": 0.25,\n",
    "    \"multi_context\": .2,\n",
    "    \"conditional\": 0.25,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f9c0891",
   "metadata": {},
   "outputs": [],
   "source": [
    "testsetgenerator = TestsetGenerator.from_default(testset_distribution=test_dist,chunk_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "553ac808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shahules/belar/experimental\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "977ab453",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                            | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 450\n",
      "seed question ['What actions should all companies take to reduce emissions?', 'Who should stop financing and investing in new projects that drive fossil fuel expansion?']\n",
      "{'reason': 'The question is too broad and does not specify the type of company or the type of emissions, making it potentially ambiguous and open to multiple interpretations.', 'verdict': 'No'}\n",
      "rewritten question What actions should all companies take to reduce emissions according to the context?\n",
      "{'reason': 'The question is vague and does not specify which context it is referring to.', 'verdict': 'No'}\n",
      "{'reason': 'The question is vague and does not specify who it is referring to, making it unclear and open to multiple interpretations.', 'verdict': 'No'}\n",
      "rewritten question Who should stop financing and investing in new projects that drive fossil fuel expansion, according to the context?\n",
      "{'reason': 'The question is vague and does not specify the context it is referring to, making it unclear and potentially unanswerable.', 'verdict': 'No'}\n",
      "Len of text chunks 3 300\n",
      "Len of text chunks 2 374\n",
      "seed question ['What impact has the USA Supreme Court ruling on abortion had on women and girls?', 'Who has expressed fear about the USA Supreme Court ruling on abortion impacting other countries?']\n",
      "{'reason': 'The question is specific and refers to a well-known legal event, making it clear and answerable.', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is specific and refers to a well-known legal event, making it clear and answerable.', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|██▌                                                 | 1/20 [00:48<15:13, 48.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': \"The question is too vague as it does not specify who the 'who' is referring to. It could be a person, a group, a country, etc.\", 'verdict': 'No'}\n",
      "rewritten question Who has expressed fear about the USA Supreme Court ruling on abortion impacting other countries?\n",
      "{'reason': \"The question is too vague as it does not specify who the 'who' is referring to. It could be a person, a group, a country, etc.\", 'verdict': 'No'}\n",
      "Len of text chunks 3 482\n",
      "seed question ['What is the role of fossil fuel companies in driving global warming?', 'Who are the largest emitters of GHG emissions in the Americas?']\n",
      "{'reason': 'The question is specific and refers to a well-known issue, making it clear and answerable.', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is specific and refers to a well-known database, making it clear and answerable.', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|███████▊                                            | 3/20 [01:28<07:46, 27.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question is specific and refers to a well-defined topic, making it clear and answerable.', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is specific and refers to a particular database and a specific region, making it clear and answerable.', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███████████████▌                                    | 6/20 [01:48<03:25, 14.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 338\n",
      "Len of text chunks 2 376\n",
      "Len of text chunks 2 414\n",
      "Len of text chunks 2 289\n",
      "seed question ['What did Amnesty International call on its supporters to do in response to the killing of the Ogoni 9?', 'Who did the collective Mujeres Amazónicas Defensoras de la Selva de las Bases frente al Extractivismo call on Ecuador to protect?']\n",
      "{'reason': 'The question is specific and refers to a well-known event and organization, making it clear and answerable.', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is specific and refers to a well-known event and organization, making it clear and answerable.', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████▌                         | 10/20 [07:19<08:33, 51.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question is specific and refers to a particular group and event, making it clear and answerable.', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is vague and does not specify which collective or incident it is referring to.', 'verdict': 'No'}\n",
      "Len of text chunks 2 335\n",
      "Len of text chunks 3 497\n",
      "seed question ['What are the recommendations made by Amnesty International to the Special Rapporteur on Human Rights Defenders?', 'Who are the target audience of the two books created by Amnesty International on child rights?']\n",
      "{'reason': 'The question is specific and refers to a well-known organization and a specific role, making it clear and answerable.', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████▎            | 15/20 [07:53<02:27, 29.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question is specific and refers to a particular set of books by a known organization, making it clear and answerable.', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "21it [08:03, 16.78s/it]                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 404\n",
      "seed question ['What is the right to truth?', 'Who has the right to know the truth regarding human rights violations?']\n",
      "{'reason': 'The question is clear and refers to a specific concept, making it answerable.', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is specific and refers to a particular human rights issue, making it clear and answerable.', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "28it [08:51, 12.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question is clear and specific, referring to a particular topic of human rights violations.', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear and specific, referring to a particular aspect of human rights, making it answerable.', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "36it [09:54, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 1 257\n",
      "seed question ['What does the Act prioritize over the investigation and disclosure of facts?']\n",
      "{'reason': 'The question is vague and does not specify which Act it is referring to, making it unclear and potentially unanswerable.', 'verdict': 'No'}\n",
      "rewritten question What does the Act prioritize over the investigation and disclosure of facts according to the context provided?\n",
      "{'reason': \"The question is vague and does not specify which 'Act' it is referring to, making it unclear and potentially unanswerable without further context.\", 'verdict': 'No'}\n",
      "Len of text chunks 2 338\n",
      "Len of text chunks 3 458\n",
      "seed question ['What is the purpose of Article 207.3 in the Russian Criminal Code?', \"Why does the prosecution consider statements contrary to the official position as 'false' under Article 207.3?\"]\n",
      "{'reason': 'The question is specific and refers to a particular article in a specific legal code, making it clear and answerable.', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is specific and refers to a particular law in the Russian Criminal Code, making it clear and answerable.', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "45it [10:39,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question is specific and refers to a particular legal article, making it clear and answerable.', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is specific and refers to a particular legal article, making it clear and answerable.', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "55it [11:01,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 1 308\n",
      "seed question ['Why are civil society organizations in Nicaragua being decimated?', 'How are human rights defenders in Venezuela being affected by repression and criminalization?']\n",
      "{'reason': 'The question is specific and refers to a current event in a specific location, making it clear and answerable.', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is specific and refers to a well-known situation in a specific country, making it clear and answerable.', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "66it [11:36,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question is specific and refers to a well-known issue, making it clear and answerable.', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is too broad and does not specify a particular time or context, making it potentially ambiguous and open to multiple interpretations.', 'verdict': 'No'}\n",
      "Len of text chunks 2 334\n",
      "seed question ['What is the purpose of the Ramsar sites designation?', 'Where was the UN Conference on Biological Diversity (COP15) held in 2022?']\n",
      "{'reason': 'The question is specific and refers to a well-known international conservation effort, making it clear and answerable.', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is specific and refers to a well-known environmental designation, making it clear and answerable.', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "78it [17:16, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question is specific and refers to a well-known event, making it clear and answerable.', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is specific and refers to a well-known event and its location in a specific year, making it clear and answerable.', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "91it [17:35,  9.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 406\n",
      "Len of text chunks 3 511\n",
      "seed question ['What is the purpose of the agreement known as 30x30?', \"Who failed to explicitly recognize Indigenous Peoples' lands and territories as a distinct category of protected area at COP15?\"]\n",
      "{'reason': 'The question refers to a specific agreement by its name, making it clear and answerable.', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "105it [17:51,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question is specific and refers to a well-known event (COP15) and a specific issue, making it clear and answerable.', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "120it [18:00,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 397\n",
      "Len of text chunks 1 297\n",
      "seed question ['What are some effects of abortion criminalization?', 'Who is most affected by the criminalization of abortion?']\n",
      "{'reason': 'The question is clear and specific, referring to a particular topic that can be researched and answered.', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is specific and refers to a well-known social issue, making it clear and answerable.', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "136it [20:02,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question is too broad and does not specify a particular region, country, or demographic, making it potentially ambiguous and open to multiple interpretations.', 'verdict': 'No'}\n",
      "rewritten question Who is most affected by the criminalization of abortion?\n",
      "{'reason': 'The question is too broad and does not specify a particular region, country, or demographic, making it potentially ambiguous and open to multiple interpretations.', 'verdict': 'No'}\n",
      "Len of text chunks 2 338\n",
      "Len of text chunks 1 20\n",
      "Len of text chunks 2 339\n",
      "seed question ['What responsibilities do social media companies have in relation to human rights?', 'How can social media companies ensure that all users can exercise their rights online?']\n",
      "{'reason': 'The question is clear and specific, referring to the responsibilities of a defined group (social media companies) in relation to a defined topic (human rights).', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is specific and clear, referring to a particular topic and the responsibilities of a specific group of entities (social media companies).', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "153it [21:55,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question is clear and specific, referring to a particular group (social media companies) and a specific issue (ensuring online rights for all users).', 'verdict': 'Yes'}\n",
      "{'reason': \"The question is clear and specific, referring to the role of social media companies in protecting users' rights online, making it answerable.\", 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "171it [22:25,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 4 420\n",
      "Len of text chunks 3 494\n",
      "seed question ['What were some of the abuses documented by Amnesty International during the tournament in Qatar?', 'When did the government of Qatar take steps to repeal certain restrictions on migrant workers?']\n",
      "{'reason': 'The question is specific and refers to a well-known organization and event, making it clear and answerable.', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is specific and refers to a well-known organization and a specific issue, making it clear and answerable.', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "190it [23:31,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question is specific and refers to a well-known event, making it clear and answerable.', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is specific and refers to a well-known event, making it clear and answerable.', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "210it [24:29,  7.00s/it]\n"
     ]
    }
   ],
   "source": [
    "test_df = testsetgenerator.generate(documents,test_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0b3b9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed_question</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_type</th>\n",
       "      <th>episode_done</th>\n",
       "      <th>evolution_elimination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What impact has the USA Supreme Court ruling o...</td>\n",
       "      <td>What are the global implications of the USA Su...</td>\n",
       "      <td>- In 2022, the USA Supreme Court handed down a...</td>\n",
       "      <td>The global implications of the USA Supreme Cou...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the role of fossil fuel companies in d...</td>\n",
       "      <td>Which companies are the main contributors to G...</td>\n",
       "      <td>- Fossil fuel companies, whether state or priv...</td>\n",
       "      <td>According to the Carbon Majors database, the m...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who are the largest emitters of GHG emissions ...</td>\n",
       "      <td>Which private companies in the Americas are th...</td>\n",
       "      <td>The private companies responsible for the most...</td>\n",
       "      <td>The largest private companies in the Americas ...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What did Amnesty International call on its sup...</td>\n",
       "      <td>What action did Amnesty International urge its...</td>\n",
       "      <td>Amnesty International called on its vast netwo...</td>\n",
       "      <td>Amnesty International urged its supporters to ...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the recommendations made by Amnesty I...</td>\n",
       "      <td>What are the recommendations made by Amnesty I...</td>\n",
       "      <td>Amnesty International recommends that the Spec...</td>\n",
       "      <td>The recommendations made by Amnesty Internatio...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Who are the target audience of the two books c...</td>\n",
       "      <td>Who are the target audience of the two books c...</td>\n",
       "      <td>Amnesty International has therefore created tw...</td>\n",
       "      <td>The target audience of the two books created b...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the right to truth?</td>\n",
       "      <td>Which right guarantees access to comprehensive...</td>\n",
       "      <td>26. The Act raises serious questions about its...</td>\n",
       "      <td>The right that guarantees access to comprehens...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Who has the right to know the truth regarding ...</td>\n",
       "      <td>Who has the right to be fully informed about h...</td>\n",
       "      <td>- The victims of gross human rights violations...</td>\n",
       "      <td>The victims of gross human rights violations a...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the purpose of Article 207.3 in the Ru...</td>\n",
       "      <td>When can individuals be found guilty under Art...</td>\n",
       "      <td>- As long as their statements are contrary to ...</td>\n",
       "      <td>Individuals can be found guilty under Article ...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Why does the prosecution consider statements c...</td>\n",
       "      <td>When does the prosecution consider statements ...</td>\n",
       "      <td>- As long as their statements are contrary to ...</td>\n",
       "      <td>The prosecution considers statements contrary ...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Why are civil society organizations in Nicarag...</td>\n",
       "      <td>What factors have contributed to the decline o...</td>\n",
       "      <td>- \"arrests and harassment of human rights defe...</td>\n",
       "      <td>The factors that have contributed to the decli...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is the purpose of the Ramsar sites design...</td>\n",
       "      <td>What conditions designate wetlands as Ramsar s...</td>\n",
       "      <td>- \"Ramsar sites are designated when they fulfi...</td>\n",
       "      <td>The conditions that designate wetlands as Rams...</td>\n",
       "      <td>conditional</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Where was the UN Conference on Biological Dive...</td>\n",
       "      <td>Where was COP15 held in 2022?</td>\n",
       "      <td>- These include the agreement known as 30x30, ...</td>\n",
       "      <td>COP15 was held in Montreal, Canada in 2022.</td>\n",
       "      <td>conditional</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is the purpose of the agreement known as ...</td>\n",
       "      <td>What is the purpose of the agreement known as ...</td>\n",
       "      <td>- These include the agreement known as 30x30, ...</td>\n",
       "      <td>The purpose of the agreement known as 30x30 is...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Who failed to explicitly recognize Indigenous ...</td>\n",
       "      <td>Who failed to explicitly recognize Indigenous ...</td>\n",
       "      <td>Unfortunately, at COP15, States failed to expl...</td>\n",
       "      <td>The States failed to explicitly recognize Indi...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What are some effects of abortion criminalizat...</td>\n",
       "      <td>What are the consequences of criminalizing abo...</td>\n",
       "      <td>- Abortion criminalization contributes to stig...</td>\n",
       "      <td>The consequences of criminalizing abortion for...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What responsibilities do social media companie...</td>\n",
       "      <td>What responsibilities should social media comp...</td>\n",
       "      <td>Social media companies involved in facilitatin...</td>\n",
       "      <td>Social media companies should have the respons...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How can social media companies ensure that all...</td>\n",
       "      <td>What role do social media companies play in pr...</td>\n",
       "      <td>Companies, including social media companies, h...</td>\n",
       "      <td>Social media companies play a role in protecti...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What were some of the abuses documented by Amn...</td>\n",
       "      <td>What labor abuses were documented by Amnesty I...</td>\n",
       "      <td>During the tournament, Amnesty International d...</td>\n",
       "      <td>Amnesty International documented labor abuses ...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>When did the government of Qatar take steps to...</td>\n",
       "      <td>When did the government of Qatar start repeali...</td>\n",
       "      <td>Between 2018 and 2020, the government took imp...</td>\n",
       "      <td>The government of Qatar started repealing rest...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        seed_question  \\\n",
       "0   What impact has the USA Supreme Court ruling o...   \n",
       "1   What is the role of fossil fuel companies in d...   \n",
       "2   Who are the largest emitters of GHG emissions ...   \n",
       "3   What did Amnesty International call on its sup...   \n",
       "4   What are the recommendations made by Amnesty I...   \n",
       "5   Who are the target audience of the two books c...   \n",
       "6                         What is the right to truth?   \n",
       "7   Who has the right to know the truth regarding ...   \n",
       "8   What is the purpose of Article 207.3 in the Ru...   \n",
       "9   Why does the prosecution consider statements c...   \n",
       "10  Why are civil society organizations in Nicarag...   \n",
       "11  What is the purpose of the Ramsar sites design...   \n",
       "12  Where was the UN Conference on Biological Dive...   \n",
       "13  What is the purpose of the agreement known as ...   \n",
       "14  Who failed to explicitly recognize Indigenous ...   \n",
       "15  What are some effects of abortion criminalizat...   \n",
       "16  What responsibilities do social media companie...   \n",
       "17  How can social media companies ensure that all...   \n",
       "18  What were some of the abuses documented by Amn...   \n",
       "19  When did the government of Qatar take steps to...   \n",
       "\n",
       "                                             question  \\\n",
       "0   What are the global implications of the USA Su...   \n",
       "1   Which companies are the main contributors to G...   \n",
       "2   Which private companies in the Americas are th...   \n",
       "3   What action did Amnesty International urge its...   \n",
       "4   What are the recommendations made by Amnesty I...   \n",
       "5   Who are the target audience of the two books c...   \n",
       "6   Which right guarantees access to comprehensive...   \n",
       "7   Who has the right to be fully informed about h...   \n",
       "8   When can individuals be found guilty under Art...   \n",
       "9   When does the prosecution consider statements ...   \n",
       "10  What factors have contributed to the decline o...   \n",
       "11  What conditions designate wetlands as Ramsar s...   \n",
       "12                      Where was COP15 held in 2022?   \n",
       "13  What is the purpose of the agreement known as ...   \n",
       "14  Who failed to explicitly recognize Indigenous ...   \n",
       "15  What are the consequences of criminalizing abo...   \n",
       "16  What responsibilities should social media comp...   \n",
       "17  What role do social media companies play in pr...   \n",
       "18  What labor abuses were documented by Amnesty I...   \n",
       "19  When did the government of Qatar start repeali...   \n",
       "\n",
       "                                              context  \\\n",
       "0   - In 2022, the USA Supreme Court handed down a...   \n",
       "1   - Fossil fuel companies, whether state or priv...   \n",
       "2   The private companies responsible for the most...   \n",
       "3   Amnesty International called on its vast netwo...   \n",
       "4   Amnesty International recommends that the Spec...   \n",
       "5   Amnesty International has therefore created tw...   \n",
       "6   26. The Act raises serious questions about its...   \n",
       "7   - The victims of gross human rights violations...   \n",
       "8   - As long as their statements are contrary to ...   \n",
       "9   - As long as their statements are contrary to ...   \n",
       "10  - \"arrests and harassment of human rights defe...   \n",
       "11  - \"Ramsar sites are designated when they fulfi...   \n",
       "12  - These include the agreement known as 30x30, ...   \n",
       "13  - These include the agreement known as 30x30, ...   \n",
       "14  Unfortunately, at COP15, States failed to expl...   \n",
       "15  - Abortion criminalization contributes to stig...   \n",
       "16  Social media companies involved in facilitatin...   \n",
       "17  Companies, including social media companies, h...   \n",
       "18  During the tournament, Amnesty International d...   \n",
       "19  Between 2018 and 2020, the government took imp...   \n",
       "\n",
       "                                               answer  question_type  \\\n",
       "0   The global implications of the USA Supreme Cou...    conditional   \n",
       "1   According to the Carbon Majors database, the m...      reasoning   \n",
       "2   The largest private companies in the Americas ...      reasoning   \n",
       "3   Amnesty International urged its supporters to ...    conditional   \n",
       "4   The recommendations made by Amnesty Internatio...         simple   \n",
       "5   The target audience of the two books created b...         simple   \n",
       "6   The right that guarantees access to comprehens...  multi_context   \n",
       "7   The victims of gross human rights violations a...  multi_context   \n",
       "8   Individuals can be found guilty under Article ...    conditional   \n",
       "9   The prosecution considers statements contrary ...    conditional   \n",
       "10  The factors that have contributed to the decli...      reasoning   \n",
       "11  The conditions that designate wetlands as Rams...    conditional   \n",
       "12        COP15 was held in Montreal, Canada in 2022.    conditional   \n",
       "13  The purpose of the agreement known as 30x30 is...         simple   \n",
       "14  The States failed to explicitly recognize Indi...         simple   \n",
       "15  The consequences of criminalizing abortion for...  multi_context   \n",
       "16  Social media companies should have the respons...      reasoning   \n",
       "17  Social media companies play a role in protecti...      reasoning   \n",
       "18  Amnesty International documented labor abuses ...  multi_context   \n",
       "19  The government of Qatar started repealing rest...  multi_context   \n",
       "\n",
       "    episode_done evolution_elimination  \n",
       "0           True                 False  \n",
       "1           True                 False  \n",
       "2           True                 False  \n",
       "3           True                  True  \n",
       "4           True                  None  \n",
       "5           True                  None  \n",
       "6           True                  True  \n",
       "7          False                 False  \n",
       "8           True                 False  \n",
       "9           True                 False  \n",
       "10         False                 False  \n",
       "11          True                 False  \n",
       "12          True                  True  \n",
       "13          True                  None  \n",
       "14          True                  None  \n",
       "15          True                 False  \n",
       "16         False                 False  \n",
       "17         False                 False  \n",
       "18          True                 False  \n",
       "19         False                 False  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d58feb8-8604-4854-9d1d-7aa27495f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_pandas().to_csv(\"amnesty_reports.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "589a6686",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "434afa58",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "4 is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: 4 is not in list"
     ]
    }
   ],
   "source": [
    "x.index(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ca508bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03897e",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Conditional question evol is working\n",
    "- reasoning/multi context are not working as expected\n",
    "- Almost all questions are closed endeded \n",
    "- Almost all questions start with \"What\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e52f7",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "144547a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file.markdown_reader import MarkdownReader\n",
    "from llama_index.schema import Document\n",
    "from typing import List, Dict, Optional\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e30ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagasMdReader(MarkdownReader):\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_file_metadata(path):\n",
    "        \n",
    "        return {\"file_name\":os.path.basename(path),\n",
    "                \"dirname\":os.path.dirname(path)}\n",
    "        \n",
    "    \n",
    "    def get_local_metadata(self, text):\n",
    "        \n",
    "\n",
    "        return_dict = {}\n",
    "        pattern = r'---\\s*title:\\s*(.+?)(?:\\s*description:\\s*\"(.*?)\")?\\s*---'\n",
    "        match = re.findall(pattern, text)\n",
    "        if match:\n",
    "            title,desc = match[0]\n",
    "            return_dict['title'] = title\n",
    "            return_dict['description'] = desc if desc else None\n",
    "            \n",
    "        return return_dict\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    def load_data(\n",
    "        self, file: Path, extra_info: Optional[Dict] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Parse file into string.\"\"\"\n",
    "        \n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        if self._remove_hyperlinks:\n",
    "            content = self.remove_hyperlinks(content)\n",
    "        if self._remove_images:\n",
    "            content = self.remove_images(content)\n",
    "            \n",
    "        local_metadata = self.get_local_metadata(content)\n",
    "\n",
    "        extra_info = dict(extra_info,**local_metadata) if local_metadata else extra_info\n",
    "        return [Document(text=content,metadata=extra_info)]\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9c01902",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_reader = RagasMdReader(remove_hyperlinks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bca9d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm2(prompt, **kwargs):\n",
    "    response = client.chat.completions.create(\n",
    "        model=kwargs.get(\"model\", \"gpt-3.5-turbo\"),\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=kwargs.get(\"temperature\", 0),\n",
    "        top_p=kwargs.get(\"top_p\", 1),\n",
    "        frequency_penalty=kwargs.get(\"frequency_penalty\", 0.0),\n",
    "        presence_penalty=kwargs.get(\"presence_penalty\", 0.0),\n",
    "        max_tokens=kwargs.get(\"max_tokens\", 500),\n",
    "        n=kwargs.get(\"n\", 1),\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feec19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.prompts import SEED_QUESTION, EVOLUTION_ELIMINATION, REWRITE_QUESTION, FILTER_QUESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bec0dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /Users/shahules/Myprojects/rag-experiments/gitlab-handbook/data/handbook/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95d00bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [\n",
    "    \"/Users/shahules/Myprojects/rag-experiments/gitlab-handbook/data/handbook/leadership/\",\n",
    "    \"/Users/shahules/Myprojects/rag-experiments/gitlab-handbook/data/handbook/company/\",\n",
    "    \"/Users/shahules/Myprojects/rag-experiments/gitlab-handbook/data/handbook/infrastructure-standards/\",\n",
    "    \"/Users/shahules/Myprojects/rag-experiments/gitlab-handbook/data/handbook/communication/\",\n",
    "    \"/Users/shahules/Myprojects/rag-experiments/gitlab-handbook/data/handbook/security\"\n",
    "]\n",
    "documents = []\n",
    "for dir_path in dirs:\n",
    "    loader = SimpleDirectoryReader(dir_path, \n",
    "                                   recursive=True,\n",
    "                                  file_extractor={\".md\":md_reader},\n",
    "                                file_metadata=RagasMdReader.get_file_metadata)\n",
    "    \n",
    "    documents.extend(loader.load_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64e715b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([doc for doc in documents if doc.metadata.get(\"file_name\") is None ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "15e60505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "---\n",
    "title: \"Ask Me Anything\"\n",
    "description: \"Learn and ask questions at GitLab's Ask Me Anything (AMA) meetings\"\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "# Regular expression to capture title and description\n",
    "pattern_optional_description = '---\\s*title:\\s*(.+?)(?:\\s*description:\\s*\"(.*?)\")?\\s*---'\n",
    "\n",
    "# Finding matches\n",
    "matches = re.match(pattern_optional_description, text)\n",
    "\n",
    "matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "08037476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4cc57d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile('---\\s*title:\\s*(.+?)(?:\\s*description:\\s*\"(.*?)\")?\\s*---')\n",
    "[m.groupdict() for m in pattern.finditer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58b4cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "---\n",
    "title: \"Book clubs\"\n",
    "---\n",
    "\n",
    "From time to time, we run internal book clubs on a book from one of our resource lists. All are welcome! However,\n",
    "each club has a suggested audience to indicate roles to which the content is tailored.\n",
    "\n",
    "- [Leadership]({{< ref \"_index.md#books\" >}})\n",
    "- [Development](https://about.gitlab.com/handbook/engineering/development/#books)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3a953420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_optional_description = r'---\\s*title:\\s*\"(.+?)\"\\s*---'\n",
    "re.findall(pattern,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64f4ad42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b94cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4bc085c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testset = testsetgenerator.generate(documents, test_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26f40412",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.to_pandas().to_csv(\"gitlab_communication_company_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ae48ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92931c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1=\"How should I use my notification settings in Slack?\"\n",
    "q2=\"What's the best way to manage Slack notification settings for efficient communication and minimal disruptions?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1052549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = EVOLUTION_ELIMINATION.prompt.template.format(question1=q1, question2=q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37bedda",
   "metadata": {},
   "source": [
    "## Create multi context\n",
    "- find similar docs using metadata \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a0773b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index.indices.query.embedding_utils import get_top_k_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bac7c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "41792a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [doc.metadata.get('title').strip('\"') for doc in documents if doc.metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "165254ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.embed_documents(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "064ef79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "19d68a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, indices = get_top_k_embeddings(embeddings[k],embeddings,similarity_cutoff=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ac6d8408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seach query :Top Cross-Functional Initiatives\n",
      " Results: [['Top Cross-Functional Initiatives', 'Building High Performing Teams', 'Product Career Development Framework Working Group', 'Leadership', 'Single Codebase Working Group']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Seach query :{titles[k]}\\n Results: [{[titles[i] for i in indices[:5]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4c9938",
   "metadata": {},
   "source": [
    "## Merge documents based on meta-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0695effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import HumanMessagePromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2f28fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITE_QUESTION = HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "Rewrite the given question so that it can be answered without context.\n",
    "\n",
    "Question: When was he born?\n",
    "Context : Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time. He was born on 14 March 1879.\n",
    "Rewritten question: When was Albert Einstein born? \n",
    "\n",
    "Context: A clothes iron (also flatiron, smoothing iron, or simply iron) is a small appliance that, when heated, is used to press clothes to remove wrinkles and unwanted creases. Domestic irons generally range in operating temperature from between 121 °C (250 °F) to 182 °C (360 °F). It is named for the metal (iron) of which the device was historically made, and the use of it is generally called ironing, the final step in the process of laundering clothes.\n",
    "Question: What is temperate range of the device?\n",
    "Rewritten question: What is temperate range of clothes iron?\n",
    "\n",
    "\n",
    "Question:{question}\n",
    "Context: {context}\n",
    "Rewritten question:\n",
    "\"\"\")\n",
    "\n",
    "QUERY =  HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "Rewrite the question using given context so that it can be read and answered without any extra information.\n",
    "\n",
    "Question:{question}\n",
    "Context: {context}\n",
    "\"\"\")\n",
    "\n",
    "text = \"\"\"\n",
    "The Mona Lisa, painted by Leonardo da Vinci in the early 16th century, is one of the most famous and valuable paintings in the world.\n",
    "Known for its enigmatic expression and innovative use of sfumato, the painting has become a symbol of Renaissance art.\n",
    "The Mona Lisa is displayed in the Louvre Museum in Paris and attracts millions of visitors annually.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fc666f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(indices):\n",
    "    \n",
    "    return '\\n'.join([documents[idx].get_content() for idx in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d871744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "455779c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm2(SEED_QUESTION.format(context=get_content([49,50])).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3baa48ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the purpose of the GPT-3 Editor in the CORE framework?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = response.choices[0].message.content\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2839379b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rewrite the question using given context so that it can be read and answered without any extra information.\n",
      "\n",
      "Question:What is the impact of training data size on the model's performance?\n",
      "Context: Preprint.\n",
      "Table 2: Overall experiment results on six tasks. Bold numbers indicate the best performance among\n",
      "non-proprietary models, and gray-colored bold text indicates the best proprietary model when\n",
      "they outperforms all non-proprietary models.∗indicates concurrent or recent results reported by\n",
      "concurrent work. – indicates numbers that are not reported by the original papers or are not applicable.\n",
      "Models are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em,\n",
      "rouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\n",
      "Short-form Closed-set Long-form generations (with citations)\n",
      "PopQA TQA Pub ARC Bio ASQA\n",
      "LM (acc) (acc) (acc) (acc) (FS) (em) (rg) (mau) (pre) (rec)\n",
      "LMs with proprietary data\n",
      "Llama2-c 13B 20.0 59.3 49.4 38.4 55.9 22.4 29.6 28.6 – –\n",
      "Ret-Llama2-c 13B 51.8 59.8 52.1 37.9 79.9 32.8 34.8 43.8 19.8 36.1\n",
      "ChatGPT 29.3 74.3 70.1 75.3 71.8 35.3 36.2 68.8 – –\n",
      "Ret-ChatGPT 50.8 65.7 54.7 75.3 – 40.7 39.9 79.7 65.1 76.6\n",
      "Perplexity.ai – – – – 71.2 – – – – –\n",
      "Baselines without retrieval\n",
      "Llama2 7B 14.7 30.5 34.2 21.8 44.5 7.9 15.3 19.0 – –\n",
      "Alpaca 7B 23.6 54.5 49.8 45.0 45.8 18.8 29.4 61.7 – –\n",
      "Llama2 13B 14.7 38.5 29.4 29.4 53.4 7.2 12.4 16.0 – –\n",
      "Alpaca 13B 24.4 61.3 55.5 54.9 50.2 22.9 32.0 70.6 – –\n",
      "CoVE 65B* – – – – 71.2 – – – – –\n",
      "Baselines with retrieval\n",
      "Toolformer* 6B – 48.8 – – – – – – – –\n",
      "Llama2 7B 38.2 42.5 30.0 48.0 78.0 15.2 22.1 32.0 2.9 4.0\n",
      "Alpaca 7B 46.7 64.1 40.2 48.0 76.6 30.9 33.3 57.9 5.5 7.2\n",
      "Llama2-FT 7B 48.7 57.3 64.3 65.8 78.2 31.0 35.8 51.2 5.0 7.5\n",
      "SAIL* 7B – – 69.2 48.4 – – – – – –\n",
      "Llama2 13B 45.7 47.0 30.2 26.0 77.5 16.3 20.5 24.7 2.3 3.6\n",
      "Alpaca 13B 46.1 66.9 51.1 57.6 77.7 34.8 36.7 56.6 2.0 3.8\n",
      "Our SELF-RAG 7B 54.9 66.4 72.4 67.3 81.2 30.0 35.7 74.3 66.9 67.8\n",
      "Our SELF-RAG 13B 55.8 69.3 74.5 73.1 80.2 31.7 37.0 71.6 70.3 71.3\n",
      "5 R ESULTS AND ANALYSIS\n",
      "5.1 M AINRESULTS\n",
      "Comparison against baselines without retrieval. Table 2 (top) presents the baselines without\n",
      "retrieval. Our SELF-RAG(bottom two rows) demonstrates a substantial performance advantage\n",
      "over supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA,\n",
      "biography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms\n",
      "a concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation\n",
      "task, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which\n",
      "iteratively prompts Llama2 65Bto refine output.\n",
      "Comparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF-RAGalso\n",
      "outperforms existing RAG in many tasks, obtaining the best performance among non-proprietary\n",
      "LM-based models on all tasks. While our method outperforms other baselines, on PopQA or Bio,\n",
      "powerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca) show large gains from\n",
      "their non-retrieval baselines. However, we found that these baselines provide limited solutions for\n",
      "tasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth\n",
      "and ARC-Challenge, baselines with retrieval do not improve performance notably from their no-\n",
      "retrieval counterparts. We also observe that most baselines with retrieval struggle to improve citation\n",
      "accuracy. On ASQA, our model shows significantly higher citation precision and recall than all\n",
      "models except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits superior efficacy\n",
      "in this particular task, surpassing smaller LMs. Our SELF-RAGbridges this performance gap, even\n",
      "outperforming ChatGPT in citation precision, which measures whether the model-generated claim is\n",
      "fully supported by cited evidence. We also found that on the metrics for factual precision, SELF-RAG\n",
      "7B occasionally outperforms our 13B due to the tendency of smaller SELF-RAGto often generate\n",
      "8\n",
      "Preprint.\n",
      "PQA Med AS\n",
      "(acc) (acc) (em)\n",
      "SELF-RAG(50k) 45.5 73.5 32.1\n",
      "Training\n",
      "No Retriever R 43.6 67.8 31.0\n",
      "No Critic C 42.6 72.0 18.1\n",
      "Test\n",
      "No retrieval 24.7 73.0 –\n",
      "Hard constraints 28.3 72.6 –\n",
      "Retrieve top1 41.8 73.1 28.6\n",
      "Remove ISSUP 44.1 73.2 30.6\n",
      "(a) Ablation\n",
      "1 270.070.5Precision\n",
      "1 2\n",
      "Weight for IsSupport9095Mauve\n",
      " (b) Customization\n",
      "0.0 0.2 0.4 0.60.980.990.991.00Accuracy\n",
      "PubHealth\n",
      "0.0 0.2 0.4 0.6\n",
      "Retrieval Threshold0.60.81.0AccuracyPopQA0.00.51.0\n",
      "Frequency\n",
      "0.250.500.751.00\n",
      "Frequency\n",
      " (c) Retrieval\n",
      "Figure 3: Analysis on SELF-RAG:(a)Ablation studies for key components of SELF-RAGtraining\n",
      "and inference based on our 7B model. (b) Effects of soft weights on ASQA citation precision and\n",
      "Mauve (fluency). (c) Retrieval frequency andnormalized accuracy on PubHealth and PopQA.\n",
      "precisely grounded yet shorter outputs. Llama2-FT 7B, which is the baseline LM trained on the same\n",
      "instruction-output pairs as SELF-RAGwithout retrieval or self-reflection and is retrieval-augmented\n",
      "at test time only, lags behind S ELF-RAG. This result indicates S ELF-RAGgains are not solely from\n",
      "training data and demonstrate the effectiveness of S ELF-RAGframework.\n",
      "5.2 A NALYSIS\n",
      "Ablation studies. We conduct a set of ablations of our framework to identify which factors play\n",
      "key roles. We evaluate two model variants trained differently than our model: No Retriever trains an\n",
      "LM using the standard instruction-following method given instruction-output pairs, without retrieved\n",
      "passages; No Critic trains an LM trained with input-output pairs that are always augmented with the\n",
      "top one retrieved document without reflection tokens. This is similar to SAIL (Luo et al., 2023), and\n",
      "we use our instruction-output data instead of using the Alpaca dataset (Dubois et al., 2023), as in\n",
      "SAIL. We also conduct ablation on our inference-time algorithm, including No retrieval disables\n",
      "retrieval during inference; Hard constraints indicates the model performance that retrieves when\n",
      "Retrieve =Yes instead of using the adaptive threshold; Retrieve top 1 always retrieves and uses the\n",
      "top one document only, similar to standard RAG approaches; Remove ISSUPindicates the model\n",
      "performance that removes ISSUPscore only during critique-guided beam search in Eq. 4. In this\n",
      "ablation experiment, we use a training instance size of 50k for a more efficient exploration of training\n",
      "variations. Later in this section, we conduct an analysis of the effect of training data size. We conduct\n",
      "the ablation studies on three datasets, PopQA, PubHealth, and ASQA. On ASQA, we evaluate models\n",
      "on sampled 150 instances and exclude ablations involving adaptive or no retrieval processes.\n",
      "We show in Table 3a the ablation results. The top part of the table shows results for training ablations,\n",
      "and the bottom part is for inference ablations. We see that all components play important roles. We\n",
      "also observe a large performance gap between SELF-RAGand No Retriever or Critic baselines across\n",
      "tasks, indicating that training an LM with those models largely contributes to the performance gain of\n",
      "SELF-RAG. Using the top passages regardless of their relevance (Retrieve top 1) as in conventional\n",
      "RAG approaches causes a large drop in PopQA and ASQA, and removing ISSUPduring the beam\n",
      "search results hurts performance on ASQA. This demonstrates the effectiveness of SELF-RAG’s\n",
      "capabilities of carefully selecting generations based fine-grained multiple criterion, instead of naively\n",
      "using all of the top passages from the retrieval model or solely depending on relevance scores.\n",
      "Effects of inference-time customization. One key benefit of our proposed framework is that it\n",
      "enables us to control how much each critique type affects the final generation sampling. We analyze\n",
      "the effects of different parameter weights on the top of our 7B model during inference time on\n",
      "ASQA, where multiple evaluation aspects are considered. Figure 3b shows the effects of changing\n",
      "the weighting term for ISSUP, which criticizes how supported the output is by the text passage. As\n",
      "the figure shows, increasing the weight leads to positive effects on the models’ citation precision\n",
      "since this puts more emphasis on whether model generation is supported by the evidence. On the\n",
      "9\n",
      "Preprint.\n",
      "0 50 100 150\n",
      "Num of training (k)3540455055Perfomance\n",
      "(a) PopQA\n",
      "0 100\n",
      "Num of training (k)717273\n",
      " (b) PubHealth\n",
      "0 100\n",
      "Num of training (k)4060\n",
      " (c) ASQA (prec)Pop Bio.\n",
      "S & P 92.5 70.0\n",
      "ISREL 95.0 90.0\n",
      "ISSUP 90.0 85.0\n",
      "(d) Human evaluation on PopQA\n",
      "and Bio generation.\n",
      "Figure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect\n",
      "of the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)\n",
      "Human analysis on S ELF-RAGoutputs as well as reflection tokens.\n",
      "contrary, a larger weight results in lower MAUVE scores: when generation gets longer and more\n",
      "fluent, there are often more claims that are not fully supported by citations, consistent with findings\n",
      "by Liu et al. (2023a). Our framework lets practitioners choose and customize models’ behaviors at\n",
      "test time by adjusting such parameters without requiring additional training.\n",
      "Efficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval\n",
      "occurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects\n",
      "overall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers\n",
      "of threshold δ(larger δresults in less retrieval) on PubHealth and PopQA. Figure 3c shows that\n",
      "the model’s retrieval frequencies dramatically change on both datasets. as δvaries. On one hand,\n",
      "performance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.\n",
      "Effects of training data size. We conduct an analysis of how the data scale affects the model’s\n",
      "performance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original\n",
      "150k training instances, and fine-tune four SELF-RAG 7Bvariants on those subsets. Then, we compare\n",
      "the model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF-\n",
      "RAGtrained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models’\n",
      "performance trained on different amount of data. Across all datasets, increasing data size often shows\n",
      "upward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do\n",
      "not observed such significant improvements on Llama2-FT 7Bwhen increasing the training data from\n",
      "50k to 150k. These results also indicate that further expanding the training data of SELF-RAGmay\n",
      "lead to further improvements, although in this work we limit our training data size to 150k.\n",
      "Human evaluations. We conduct small human evaluations on SELF-RAGoutputs, as well as the\n",
      "reliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio\n",
      "results. Following Menick et al. (2022), human annotators evaluate S&P , which indicates whether\n",
      "the model output is plausible (i.e., the output is a reasonable and on-topic response to the question\n",
      "as if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to\n",
      "verify the validity of the answer). For S&P, we do not consider the instances where SELF-RAG\n",
      "predicts irrelevant orno support . We then ask our annotators whether the model-predicted\n",
      "reflection tokens about ISRELand ISSUPmatch their inspections (e.g., whether the fully supported\n",
      "output is supported by the cited evidence). Human annotators find SELF-RAGanswers are often\n",
      "plausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is\n",
      "consistent with Menick et al. (2022). Human annotators also find ISRELand ISSUPreflection token\n",
      "predictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated\n",
      "examples and explanations on assessments.\n",
      "6 C ONCLUSION\n",
      "This work introduces SELF-RAG, a new framework to enhance the quality and factuality of LLMs\n",
      "through retrieval on demand and self-reflection. SELF-RAGtrains an LM to learn to retrieve, generate,\n",
      "and critique text passages and its own generation by predicting the next tokens from its original\n",
      "vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables\n",
      "the tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on\n",
      "six tasks using multiple metrics demonstrate that SELF-RAGsignificantly outperforms LLMs with\n",
      "more parameters or with conventional retrieval-augmented generation approaches.\n",
      "10\n",
      "\n",
      "Rewritten question: How does the size of the training data affect the performance of the SELF-RAG model according to the experiment results?\n"
     ]
    }
   ],
   "source": [
    "print(QUERY.format(question=q, context=get_content([12,13,14])).content)\n",
    "print('Rewritten question: How does the size of the training data affect the performance of the SELF-RAG model according to the experiment results?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fd1f148b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1909"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_content([12,13,14]).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3bad307a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimuli BMS [41] Pan et al. [29] Salicon [15] ML-Net [9] Ours Ground Truth\n",
      "Figure 3: Qualitative performance comparison .\n",
      "SateliteJumbledSocial\n",
      "OutdoorNaturalArt\n",
      "InvertedFractalObjectCartoon\n",
      "OutdoorManMadeIndoorActionPatternNoisy\n",
      "Affective\n",
      "BlackWhiteRandom\n",
      "LowResolutionLineDrawingSketchSketchLineDrawingLowResolutionRandomBlackWhiteAffectiveNoisyPatternActionIndoorOutdoorManMadeCartoonObjectFractalInvertedArtOutdoorNaturalSocialJumbledSatelite\n",
      "0.00.20.40.60.81.0\n",
      "Figure 4: Classiﬁcation confusion matrix .\n",
      "bias. Location bias is an important component of human\n",
      "visual attention, and so should be included in any model.\n",
      "Table 2 shows the results of the 5-fold cross validation\n",
      "tests on the CAT2000 dataset. We achieve the best perfor-\n",
      "mance of the tested models. Table 3 shows the performance\n",
      "on the held out test images. Our model achieves the second\n",
      "best performance behind the DeepFix model [22]. Com-\n",
      "pared with DeepFix our network is shallower with less pa-\n",
      "rameters. The DeepFix model is not publicly available to\n",
      "test on the cross-validation data. Figure 3 shows example\n",
      "saliency maps generated by the tested models.\n",
      "Table 4 shows the results on the Salicon test set. Under\n",
      "this dataset our model does not have a performance advan-CC SAUC AUC Judd\n",
      "Our Model 0.730 0.771 0.861\n",
      "ML-Net [9] 0.7430 0.7680 0.8660\n",
      "Deep Convnet [29] 0.6220 0.7240 0.8580\n",
      "Shallow Convnet [29] 0.5957 0.6698 0.8364\n",
      "Rare 2012 Improved [31] 0.5108 0.6644 0.8148\n",
      "Baseline: BMS [41] 0.4268 0.6935 0.7899\n",
      "Baseline: GBVS [12] 0.4212 0.6303 0.7899\n",
      "Baseline: Itti [16] 0.2046 0.6101 0.6669\n",
      "Table 4: Salicon test set results.\n",
      "tage over ML-Net. This is largely because the dataset does\n",
      "not have diverse categories like the CAT-2000 dataset. Nev-\n",
      "ertheless, the performance of our model is roughly equiva-\n",
      "lent to the ML-net model.\n",
      "Comparison with averaging ensemble Given that the\n",
      "mixture model can be seen as an ensemble, it is important\n",
      "to compare the performance with a vanilla ensemble. We\n",
      "generate an ensemble of 5 networks where each network is\n",
      "trained on a random 80% of the training split. The saliency\n",
      "maps from each ensemble member are averaged to produce\n",
      "the ﬁnal saliency map. Our model still achieves superior\n",
      "performance compared with the ensemble (Table 5). Addi-\n",
      "tionally, our model is signiﬁcantly faster and requires less\n",
      "storage space due to the weight sharing.\n"
     ]
    }
   ],
   "source": [
    "print(get_content([14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8464da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser.simple import SimpleNodeParser\n",
    "node_parser = SimpleNodeParser.from_defaults(\n",
    "            chunk_size=1000, chunk_overlap=0, include_metadata=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec05e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = node_parser.get_nodes_from_documents(\n",
    "            documents=documents\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a6cd93d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '2',\n",
       " 'file_name': '00891b5a3dbbe4dae6b06432c3f335f8.pdf',\n",
       " 'file_path': 'arxiv-papers/00891b5a3dbbe4dae6b06432c3f335f8.pdf',\n",
       " 'creation_date': '2023-11-14',\n",
       " 'last_modified_date': '2023-11-14',\n",
       " 'last_accessed_date': '2023-11-14'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "03315c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0c61a824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fe3e30",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c5cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e60c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/shahules/Downloads/gitlab_communication_company_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bbae5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'seed_question', 'question', 'context', 'answer',\n",
       "       'question_type', 'episode_done', 'evolution_elimination'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a7cafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(['Unnamed: 0','seed_question','evolution_elimination'],axis=1)\n",
    "df['question'] = df['question'].apply(lambda x : x.replace(\"Casual Rewrite:\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d32ae2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"gitlab_company_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61f0af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6b58dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6fe6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What is the percentage of repetition in the generations produced by RETRO compared to GPT across different sizes?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b5ebe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm2(FILTER_QUESTION.format(question=question).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8137fc07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "ragas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
