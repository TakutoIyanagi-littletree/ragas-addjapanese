{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5be51b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34105067-eed8-4f09-913f-7043ff77cb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ragas/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b1990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader, SimpleDirectoryReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "321dbebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7457125d",
   "metadata": {},
   "source": [
    "## Assessing current state of test date generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ce32d303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shahules/belar/experimental\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bc5970e-0873-4e06-9b68-3dda31b1d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ArxivReader = download_loader(\"ArxivReader\")\n",
    "\n",
    "loader = ArxivReader() #uses simpledirectory reader under the hood, hence need modification to laod pages properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46684465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = loader.load_data(\"retrieval augmented generation AND large language model\",max_results=20,papers_dir='./arxiv-papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b52b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(\"./arxiv-papers/\",num_files_limit=20)\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc3d9cf",
   "metadata": {},
   "source": [
    "## SimpleDirectoryReader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab30508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b28f535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(documents[14].get_metadata_str())\n",
    "# print(\"CONTENT\")\n",
    "# print(documents[14].get_content())\n",
    "# print(len(documents[14].get_content().split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4139e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5.,  3.,  6., 13., 28., 21., 39., 10.,  5.,  1.]),\n",
       " array([  10. ,  113.1,  216.2,  319.3,  422.4,  525.5,  628.6,  731.7,\n",
       "         834.8,  937.9, 1041. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(doc.get_content().split()) for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df051faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dist =  {\n",
    "    \"simple\": 0.3,\n",
    "    \"reasoning\": 0.25,\n",
    "    \"multi_context\": .2,\n",
    "    \"conditional\": 0.25,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f9c0891",
   "metadata": {},
   "outputs": [],
   "source": [
    "testsetgenerator = TestsetGenerator.from_default(testset_distribution=test_dist,chunk_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "553ac808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shahules/belar/experimental\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ab453",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                            | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 403\n",
      "seed question ['What are the pillars of the Security Architecture Principles?', 'Why is it important to assign the least privilege possible?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sequence item 0: expected str instance, ReadTimeoutError found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sequence item 0: expected str instance, ReadTimeoutError found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█                                                   | 1/50 [00:32<26:32, 32.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|███                                                 | 3/50 [00:52<12:21, 15.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 405\n",
      "seed question ['What is the purpose of maintaining the Security Division ecosystem?', 'How are the diagrams in the Security Division ecosystem created and maintained?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████▏                                             | 6/50 [01:19<08:31, 11.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████▏                                        | 10/50 [01:35<05:01,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 328\n",
      "Len of text chunks 3 460\n",
      "seed question ['What is the expected response when a team is assigned an S1 or S2 security issue?', 'How should non-vulnerability security issues be handled by the security team?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███████████████▎                                   | 15/50 [03:55<10:16, 17.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|█████████████████████▍                             | 21/50 [04:23<05:38, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 472\n",
      "Len of text chunks 4 503\n",
      "Len of text chunks 2 351\n",
      "seed question ['What are some potential causes of workplace conflict?', 'How can conflict be managed and resolved in the workplace?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|████████████████████████████▌                      | 28/50 [04:58<03:12,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|████████████████████████████████████▋              | 36/50 [05:16<01:25,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 1 284\n",
      "seed question ['Why is a security sales training program important for Field Security?', 'What are the objectives of the security sales enablement program?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████████████████████████████████████████▉     | 45/50 [05:31<00:21,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "55it [05:40,  3.01s/it]                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 1 186\n",
      "seed question ['What is the goal of the Ecommerce Motion working group?', 'Where can the Google Doc for the Ecommerce Motion working group be accessed?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "66it [06:04,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "78it [06:30,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 7 117\n",
      "Len of text chunks 3 354\n",
      "Len of text chunks 3 487\n",
      "Len of text chunks 1 310\n",
      "seed question ['What are some of the actions suggested to improve design discussions?', 'Who supported UX Research in conducting user interviews?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': \"The question mentions a 'given context' but does not provide what that context is\", 'verdict': 'No'}\n",
      "{'reason': \"The question mentions 'UX Research' and 'user interviews' but does not specify in what context or project, making it unclear\", 'verdict': 'No'}\n",
      "rewritten question Who supported UX Research in conducting user interviews in the UX department?\n",
      "{'reason': \"The question mentions 'UX Research' and 'UX department' but it is not clear which organization or context this refers to\", 'verdict': 'No'}\n",
      "Len of text chunks 3 435\n",
      "Len of text chunks 1 191\n",
      "Len of text chunks 1 248\n",
      "Len of text chunks 3 475\n",
      "Len of text chunks 3 488\n",
      "Len of text chunks 3 494\n",
      "seed question ['What type of security testing does GitLab conduct on its production architecture?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "91it [07:50,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 1 229\n",
      "seed question ['What is Vendorpedia used for?', 'How can GitLab team members access AnswerBase?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "105it [08:14,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "120it [08:33,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 407\n",
      "seed question ['What measures has GitLab taken to address the concern of an economic downturn?', 'How does GitLab address the concern of a pandemic like COVID-19?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "136it [09:11,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "153it [09:48,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 355\n",
      "seed question ['Are FAQs considered unstructured content?', 'What is the recommended method for announcing important decisions in a distributed organization?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "171it [09:59,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "190it [10:08,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 481\n",
      "seed question ['When should threat modeling be done in the development process?', 'What is the purpose of threat modeling?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "210it [10:36,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'Each question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "231it [11:03,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 367\n",
      "seed question ['What are the benefits of using SOCIAL STYLES?', 'How can the SOCIAL STYLES framework enhance team performance?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "253it [11:35,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "276it [11:59,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 432\n",
      "seed question ['What are the roles and responsibilities of the working group in automotive software development?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "300it [12:31,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 497\n",
      "seed question ['What is the purpose of having cadences in a company?', 'How do cadences at GitLab differ from each other?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': \"The question mentions 'this cadence' which is not specified, making it unclear\", 'verdict': 'No'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "325it [13:12,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 1 294\n",
      "Len of text chunks 4 333\n",
      "Len of text chunks 5 462\n",
      "Len of text chunks 3 264\n",
      "Len of text chunks 4 478\n",
      "Len of text chunks 2 359\n",
      "seed question ['What is the purpose of the Security Shadow Program?', 'Who can join the Security Shadow Program?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "351it [13:53,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "378it [14:09,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 458\n",
      "seed question ['What is the purpose of the New Hire Security Training?', 'Who is responsible for approving significant changes and exceptions to this standard?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "406it [14:44,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': \"The question is unclear without specifying what 'standard' is being referred to\", 'verdict': 'No'}\n",
      "rewritten question Who is responsible for approving significant changes and exceptions to the standard outlined in the Roles & Responsibilities section?\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "435it [15:01,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 259\n",
      "Len of text chunks 1 241\n",
      "seed question [\"What are the benefits of GitLab's user access review?\"]\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is too specific and requires additional context', 'verdict': 'No'}\n",
      "Len of text chunks 4 445\n",
      "seed question ['What are the procedures followed by TPRM engineers for vendors accessing or being transmitted different classifications of GitLab data?', 'When should TPRM engineers perform additional Security review or validation?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "465it [16:05,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "496it [16:35,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 433\n",
      "Len of text chunks 1 193\n",
      "seed question ['What are the business goals of the Learning Experience Working Group?', 'How will the Learning Experience Working Group determine if Field Enablement and Learning & Development use cases can be met by the Professional Services Learning Experience Platform?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "528it [16:51,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "561it [16:57,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 3 381\n",
      "Len of text chunks 4 450\n",
      "seed question ['What are KPIs?', 'Where are the Sales KPIs documented?']\n",
      "{'reason': 'The question is clear', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "595it [17:30,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "630it [17:49,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 1 303\n",
      "Len of text chunks 3 483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahules/belar/src/ragas/testset/utils.py:16: UserWarning: Invalid json\n",
      "  warnings.warn(\"Invalid json\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed question []\n",
      "Len of text chunks 2 354\n",
      "seed question ['What are some common criticisms of Open Core?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "666it [18:05,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 503\n",
      "seed question ['What should be done to begin a security incident investigation?', 'How should artifacts from an investigation be handled?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "703it [18:59,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "741it [19:30,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 4 275\n",
      "seed question [\"What is the highest level in GitLab's organizational structure?\", \"How many layers are there in GitLab's organizational structure?\"]\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "780it [19:56,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "820it [20:14,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 500\n",
      "Len of text chunks 3 462\n",
      "Len of text chunks 2 354\n",
      "seed question ['What are the advantages of using a single application in GitLab?', 'How does increasing Stages per Organization drive revenue in GitLab?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The questions can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "861it [21:02,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "903it [21:29,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 351\n",
      "Len of text chunks 2 275\n",
      "seed question ['What is the purpose of coaching in an all-remote organization?', 'How does coaching facilitate the career development of team members?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "946it [27:04,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear', 'verdict': 'Yes'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "990it [27:33,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of text chunks 2 350\n",
      "Len of text chunks 2 506\n",
      "seed question ['What is the purpose of having stable counterparts in Product Groups?', 'Why does GitLab avoid having internal platform groups?']\n",
      "{'reason': 'The question can be understood without additional context', 'verdict': 'Yes'}\n",
      "{'reason': 'The question is clear', 'verdict': 'Yes'}\n"
     ]
    }
   ],
   "source": [
    "test_df = testsetgenerator.generate(documents,test_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b3b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d58feb8-8604-4854-9d1d-7aa27495f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_pandas().to_csv(\"gitlab_3wh_50.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "589a6686",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "434afa58",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "4 is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: 4 is not in list"
     ]
    }
   ],
   "source": [
    "x.index(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ca508bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03897e",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Conditional question evol is working\n",
    "- reasoning/multi context are not working as expected\n",
    "- Almost all questions are closed endeded \n",
    "- Almost all questions start with \"What\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e52f7",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "144547a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file.markdown_reader import MarkdownReader\n",
    "from llama_index.schema import Document\n",
    "from typing import List, Dict, Optional\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e30ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagasMdReader(MarkdownReader):\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_file_metadata(path):\n",
    "        \n",
    "        return {\"file_name\":os.path.basename(path),\n",
    "                \"dirname\":os.path.dirname(path)}\n",
    "        \n",
    "    \n",
    "    def get_local_metadata(self, text):\n",
    "        \n",
    "\n",
    "        return_dict = {}\n",
    "        pattern = r'---\\s*title:\\s*(.+?)(?:\\s*description:\\s*\"(.*?)\")?\\s*---'\n",
    "        match = re.findall(pattern, text)\n",
    "        if match:\n",
    "            title,desc = match[0]\n",
    "            return_dict['title'] = title\n",
    "            return_dict['description'] = desc if desc else None\n",
    "            \n",
    "        return return_dict\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    def load_data(\n",
    "        self, file: Path, extra_info: Optional[Dict] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Parse file into string.\"\"\"\n",
    "        \n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        if self._remove_hyperlinks:\n",
    "            content = self.remove_hyperlinks(content)\n",
    "        if self._remove_images:\n",
    "            content = self.remove_images(content)\n",
    "            \n",
    "        local_metadata = self.get_local_metadata(content)\n",
    "\n",
    "        extra_info = dict(extra_info,**local_metadata) if local_metadata else extra_info\n",
    "        return [Document(text=content,metadata=extra_info)]\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9c01902",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_reader = RagasMdReader(remove_hyperlinks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bca9d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm2(prompt, **kwargs):\n",
    "    response = client.chat.completions.create(\n",
    "        model=kwargs.get(\"model\", \"gpt-3.5-turbo\"),\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=kwargs.get(\"temperature\", 0),\n",
    "        top_p=kwargs.get(\"top_p\", 1),\n",
    "        frequency_penalty=kwargs.get(\"frequency_penalty\", 0.0),\n",
    "        presence_penalty=kwargs.get(\"presence_penalty\", 0.0),\n",
    "        max_tokens=kwargs.get(\"max_tokens\", 500),\n",
    "        n=kwargs.get(\"n\", 1),\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feec19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.prompts import SEED_QUESTION, EVOLUTION_ELIMINATION, REWRITE_QUESTION, FILTER_QUESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bec0dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /Users/shahules/Myprojects/rag-experiments/gitlab-handbook/data/handbook/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95d00bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [\n",
    "    \"/Users/shahules/Myprojects/rag-experiments/gitlab-handbook/data/handbook/leadership/\",\n",
    "    \"/Users/shahules/Myprojects/rag-experiments/gitlab-handbook/data/handbook/company/\",\n",
    "    \"/Users/shahules/Myprojects/rag-experiments/gitlab-handbook/data/handbook/infrastructure-standards/\",\n",
    "    \"/Users/shahules/Myprojects/rag-experiments/gitlab-handbook/data/handbook/communication/\",\n",
    "    \"/Users/shahules/Myprojects/rag-experiments/gitlab-handbook/data/handbook/security\"\n",
    "]\n",
    "documents = []\n",
    "for dir_path in dirs:\n",
    "    loader = SimpleDirectoryReader(dir_path, \n",
    "                                   recursive=True,\n",
    "                                  file_extractor={\".md\":md_reader},\n",
    "                                file_metadata=RagasMdReader.get_file_metadata)\n",
    "    \n",
    "    documents.extend(loader.load_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64e715b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([doc for doc in documents if doc.metadata.get(\"file_name\") is None ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "15e60505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "---\n",
    "title: \"Ask Me Anything\"\n",
    "description: \"Learn and ask questions at GitLab's Ask Me Anything (AMA) meetings\"\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "# Regular expression to capture title and description\n",
    "pattern_optional_description = '---\\s*title:\\s*(.+?)(?:\\s*description:\\s*\"(.*?)\")?\\s*---'\n",
    "\n",
    "# Finding matches\n",
    "matches = re.match(pattern_optional_description, text)\n",
    "\n",
    "matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "08037476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4cc57d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile('---\\s*title:\\s*(.+?)(?:\\s*description:\\s*\"(.*?)\")?\\s*---')\n",
    "[m.groupdict() for m in pattern.finditer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58b4cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "---\n",
    "title: \"Book clubs\"\n",
    "---\n",
    "\n",
    "From time to time, we run internal book clubs on a book from one of our resource lists. All are welcome! However,\n",
    "each club has a suggested audience to indicate roles to which the content is tailored.\n",
    "\n",
    "- [Leadership]({{< ref \"_index.md#books\" >}})\n",
    "- [Development](https://about.gitlab.com/handbook/engineering/development/#books)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3a953420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_optional_description = r'---\\s*title:\\s*\"(.+?)\"\\s*---'\n",
    "re.findall(pattern,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64f4ad42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b94cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4bc085c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testset = testsetgenerator.generate(documents, test_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26f40412",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.to_pandas().to_csv(\"gitlab_communication_company_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ae48ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92931c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1=\"How should I use my notification settings in Slack?\"\n",
    "q2=\"What's the best way to manage Slack notification settings for efficient communication and minimal disruptions?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1052549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = EVOLUTION_ELIMINATION.prompt.template.format(question1=q1, question2=q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37bedda",
   "metadata": {},
   "source": [
    "## Create multi context\n",
    "- find similar docs using metadata \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a0773b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index.indices.query.embedding_utils import get_top_k_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bac7c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "41792a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [doc.metadata.get('title').strip('\"') for doc in documents if doc.metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "165254ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.embed_documents(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "064ef79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "19d68a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, indices = get_top_k_embeddings(embeddings[k],embeddings,similarity_cutoff=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ac6d8408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seach query :Top Cross-Functional Initiatives\n",
      " Results: [['Top Cross-Functional Initiatives', 'Building High Performing Teams', 'Product Career Development Framework Working Group', 'Leadership', 'Single Codebase Working Group']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Seach query :{titles[k]}\\n Results: [{[titles[i] for i in indices[:5]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4c9938",
   "metadata": {},
   "source": [
    "## Merge documents based on meta-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0695effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import HumanMessagePromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2f28fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITE_QUESTION = HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "Rewrite the given question so that it can be answered without context.\n",
    "\n",
    "Question: When was he born?\n",
    "Context : Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time. He was born on 14 March 1879.\n",
    "Rewritten question: When was Albert Einstein born? \n",
    "\n",
    "Context: A clothes iron (also flatiron, smoothing iron, or simply iron) is a small appliance that, when heated, is used to press clothes to remove wrinkles and unwanted creases. Domestic irons generally range in operating temperature from between 121 °C (250 °F) to 182 °C (360 °F). It is named for the metal (iron) of which the device was historically made, and the use of it is generally called ironing, the final step in the process of laundering clothes.\n",
    "Question: What is temperate range of the device?\n",
    "Rewritten question: What is temperate range of clothes iron?\n",
    "\n",
    "\n",
    "Question:{question}\n",
    "Context: {context}\n",
    "Rewritten question:\n",
    "\"\"\")\n",
    "\n",
    "QUERY =  HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "Rewrite the question using given context so that it can be read and answered without any extra information.\n",
    "\n",
    "Question:{question}\n",
    "Context: {context}\n",
    "\"\"\")\n",
    "\n",
    "text = \"\"\"\n",
    "The Mona Lisa, painted by Leonardo da Vinci in the early 16th century, is one of the most famous and valuable paintings in the world.\n",
    "Known for its enigmatic expression and innovative use of sfumato, the painting has become a symbol of Renaissance art.\n",
    "The Mona Lisa is displayed in the Louvre Museum in Paris and attracts millions of visitors annually.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fc666f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(indices):\n",
    "    \n",
    "    return '\\n'.join([documents[idx].get_content() for idx in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d871744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "455779c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm2(SEED_QUESTION.format(context=get_content([49,50])).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3baa48ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the purpose of the GPT-3 Editor in the CORE framework?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = response.choices[0].message.content\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2839379b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rewrite the question using given context so that it can be read and answered without any extra information.\n",
      "\n",
      "Question:What is the impact of training data size on the model's performance?\n",
      "Context: Preprint.\n",
      "Table 2: Overall experiment results on six tasks. Bold numbers indicate the best performance among\n",
      "non-proprietary models, and gray-colored bold text indicates the best proprietary model when\n",
      "they outperforms all non-proprietary models.∗indicates concurrent or recent results reported by\n",
      "concurrent work. – indicates numbers that are not reported by the original papers or are not applicable.\n",
      "Models are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em,\n",
      "rouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\n",
      "Short-form Closed-set Long-form generations (with citations)\n",
      "PopQA TQA Pub ARC Bio ASQA\n",
      "LM (acc) (acc) (acc) (acc) (FS) (em) (rg) (mau) (pre) (rec)\n",
      "LMs with proprietary data\n",
      "Llama2-c 13B 20.0 59.3 49.4 38.4 55.9 22.4 29.6 28.6 – –\n",
      "Ret-Llama2-c 13B 51.8 59.8 52.1 37.9 79.9 32.8 34.8 43.8 19.8 36.1\n",
      "ChatGPT 29.3 74.3 70.1 75.3 71.8 35.3 36.2 68.8 – –\n",
      "Ret-ChatGPT 50.8 65.7 54.7 75.3 – 40.7 39.9 79.7 65.1 76.6\n",
      "Perplexity.ai – – – – 71.2 – – – – –\n",
      "Baselines without retrieval\n",
      "Llama2 7B 14.7 30.5 34.2 21.8 44.5 7.9 15.3 19.0 – –\n",
      "Alpaca 7B 23.6 54.5 49.8 45.0 45.8 18.8 29.4 61.7 – –\n",
      "Llama2 13B 14.7 38.5 29.4 29.4 53.4 7.2 12.4 16.0 – –\n",
      "Alpaca 13B 24.4 61.3 55.5 54.9 50.2 22.9 32.0 70.6 – –\n",
      "CoVE 65B* – – – – 71.2 – – – – –\n",
      "Baselines with retrieval\n",
      "Toolformer* 6B – 48.8 – – – – – – – –\n",
      "Llama2 7B 38.2 42.5 30.0 48.0 78.0 15.2 22.1 32.0 2.9 4.0\n",
      "Alpaca 7B 46.7 64.1 40.2 48.0 76.6 30.9 33.3 57.9 5.5 7.2\n",
      "Llama2-FT 7B 48.7 57.3 64.3 65.8 78.2 31.0 35.8 51.2 5.0 7.5\n",
      "SAIL* 7B – – 69.2 48.4 – – – – – –\n",
      "Llama2 13B 45.7 47.0 30.2 26.0 77.5 16.3 20.5 24.7 2.3 3.6\n",
      "Alpaca 13B 46.1 66.9 51.1 57.6 77.7 34.8 36.7 56.6 2.0 3.8\n",
      "Our SELF-RAG 7B 54.9 66.4 72.4 67.3 81.2 30.0 35.7 74.3 66.9 67.8\n",
      "Our SELF-RAG 13B 55.8 69.3 74.5 73.1 80.2 31.7 37.0 71.6 70.3 71.3\n",
      "5 R ESULTS AND ANALYSIS\n",
      "5.1 M AINRESULTS\n",
      "Comparison against baselines without retrieval. Table 2 (top) presents the baselines without\n",
      "retrieval. Our SELF-RAG(bottom two rows) demonstrates a substantial performance advantage\n",
      "over supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA,\n",
      "biography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms\n",
      "a concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation\n",
      "task, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which\n",
      "iteratively prompts Llama2 65Bto refine output.\n",
      "Comparison against baselines with retrieval. As shown in Tables 2 (bottom), our SELF-RAGalso\n",
      "outperforms existing RAG in many tasks, obtaining the best performance among non-proprietary\n",
      "LM-based models on all tasks. While our method outperforms other baselines, on PopQA or Bio,\n",
      "powerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca) show large gains from\n",
      "their non-retrieval baselines. However, we found that these baselines provide limited solutions for\n",
      "tasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth\n",
      "and ARC-Challenge, baselines with retrieval do not improve performance notably from their no-\n",
      "retrieval counterparts. We also observe that most baselines with retrieval struggle to improve citation\n",
      "accuracy. On ASQA, our model shows significantly higher citation precision and recall than all\n",
      "models except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits superior efficacy\n",
      "in this particular task, surpassing smaller LMs. Our SELF-RAGbridges this performance gap, even\n",
      "outperforming ChatGPT in citation precision, which measures whether the model-generated claim is\n",
      "fully supported by cited evidence. We also found that on the metrics for factual precision, SELF-RAG\n",
      "7B occasionally outperforms our 13B due to the tendency of smaller SELF-RAGto often generate\n",
      "8\n",
      "Preprint.\n",
      "PQA Med AS\n",
      "(acc) (acc) (em)\n",
      "SELF-RAG(50k) 45.5 73.5 32.1\n",
      "Training\n",
      "No Retriever R 43.6 67.8 31.0\n",
      "No Critic C 42.6 72.0 18.1\n",
      "Test\n",
      "No retrieval 24.7 73.0 –\n",
      "Hard constraints 28.3 72.6 –\n",
      "Retrieve top1 41.8 73.1 28.6\n",
      "Remove ISSUP 44.1 73.2 30.6\n",
      "(a) Ablation\n",
      "1 270.070.5Precision\n",
      "1 2\n",
      "Weight for IsSupport9095Mauve\n",
      " (b) Customization\n",
      "0.0 0.2 0.4 0.60.980.990.991.00Accuracy\n",
      "PubHealth\n",
      "0.0 0.2 0.4 0.6\n",
      "Retrieval Threshold0.60.81.0AccuracyPopQA0.00.51.0\n",
      "Frequency\n",
      "0.250.500.751.00\n",
      "Frequency\n",
      " (c) Retrieval\n",
      "Figure 3: Analysis on SELF-RAG:(a)Ablation studies for key components of SELF-RAGtraining\n",
      "and inference based on our 7B model. (b) Effects of soft weights on ASQA citation precision and\n",
      "Mauve (fluency). (c) Retrieval frequency andnormalized accuracy on PubHealth and PopQA.\n",
      "precisely grounded yet shorter outputs. Llama2-FT 7B, which is the baseline LM trained on the same\n",
      "instruction-output pairs as SELF-RAGwithout retrieval or self-reflection and is retrieval-augmented\n",
      "at test time only, lags behind S ELF-RAG. This result indicates S ELF-RAGgains are not solely from\n",
      "training data and demonstrate the effectiveness of S ELF-RAGframework.\n",
      "5.2 A NALYSIS\n",
      "Ablation studies. We conduct a set of ablations of our framework to identify which factors play\n",
      "key roles. We evaluate two model variants trained differently than our model: No Retriever trains an\n",
      "LM using the standard instruction-following method given instruction-output pairs, without retrieved\n",
      "passages; No Critic trains an LM trained with input-output pairs that are always augmented with the\n",
      "top one retrieved document without reflection tokens. This is similar to SAIL (Luo et al., 2023), and\n",
      "we use our instruction-output data instead of using the Alpaca dataset (Dubois et al., 2023), as in\n",
      "SAIL. We also conduct ablation on our inference-time algorithm, including No retrieval disables\n",
      "retrieval during inference; Hard constraints indicates the model performance that retrieves when\n",
      "Retrieve =Yes instead of using the adaptive threshold; Retrieve top 1 always retrieves and uses the\n",
      "top one document only, similar to standard RAG approaches; Remove ISSUPindicates the model\n",
      "performance that removes ISSUPscore only during critique-guided beam search in Eq. 4. In this\n",
      "ablation experiment, we use a training instance size of 50k for a more efficient exploration of training\n",
      "variations. Later in this section, we conduct an analysis of the effect of training data size. We conduct\n",
      "the ablation studies on three datasets, PopQA, PubHealth, and ASQA. On ASQA, we evaluate models\n",
      "on sampled 150 instances and exclude ablations involving adaptive or no retrieval processes.\n",
      "We show in Table 3a the ablation results. The top part of the table shows results for training ablations,\n",
      "and the bottom part is for inference ablations. We see that all components play important roles. We\n",
      "also observe a large performance gap between SELF-RAGand No Retriever or Critic baselines across\n",
      "tasks, indicating that training an LM with those models largely contributes to the performance gain of\n",
      "SELF-RAG. Using the top passages regardless of their relevance (Retrieve top 1) as in conventional\n",
      "RAG approaches causes a large drop in PopQA and ASQA, and removing ISSUPduring the beam\n",
      "search results hurts performance on ASQA. This demonstrates the effectiveness of SELF-RAG’s\n",
      "capabilities of carefully selecting generations based fine-grained multiple criterion, instead of naively\n",
      "using all of the top passages from the retrieval model or solely depending on relevance scores.\n",
      "Effects of inference-time customization. One key benefit of our proposed framework is that it\n",
      "enables us to control how much each critique type affects the final generation sampling. We analyze\n",
      "the effects of different parameter weights on the top of our 7B model during inference time on\n",
      "ASQA, where multiple evaluation aspects are considered. Figure 3b shows the effects of changing\n",
      "the weighting term for ISSUP, which criticizes how supported the output is by the text passage. As\n",
      "the figure shows, increasing the weight leads to positive effects on the models’ citation precision\n",
      "since this puts more emphasis on whether model generation is supported by the evidence. On the\n",
      "9\n",
      "Preprint.\n",
      "0 50 100 150\n",
      "Num of training (k)3540455055Perfomance\n",
      "(a) PopQA\n",
      "0 100\n",
      "Num of training (k)717273\n",
      " (b) PubHealth\n",
      "0 100\n",
      "Num of training (k)4060\n",
      " (c) ASQA (prec)Pop Bio.\n",
      "S & P 92.5 70.0\n",
      "ISREL 95.0 90.0\n",
      "ISSUP 90.0 85.0\n",
      "(d) Human evaluation on PopQA\n",
      "and Bio generation.\n",
      "Figure 4: Training scale and Human analysis: (a) (b) (c) Training scale analysis shows the effect\n",
      "of the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)\n",
      "Human analysis on S ELF-RAGoutputs as well as reflection tokens.\n",
      "contrary, a larger weight results in lower MAUVE scores: when generation gets longer and more\n",
      "fluent, there are often more claims that are not fully supported by citations, consistent with findings\n",
      "by Liu et al. (2023a). Our framework lets practitioners choose and customize models’ behaviors at\n",
      "test time by adjusting such parameters without requiring additional training.\n",
      "Efficiency and accuracy trade-off. Using our framework, practitioners can adjust how often retrieval\n",
      "occurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects\n",
      "overall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers\n",
      "of threshold δ(larger δresults in less retrieval) on PubHealth and PopQA. Figure 3c shows that\n",
      "the model’s retrieval frequencies dramatically change on both datasets. as δvaries. On one hand,\n",
      "performance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.\n",
      "Effects of training data size. We conduct an analysis of how the data scale affects the model’s\n",
      "performance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original\n",
      "150k training instances, and fine-tune four SELF-RAG 7Bvariants on those subsets. Then, we compare\n",
      "the model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELF-\n",
      "RAGtrained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models’\n",
      "performance trained on different amount of data. Across all datasets, increasing data size often shows\n",
      "upward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do\n",
      "not observed such significant improvements on Llama2-FT 7Bwhen increasing the training data from\n",
      "50k to 150k. These results also indicate that further expanding the training data of SELF-RAGmay\n",
      "lead to further improvements, although in this work we limit our training data size to 150k.\n",
      "Human evaluations. We conduct small human evaluations on SELF-RAGoutputs, as well as the\n",
      "reliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio\n",
      "results. Following Menick et al. (2022), human annotators evaluate S&P , which indicates whether\n",
      "the model output is plausible (i.e., the output is a reasonable and on-topic response to the question\n",
      "as if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to\n",
      "verify the validity of the answer). For S&P, we do not consider the instances where SELF-RAG\n",
      "predicts irrelevant orno support . We then ask our annotators whether the model-predicted\n",
      "reflection tokens about ISRELand ISSUPmatch their inspections (e.g., whether the fully supported\n",
      "output is supported by the cited evidence). Human annotators find SELF-RAGanswers are often\n",
      "plausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is\n",
      "consistent with Menick et al. (2022). Human annotators also find ISRELand ISSUPreflection token\n",
      "predictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated\n",
      "examples and explanations on assessments.\n",
      "6 C ONCLUSION\n",
      "This work introduces SELF-RAG, a new framework to enhance the quality and factuality of LLMs\n",
      "through retrieval on demand and self-reflection. SELF-RAGtrains an LM to learn to retrieve, generate,\n",
      "and critique text passages and its own generation by predicting the next tokens from its original\n",
      "vocabulary as well as newly added special tokens, called reflection tokens. SELF-RAGfurther enables\n",
      "the tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on\n",
      "six tasks using multiple metrics demonstrate that SELF-RAGsignificantly outperforms LLMs with\n",
      "more parameters or with conventional retrieval-augmented generation approaches.\n",
      "10\n",
      "\n",
      "Rewritten question: How does the size of the training data affect the performance of the SELF-RAG model according to the experiment results?\n"
     ]
    }
   ],
   "source": [
    "print(QUERY.format(question=q, context=get_content([12,13,14])).content)\n",
    "print('Rewritten question: How does the size of the training data affect the performance of the SELF-RAG model according to the experiment results?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fd1f148b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1909"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_content([12,13,14]).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3bad307a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimuli BMS [41] Pan et al. [29] Salicon [15] ML-Net [9] Ours Ground Truth\n",
      "Figure 3: Qualitative performance comparison .\n",
      "SateliteJumbledSocial\n",
      "OutdoorNaturalArt\n",
      "InvertedFractalObjectCartoon\n",
      "OutdoorManMadeIndoorActionPatternNoisy\n",
      "Affective\n",
      "BlackWhiteRandom\n",
      "LowResolutionLineDrawingSketchSketchLineDrawingLowResolutionRandomBlackWhiteAffectiveNoisyPatternActionIndoorOutdoorManMadeCartoonObjectFractalInvertedArtOutdoorNaturalSocialJumbledSatelite\n",
      "0.00.20.40.60.81.0\n",
      "Figure 4: Classiﬁcation confusion matrix .\n",
      "bias. Location bias is an important component of human\n",
      "visual attention, and so should be included in any model.\n",
      "Table 2 shows the results of the 5-fold cross validation\n",
      "tests on the CAT2000 dataset. We achieve the best perfor-\n",
      "mance of the tested models. Table 3 shows the performance\n",
      "on the held out test images. Our model achieves the second\n",
      "best performance behind the DeepFix model [22]. Com-\n",
      "pared with DeepFix our network is shallower with less pa-\n",
      "rameters. The DeepFix model is not publicly available to\n",
      "test on the cross-validation data. Figure 3 shows example\n",
      "saliency maps generated by the tested models.\n",
      "Table 4 shows the results on the Salicon test set. Under\n",
      "this dataset our model does not have a performance advan-CC SAUC AUC Judd\n",
      "Our Model 0.730 0.771 0.861\n",
      "ML-Net [9] 0.7430 0.7680 0.8660\n",
      "Deep Convnet [29] 0.6220 0.7240 0.8580\n",
      "Shallow Convnet [29] 0.5957 0.6698 0.8364\n",
      "Rare 2012 Improved [31] 0.5108 0.6644 0.8148\n",
      "Baseline: BMS [41] 0.4268 0.6935 0.7899\n",
      "Baseline: GBVS [12] 0.4212 0.6303 0.7899\n",
      "Baseline: Itti [16] 0.2046 0.6101 0.6669\n",
      "Table 4: Salicon test set results.\n",
      "tage over ML-Net. This is largely because the dataset does\n",
      "not have diverse categories like the CAT-2000 dataset. Nev-\n",
      "ertheless, the performance of our model is roughly equiva-\n",
      "lent to the ML-net model.\n",
      "Comparison with averaging ensemble Given that the\n",
      "mixture model can be seen as an ensemble, it is important\n",
      "to compare the performance with a vanilla ensemble. We\n",
      "generate an ensemble of 5 networks where each network is\n",
      "trained on a random 80% of the training split. The saliency\n",
      "maps from each ensemble member are averaged to produce\n",
      "the ﬁnal saliency map. Our model still achieves superior\n",
      "performance compared with the ensemble (Table 5). Addi-\n",
      "tionally, our model is signiﬁcantly faster and requires less\n",
      "storage space due to the weight sharing.\n"
     ]
    }
   ],
   "source": [
    "print(get_content([14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8464da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser.simple import SimpleNodeParser\n",
    "node_parser = SimpleNodeParser.from_defaults(\n",
    "            chunk_size=1000, chunk_overlap=0, include_metadata=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec05e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = node_parser.get_nodes_from_documents(\n",
    "            documents=documents\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a6cd93d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '2',\n",
       " 'file_name': '00891b5a3dbbe4dae6b06432c3f335f8.pdf',\n",
       " 'file_path': 'arxiv-papers/00891b5a3dbbe4dae6b06432c3f335f8.pdf',\n",
       " 'creation_date': '2023-11-14',\n",
       " 'last_modified_date': '2023-11-14',\n",
       " 'last_accessed_date': '2023-11-14'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "03315c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0c61a824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fe3e30",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c5cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e60c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/shahules/Downloads/gitlab_communication_company_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bbae5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'seed_question', 'question', 'context', 'answer',\n",
       "       'question_type', 'episode_done', 'evolution_elimination'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a7cafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(['Unnamed: 0','seed_question','evolution_elimination'],axis=1)\n",
    "df['question'] = df['question'].apply(lambda x : x.replace(\"Casual Rewrite:\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d32ae2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"gitlab_company_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61f0af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6b58dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6fe6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What is the percentage of repetition in the generations produced by RETRO compared to GPT across different sizes?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b5ebe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm2(FILTER_QUESTION.format(question=question).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748f90fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "ragas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
