{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd28a91a",
   "metadata": {},
   "source": [
    "# Vision\n",
    "**Develop Unsupervised model assisted evaluation methods**\n",
    "\n",
    "**Factual consistency**\n",
    "- NLI\n",
    "- QAQG\n",
    "\n",
    "**Relevance**\n",
    "- Prompt based scoring and normalisation\n",
    "\n",
    "**Retriever score**\n",
    "- Crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d341594d",
   "metadata": {},
   "source": [
    "## Logs\n",
    "- Factuality NLI\n",
    "    - Without CoT\n",
    "    - With CoT ( WIN)  \n",
    "    - WikiQA \n",
    "        - generated non factual answer for measuring factuality agreement.\n",
    "        - Kendall Score = 0.7\n",
    "    - HotPotQA\n",
    "        - Kendall Score = \n",
    "    - Possible Improvements \n",
    "        - improve statement generation\n",
    "\n",
    "- Relevance scores\n",
    "    - QGen method\n",
    "        - models tried : t5-base / gptneo-125M\n",
    "        - WikiQA\n",
    "            - Kendall score = 0.65\n",
    "            - observations : finetune model on prompt/answer pairs to improve performance.\n",
    "    - Cross-encoder method\n",
    "        - models tried : distilbert \n",
    "        - WikiQA\n",
    "            - kendall score = 0.63\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bfb2480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/alerts/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import kendalltau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aa3b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/shahules/belar/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9adac051",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_KEY =  json.load(open('/Users/shahules/openai-key.json'))[\"jj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a37bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2a602",
   "metadata": {},
   "source": [
    "## OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bce4c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = OPENAI_KEY\n",
    "def llm(prompt,**kwargs):\n",
    "    response = openai.Completion.create(\n",
    "      model=kwargs.get(\"model\",\"text-davinci-003\"),\n",
    "      prompt=prompt,\n",
    "      temperature=kwargs.get(\"temperature\",0),\n",
    "      top_p=kwargs.get(\"top_p\",1),\n",
    "      frequency_penalty=kwargs.get(\"frequency_penalty\",0.0),\n",
    "      presence_penalty=kwargs.get(\"presence_penalty\",0.0),\n",
    "      max_tokens=kwargs.get(\"max_tokens\",500),\n",
    "      logprobs=kwargs.get(\"logprobs\",1),\n",
    "      n=kwargs.get(\"n\",1),\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d9b4e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_logger(data,filename=\"nli_check\"):\n",
    "    output = json.load(open(filename+'.json'))\n",
    "    output.append(data)\n",
    "    with open(filename+'.json',\"w\") as file:\n",
    "        json.dump(output,file,indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142763a0",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89bf317c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/shahules/.cache/huggingface/datasets/explodinggradients___parquet/explodinggradients--ragas-wikiqa-5b5116e5cb909aca/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|████████████████████████████████████████████████████| 1/1 [00:00<00:00, 163.10it/s]\n"
     ]
    }
   ],
   "source": [
    "wikiqa_ragas = load_dataset(\"explodinggradients/ragas-wikiqa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d3a69",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be345ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tau(target, prediction):\n",
    "    target = [np.argsort(item) for item in target]\n",
    "    prediction = [np.argsort(item) for item in prediction]\n",
    "    return kendalltau(target,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5563146",
   "metadata": {},
   "source": [
    "## QA-QG paradigm\n",
    "- Generate question and answer pair from `generated answer`.\n",
    "- Given `context`, ask these questions\n",
    "- Verify answer correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3e35532",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Question_generation = \"\"\"Given a text, extract {} noun phrases and create questions for each based on given text.\n",
    "text: Albert Einstein was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. Best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\n",
    "A: Germany\n",
    "Q: Where was Albert Einstein born?\n",
    "A: theory of relativity\n",
    "Q: What is Albert Einstein best known for?\n",
    "text: {}\n",
    "\"\"\"\n",
    "\n",
    "Question_answering = \"\"\"Given a text and set of questions, answer the questions\n",
    "text: Albert Einstein was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. Best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\n",
    "questions: Where was Albert Einstein born?\\n\\nWhat is Albert Einstein best known for?\n",
    "answers:Germany\\n\\ntheory of relativity\n",
    "text: {}\n",
    "questions:{}\n",
    "answers:\"\"\"\n",
    "\n",
    "Answer_verification = \"\"\"Given a set of questions, correct answer and student's answer return the number of questions incorrectly answered by student.\n",
    "Where was Albert Einstein born?\\nCorrect answer: Germany\\nStudent answer:India\\n\\n\n",
    "What is Albert Einstein best known for?\\nCorrect answer:  theory of relativity\\nStudent answer: theory of relativity\\n\\n\n",
    "Number of incorrect answers:1\n",
    "{}\n",
    "Number of incorrect answers:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "335081e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QAQG_fun(question,context,answer):\n",
    "    \n",
    "    \"\"\"\n",
    "    returns number of factual inconsistencies.\n",
    "    \"\"\"\n",
    "    def answer_ver(qstn,answer,cand):\n",
    "        \n",
    "        return f\"{qstn}\\nCorrect answer: {answer}\\nStudent answer: {cand}\"\n",
    "    \n",
    "    num = len(answer.split('.')) - 1\n",
    "    prompt = Question_generation.format(num,answer)\n",
    "    output = llm(prompt)\n",
    "    qa_pairs = [re.sub(r'A:|Q:','',x).strip() for item in output['choices'][0]['text'].strip().split(\"\\n\\n\") for x in item.split('\\n')]\n",
    "    qa_pairs = [tuple(qa_pairs[i:i+2]) for i in range(0,len(qa_pairs),2)]\n",
    "    print(qa_pairs)\n",
    "    questions = \"\\n\\n\".join([qstn for ans,qstn in qa_pairs])\n",
    "    prompt = Question_answering.format(context,questions)\n",
    "    answers = llm(prompt)['choices'][0]['text'].split('\\n\\n')\n",
    "    \n",
    "    prompt = \"\\n\\n\".join([answer_ver(qstn,ans,cand) for (ans,qstn),cand in zip(qa_pairs,answers)])\n",
    "    output = llm(Answer_verification.format(prompt))['choices'][0]['text'].strip()\n",
    "    return int(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2642e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"The actress who played Lolita, Sue Lyon, was 14 at the time of filming.\"\n",
    "question = \"What was the age of Sue Lyon when she played Lolita?\"\n",
    "context = \"\"\"\n",
    "Lolita is a 1962 psychological comedy-drama film[5] directed by Stanley Kubrick and based on the 1955 novel of the same title by Vladimir Nabokov, who is also credited with writing the screenplay. The film follows Humbert Humbert, a middle-aged literature lecturer who becomes sexually infatuated with Dolores Haze (nicknamed \"Lolita\"), a young adolescent girl. It stars James Mason, Shelley Winters, Peter Sellers and, as the titular character, Sue Lyon.\n",
    "\n",
    "Owing to restrictions imposed by the Motion Picture Production Code, the film toned down the most provocative aspects of the novel, sometimes leaving much to the audience's imagination. The actress who played Lolita, Sue Lyon, was 14 at the time of filming.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26ca4af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sue Lyon', 'Who played the role of Lolita in the movie?')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QAQG_fun(question,context,answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2078ece",
   "metadata": {},
   "source": [
    "## G-Eval\n",
    "- Define criterions to evaluate model.\n",
    "- Normalize `score = prob(s) * s`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca1c56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevence = \"\"\"\n",
    "Evaluation Criteria.\\n\n",
    "Relevance (1-5) - how relevant is the reply to the given question.\n",
    "1. Read the reply and compare it to the question. Check if the given reply\n",
    "actually answers the question, and if it presents them in a clear and logical order.\n",
    "2. The reply should include only required information to answer the question.\n",
    "3. Penalize replies that contain redundancies and excess information.\n",
    "4. Assign a score for Relevance on a scale of 1 to 5, where 1 is the lowest and\n",
    "5 is the highest based on the Evaluation Criteria.\n",
    "\n",
    "question:{}\n",
    "reply:{}\n",
    "score:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd7fed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_eval(question,context,answer):\n",
    "    \n",
    "    prompt = relevence.format(question,answer)\n",
    "    output = llm(prompt)[\"choices\"][0]\n",
    "    prob = np.exp(sum(output[\"logprobs\"][\"token_logprobs\"]))\n",
    "    score = int(output[\"text\"].strip())\n",
    "    print(score)\n",
    "    return prob * score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35113558",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which year did Lolita release?\"\n",
    "answer = \"Lolita film released in 1947.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e82d0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.514920235612768"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_eval(question,context,answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce1baa",
   "metadata": {},
   "source": [
    "## Relevancy Score \n",
    "- Scores `answers` according to `prompt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e00e7a",
   "metadata": {},
   "source": [
    "### QGen scoring method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37a32ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experimental.relevance import QGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9108960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/alerts/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t5_qgen = QGen(\"t5-base\",\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39213a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_relevance(examples):\n",
    "    scores = {}\n",
    "    questions = examples[\"question\"]\n",
    "    for col in COLUMNS:\n",
    "        passage = examples[col]\n",
    "        inputs = list(zip(questions,passage))\n",
    "        scores[f'{col}_relevance'] = t5_qgen.predict(inputs,show_progress=False)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b06d56",
   "metadata": {},
   "source": [
    "- We assume `generated_with_rag > correct_answer > incorrect_answer` for relevancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "595c6b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COLUMNS = [\"generated_with_rag\",\"correct_answer\",\"incorrect_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65cf9e0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    }
   ],
   "source": [
    "output = wikiqa_ragas[\"train\"].select(range(0,10)).map(predict_relevance,batched=True,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9236cf0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.7999999999999998, pvalue=1.2728554897313974e-06)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [[item[f'{k}_relevance'] for k in COLUMNS] for item in output]\n",
    "target = [[2,1,0] for i in range(len(output))]\n",
    "get_tau(target,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609c82e5",
   "metadata": {},
   "source": [
    "### Cross encoder method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6d76ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import cross encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c68ade37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_relevance(examples):\n",
    "    scores = {}\n",
    "    questions = examples[\"question\"]\n",
    "    for col in COLUMNS:\n",
    "        passage = examples[col]\n",
    "        inputs = list(zip(questions,passage))\n",
    "        scores[f'{col}_relevance'] = cross_encoder.predict(inputs,show_progress=False)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440be8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = wikiqa_ragas[\"train\"].select(range(0,10)).map(predict_relevance,batched=True,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d77a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [[item[f'{k}_relevance'] for k in COLUMNS] for item in output]\n",
    "target = [[2,1,0] for i in range(len(output))]\n",
    "get_tau(target,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd9923",
   "metadata": {},
   "source": [
    "## Factuality on HotpotQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5269ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c675e418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'experimental' (namespace)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "088c1505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experimental.nli import NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3f9bd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset hotpot_qa (/Users/shahules/.cache/huggingface/datasets/hotpot_qa/distractor/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5)\n"
     ]
    }
   ],
   "source": [
    "hotpot_qa = load_dataset(\"hotpot_qa\",\"distractor\",split=\"validation\",).select(range(0,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ab98cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_answer_prompt = \"\"\"Given a question and correct answer, generate a plausible wrong answer\n",
    "question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
    "correct answer: yes\n",
    "answer: no\n",
    "question: {}\n",
    "correct answer: {}\n",
    "answer:\"\"\"\n",
    "\n",
    "def generate_false_answers(question,answer):\n",
    "    answer = llm(false_answer_prompt.format(question,answer))['choices'][0]['text'].strip()\n",
    "    return {'false_answer':answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d95c0202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    }
   ],
   "source": [
    "hotpot_qa = hotpot_qa.map(lambda x : generate_false_answers(x[\"question\"],x[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "175d181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(item):\n",
    "    \n",
    "    titles,ids = item['supporting_facts'].values()\n",
    "    title_ids = [item['context']['title'].index(i) for i in titles]\n",
    "    sentences = [item['context']['sentences'][i][k] for i,k in zip(title_ids,item[\"supporting_facts\"][\"sent_id\"])]\n",
    "    orig_context = ' '.join(sentences)\n",
    "    return {'answer_context':orig_context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b550a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    }
   ],
   "source": [
    "hotpot_qa = hotpot_qa.map(lambda x : get_context(x),batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "368b6231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_factuality(examples):\n",
    "    scores = {}\n",
    "    questions = examples[\"question\"]\n",
    "    contexts = examples[\"answer_context\"]\n",
    "    for col in COLUMNS:\n",
    "        answers = examples[col]\n",
    "        scores[f'{col}_factual'] = NLI.score(questions,contexts,answers)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2fdf3069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|                                                | 0/20 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"completion_tokens\": 84,\n",
      "  \"prompt_tokens\": 751,\n",
      "  \"total_tokens\": 835\n",
      "}\n",
      "{\n",
      "  \"completion_tokens\": 394,\n",
      "  \"prompt_tokens\": 2632,\n",
      "  \"total_tokens\": 3026\n",
      "}\n",
      "{\n",
      "  \"completion_tokens\": 86,\n",
      "  \"prompt_tokens\": 709,\n",
      "  \"total_tokens\": 795\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:  20%|████████                                | 4/20 [00:34<02:17,  8.62s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"completion_tokens\": 377,\n",
      "  \"prompt_tokens\": 2636,\n",
      "  \"total_tokens\": 3013\n",
      "}\n",
      "{\n",
      "  \"completion_tokens\": 79,\n",
      "  \"prompt_tokens\": 760,\n",
      "  \"total_tokens\": 839\n",
      "}\n",
      "{\n",
      "  \"completion_tokens\": 292,\n",
      "  \"prompt_tokens\": 2432,\n",
      "  \"total_tokens\": 2724\n",
      "}\n",
      "{\n",
      "  \"completion_tokens\": 73,\n",
      "  \"prompt_tokens\": 754,\n",
      "  \"total_tokens\": 827\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:  40%|████████████████                        | 8/20 [01:04<01:34,  7.92s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"completion_tokens\": 304,\n",
      "  \"prompt_tokens\": 2426,\n",
      "  \"total_tokens\": 2730\n",
      "}\n",
      "{\n",
      "  \"completion_tokens\": 68,\n",
      "  \"prompt_tokens\": 750,\n",
      "  \"total_tokens\": 818\n",
      "}\n",
      "{\n",
      "  \"completion_tokens\": 253,\n",
      "  \"prompt_tokens\": 2483,\n",
      "  \"total_tokens\": 2736\n",
      "}\n",
      "{\n",
      "  \"completion_tokens\": 70,\n",
      "  \"prompt_tokens\": 751,\n",
      "  \"total_tokens\": 821\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:  60%|███████████████████████▍               | 12/20 [01:33<01:00,  7.60s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"completion_tokens\": 280,\n",
      "  \"prompt_tokens\": 2485,\n",
      "  \"total_tokens\": 2765\n",
      "}\n",
      "{\n",
      "  \"completion_tokens\": 72,\n",
      "  \"prompt_tokens\": 744,\n",
      "  \"total_tokens\": 816\n",
      "}\n",
      "{\n",
      "  \"completion_tokens\": 359,\n",
      "  \"prompt_tokens\": 2459,\n",
      "  \"total_tokens\": 2818\n",
      "}\n",
      "{\n",
      "  \"completion_tokens\": 71,\n",
      "  \"prompt_tokens\": 743,\n",
      "  \"total_tokens\": 814\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:  80%|███████████████████████████████▏       | 16/20 [02:03<00:30,  7.56s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"completion_tokens\": 298,\n",
      "  \"prompt_tokens\": 2458,\n",
      "  \"total_tokens\": 2756\n",
      "}\n",
      "{\n",
      "  \"completion_tokens\": 84,\n",
      "  \"prompt_tokens\": 765,\n",
      "  \"total_tokens\": 849\n",
      "}\n",
      "{\n",
      "  \"completion_tokens\": 323,\n",
      "  \"prompt_tokens\": 2480,\n",
      "  \"total_tokens\": 2803\n",
      "}\n",
      "{\n",
      "  \"completion_tokens\": 76,\n",
      "  \"prompt_tokens\": 758,\n",
      "  \"total_tokens\": 834\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"completion_tokens\": 299,\n",
      "  \"prompt_tokens\": 2472,\n",
      "  \"total_tokens\": 2771\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "COLUMNS = [\"answer\",\"false_answer\"]\n",
    "hotpot_qa = hotpot_qa.map(predict_factuality,batched=True,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca2cd14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.3, pvalue=0.06099945558705441)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [[item[f'{k}_factual'] for k in COLUMNS] for item in hotpot_qa]\n",
    "target = [[1,0] for i in range(len(hotpot_qa))]\n",
    "get_tau(target,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c17ecda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 8, 10, 12, 13, 16]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx for idx,item in enumerate(predictions) if (item!=[1.0,0.0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0b08ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "14719115",
   "metadata": {},
   "outputs": [],
   "source": [
    "q,c,a = hotpot_qa[i]['question'],hotpot_qa[i]['answer_context'],hotpot_qa[i]['answer'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e09c1271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"completion_tokens\": 22,\n",
      "  \"prompt_tokens\": 190,\n",
      "  \"total_tokens\": 212\n",
      "}\n",
      "{\n",
      "  \"completion_tokens\": 109,\n",
      "  \"prompt_tokens\": 638,\n",
      "  \"total_tokens\": 747\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLI.score([q],[c],[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b67d68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alerts",
   "language": "python",
   "name": "alerts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
