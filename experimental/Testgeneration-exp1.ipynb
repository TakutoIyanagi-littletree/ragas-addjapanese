{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa0f38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "from openai import OpenAI\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from llama_index import download_loader, SimpleDirectoryReader\n",
    "\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b475fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ragas/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.prompts import SEED_QUESTION, EVOLUTION_ELIMINATION, REWRITE_QUESTION, FILTER_QUESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80548439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm2(prompt, **kwargs):\n",
    "    response = client.chat.completions.create(\n",
    "        model=kwargs.get(\"model\", \"gpt-3.5-turbo-16k\"),\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        temperature=kwargs.get(\"temperature\", 0),\n",
    "        top_p=kwargs.get(\"top_p\", 1),\n",
    "        frequency_penalty=kwargs.get(\"frequency_penalty\", 0.0),\n",
    "        presence_penalty=kwargs.get(\"presence_penalty\", 0.0),\n",
    "        max_tokens=kwargs.get(\"max_tokens\", 500),\n",
    "        n=kwargs.get(\"n\", 1),\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1020ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(\"./arxiv-papers/\",num_files_limit=5)\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3127e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_question = \"\"\"\n",
    "Generate two questions from given context satisfying the rules given below:\n",
    "    2.The question should be framed such that it must be clearly understood without providing context.\n",
    "    3.The question should be fully answerable from information present in given context.\n",
    "    \n",
    "{demonstration}\n",
    "\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "Questions: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a4e48a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstrations = [\n",
    "    {\"context\":\"The Eiffel Tower in Paris was originally intended as a temporary structure, built for the 1889 World's Fair. It was almost dismantled in 1909 but was saved because it was repurposed as a giant radio antenna.\",\n",
    "     \"questions\":[\n",
    "     {\"question_Why\": \"Why was the Eiffel Tower originally planned to be a temporary structure?\"},\n",
    "     {\"question_Was\":\"Was the Eiffel Tower originally designed to be a permanent structure?\"},\n",
    "     {\"question_What\":\"What was the original purpose of the Eiffel Tower when it was built for the 1889 World's Fair?\"},\n",
    "     {\"question_How\":\"How did the Eiffel Tower avoid being dismantled in 1909?\"},\n",
    "     {\"question_Where\":\"Where is the Eiffel Tower?\"}]     \n",
    "    },\n",
    "    \n",
    "    {\n",
    "      \"context\": \"Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.\",\n",
    "      \"questions\": [\n",
    "        {\"question_Why\": \"Why do plants perform photosynthesis?\"},\n",
    "        {\"question_Was\": \"Was photosynthesis discovered in plants, algae, or bacteria first?\"},\n",
    "        {\"question_What\": \"What converts light energy into chemical energy in photosynthesis?\"},\n",
    "        {\"question_How\": \"How do plants capture light energy for photosynthesis?\"},\n",
    "        {\"question_Where\": \"Where in plants does photosynthesis primarily occur?\"},\n",
    "        {\"question_Can\": \"Can photosynthesis occur in the absence of light?\"}\n",
    "      ]\n",
    "    },\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1267af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sample = np.random.choice(demonstrations,1)[0]\n",
    "questions = np.random.choice(sample['questions'],2,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6646cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = \"{\"+str({k:v for dic in questions.tolist() for k,v in dic.items()}).replace(\"'\",'\"')+\"}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04908264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{{\"question_Why\": \"Why do plants perform photosynthesis?\", \"question_Was\": \"Was photosynthesis discovered in plants, algae, or bacteria first?\"}}'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9158b292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:The Eiffel Tower in Paris was originally intended as a temporary structure, built for the 1889 World's Fair. It was almost dismantled in 1909 but was saved because it was repurposed as a giant radio antenna.\n",
      "Questions:{{'question_How': 'How did the Eiffel Tower avoid being dismantled in 1909?', 'question_Where': 'Where is the Eiffel Tower?'}}\n"
     ]
    }
   ],
   "source": [
    "print(f'Context:{sample[\"context\"]}\\nQuestions:{questions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "36b32a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = documents[10].get_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "efadcaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(seed_question.format(context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "64a12cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8Nbs8F3zJTbjhfxfcjtrYnUUMUHcY', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='{\"question_what\": \"What are the two closed-set tasks evaluated in the experiments?\",\\n\"question_how\": \"How does SELF-RAG tailor its behavior during the inference phase?\"}', role='assistant', function_call=None, tool_calls=None))], created=1700637816, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=38, prompt_tokens=1148, total_tokens=1186))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e83b140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit\n",
      "Jiongnan Liu1, Jiajie Jin2, Zihan Wang1, Jiehan Cheng1, Zhicheng Dou1∗,andJi-Rong Wen1\n",
      "1Gaoling School of Artificial Intelligence, Renmin University of China\n",
      "2University of Science and Technology of China\n",
      "1{liujn, wangzihan0527, jiehan_cheng, dou, jrwen}@ruc.edu.cn\n",
      "2jinjiajie@mail.ustc.edu.cn\n",
      "Abstract\n",
      "Although Large Language Models (LLMs)\n",
      "have demonstrated extraordinary capabilities\n",
      "in many domains, they still have a tendency\n",
      "to hallucinate and generate fictitious responses\n",
      "to user requests. This problem can be allevi-\n",
      "ated by augmenting LLMs with information\n",
      "retrieval (IR) systems (also known as retrieval-\n",
      "augmented LLMs). Applying this strategy,\n",
      "LLMs can generate more factual texts in re-\n",
      "sponse to user input according to the relevant\n",
      "content retrieved by IR systems from external\n",
      "corpora as references. In addition, by incorpo-\n",
      "rating external knowledge, retrieval-augmented\n",
      "LLMs can answer in-domain questions that\n",
      "cannot be answered by solely relying on the\n",
      "world knowledge stored in parameters. To sup-\n",
      "port research in this area and facilitate the de-\n",
      "velopment of retrieval-augmented LLM sys-\n",
      "tems, we develop RETA-LLM, a RETreival-\n",
      "Augmented LLM toolkit. In RETA-LLM, we\n",
      "create a complete pipeline to help researchers\n",
      "and users build their customized in-domain\n",
      "LLM-based systems. Compared with previ-\n",
      "ous retrieval-augmented LLM systems, RETA-\n",
      "LLM provides more plug-and-play modules to\n",
      "support better interaction between IR systems\n",
      "and LLMs, including request rewriting, docu-\n",
      "ment retrieval, passage extraction, answer gen-\n",
      "eration, and fact checking modules. Our toolkit\n",
      "is publicly available at https://github.com/\n",
      "RUC-GSAI/YuLan-IR/tree/main/RETA-LLM .\n",
      "1 Introduction\n",
      "Large language models (LLMs) have attracted in-\n",
      "creasing attention from both research community\n",
      "and industry (Brown et al., 2020; OpenAI, 2023;\n",
      "Ouyang et al., 2022; Touvron et al., 2023; Chowdh-\n",
      "ery et al., 2022; Zhao et al., 2023; Zeng et al., 2022).\n",
      "With tremendous world knowledge stored in pa-\n",
      "rameters (Petroni et al., 2019; Roberts et al., 2020;\n",
      "Jiang et al., 2020) and the Reinforcement Learning\n",
      "*Corresponding author.from Human Feedback (RLHF) techniques (Chris-\n",
      "tiano et al., 2017; Ziegler et al., 2019), LLMs can\n",
      "generate helpful, detailed, and polite texts in re-\n",
      "sponse to user inputs. Many studies have demon-\n",
      "strated LLMs’ extraordinary abilities in various ar-\n",
      "eas, including nature language processing (Moslem\n",
      "et al., 2023), information retrieval (Sun et al., 2023;\n",
      "Wang et al., 2023; Mao et al., 2023), and recom-\n",
      "mendation (Hou et al., 2023; Zhang et al., 2023).\n",
      "However, LLMs still tend to hallucinate and\n",
      "sometimes generate texts opposite to facts (Zhou\n",
      "et al., 2021; Zhao et al., 2023). To tackle these prob-\n",
      "lems, researchers have proposed a new paradigm\n",
      "to strengthen LLMs with information retrieval\n",
      "systems (retrieval-augmented LLMs) (Shi et al.,\n",
      "2023; Jiang et al., 2023; Nakano et al., 2022),\n",
      "which enables LLMs to retrieve relevant contents\n",
      "from an external repository (knowledge corpus)\n",
      "to generate texts based on them. It has been\n",
      "verified that retrieval-augmented LLMs can gen-\n",
      "erate texts in response to user input with fewer\n",
      "hallucinations (Nakano et al., 2022). Further-\n",
      "more, by incorporating customized private data\n",
      "resources, retrieval-augmented LLMs can respond\n",
      "to in-domain queries that cannot be answered by\n",
      "LLMs trained with public data.\n",
      "To support research in this area and help users\n",
      "build their own in-domain LLM-based systems, we\n",
      "devise RETA-LLM, a RET reival- Augmented LLM\n",
      "toolkit. Different from previous general LLM-\n",
      "enhanced toolkits such as LangChain,1RETA-\n",
      "LLM focuses on the retrieval-augmented LLMs\n",
      "and provides more plug-in modules. Typically,\n",
      "retrieval-augmented LLMs use a retrieve-and-\n",
      "generate strategy with two modules: First, they\n",
      "retrieve documents or passages based on user re-\n",
      "quest ( document retrieval module); then, they gen-\n",
      "erate answers utilizing these relevant documents as\n",
      "references ( answer generation module). In addi-\n",
      "1LangChain, https://github.com/hwchase17/\n",
      "langchainarXiv:2306.05212v1  [cs.IR]  8 Jun 2023\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5dafa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\\nGenerate two questions from given context satisfying the rules given below:\\n    2.The question should be framed such that it must be clearly understood without providing context.\\n    3.The question should be fully answerable from information present in given context.\\n    \\nContext:Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.\\nQuestions:{{'question_How': 'How do plants capture light energy for photosynthesis?', 'question_Where': 'Where in plants does photosynthesis primarily occur?'}}\\n\\n\\nContext:\\nBased on manual examinations we further ﬁlter out examples where\\nevidence is too long, where history is not well-formed (in so me of the examples there are missing turns) or where the golde n\\nanswer is a single word. See Appendix for the full impact of ﬁl tering on the dataset size.\\nFor our experiments we randomly select 100 examples from the remaining after ﬁltering 324 examples in the dev split. We al so\\nmanually verify these 100 examples to ensure dialog quality .\\n4 Human Evals\\n4.1 Pilot\\nMeena paper [ 2] introduces a proxy for ﬂuency in the form of Sensibleness an d Speciﬁcity Average metric (SSA), while\\npaper [ 10] presents an evaluation framework called Attributable to I dentiﬁed Sources (AIS) for assessing the output of natural\\nlanguage generation models for attributability.\\nBoth SSA a nd AIS assume the use of human raters and in the ideal world wit h\\ninﬁnite resources all our experiments would be evaluated th is way.\\nTo get a sense of the problem, we started with conducting smal l pilot human eval, 400 examples in total. We sampled 100\\ndialogs from QReCC, as described above, and generated respo nses for them using PaLM 540B model [ 16] with 4 different\\nsetups (see next section for the speciﬁcs about how we use PaL M’s native dialog prompt):\\n1. temperature = 0.0 and “no evidence”\\n2. temperature = 0.7 and “no evidence”\\n3. temperature = 0.0 and “golden evidence”\\n4. temperature = 0.7 and “golden evidence”\\nWe have run human SSA eval for all 4 pilot setups and human AIS e val for setups #3 and #4.\\nQuestions:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc8aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm2(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eec8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a3d9516",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ragas/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ragas/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ragas/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "json.loads(output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d04f642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'question_What': 'What is the purpose of the Sensibleness and Specificity Average metric (SSA)?', 'question_How': 'How many examples were sampled for the pilot human evaluation?'}\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e0b0a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generate two questions from given context satisfying the rules given below:\n",
      "    2.The question should be framed such that it must be clearly understood without providing context.\n",
      "    3.The question should be fully answerable from information present in given context.\n",
      "    \n",
      "Context:Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.\n",
      "Questions:{{'question_How': 'How do plants capture light energy for photosynthesis?', 'question_Where': 'Where in plants does photosynthesis primarily occur?'}}\n",
      "\n",
      "\n",
      "Context:\n",
      "Based on manual examinations we further ﬁlter out examples where\n",
      "evidence is too long, where history is not well-formed (in so me of the examples there are missing turns) or where the golde n\n",
      "answer is a single word. See Appendix for the full impact of ﬁl tering on the dataset size.\n",
      "For our experiments we randomly select 100 examples from the remaining after ﬁltering 324 examples in the dev split. We al so\n",
      "manually verify these 100 examples to ensure dialog quality .\n",
      "4 Human Evals\n",
      "4.1 Pilot\n",
      "Meena paper [ 2] introduces a proxy for ﬂuency in the form of Sensibleness an d Speciﬁcity Average metric (SSA), while\n",
      "paper [ 10] presents an evaluation framework called Attributable to I dentiﬁed Sources (AIS) for assessing the output of natural\n",
      "language generation models for attributability.\n",
      "Both SSA a nd AIS assume the use of human raters and in the ideal world wit h\n",
      "inﬁnite resources all our experiments would be evaluated th is way.\n",
      "To get a sense of the problem, we started with conducting smal l pilot human eval, 400 examples in total. We sampled 100\n",
      "dialogs from QReCC, as described above, and generated respo nses for them using PaLM 540B model [ 16] with 4 different\n",
      "setups (see next section for the speciﬁcs about how we use PaL M’s native dialog prompt):\n",
      "1. temperature = 0.0 and “no evidence”\n",
      "2. temperature = 0.7 and “no evidence”\n",
      "3. temperature = 0.0 and “golden evidence”\n",
      "4. temperature = 0.7 and “golden evidence”\n",
      "We have run human SSA eval for all 4 pilot setups and human AIS e val for setups #3 and #4.\n",
      "Questions:\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d8e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "ragas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
