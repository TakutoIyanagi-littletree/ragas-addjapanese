{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d341594d",
   "metadata": {},
   "source": [
    "## Logs\n",
    "- Faithfulness NLI\n",
    "    - Without CoT\n",
    "    - With CoT ( WIN)  \n",
    "    - WikiQA \n",
    "        - generated non factual answer for measuring faithfulness agreement.\n",
    "        - Kendall Score = 0.7\n",
    "    - HotPotQA\n",
    "        - Accuracy = 0.75 \n",
    "    - Possible Improvements \n",
    "        - improve statement generation\n",
    "\n",
    "- Relevance scores\n",
    "    - QGen method\n",
    "        - models tried : t5-base / gptneo-125M\n",
    "        - WikiQA\n",
    "            - Kendall score = 0.65\n",
    "            - observations : finetune model on prompt/answer pairs to improve performance.\n",
    "    - Cross-encoder method\n",
    "        - models tried : distilbert \n",
    "        - WikiQA\n",
    "            - kendall score = 0.63\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bfb2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import kendalltau, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4168502",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/shahules/belar/src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9adac051",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_KEY = json.load(open(\"/Users/shahules/openai-key.json\"))[\"jj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21e09881",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2a602",
   "metadata": {},
   "source": [
    "## OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3139189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": \"How can I assist you today?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bce4c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def llm2(prompt, **kwargs):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=kwargs.get(\"model\",\"gpt-3.5-turbo-16k\"),\n",
    "        messages=[{\"role\": \"system\", \"content\":prompt}],\n",
    "        temperature=kwargs.get(\"temperature\", 0),\n",
    "        top_p=kwargs.get(\"top_p\", 1),\n",
    "        frequency_penalty=kwargs.get(\"frequency_penalty\", 0.0),\n",
    "        presence_penalty=kwargs.get(\"presence_penalty\", 0.0),\n",
    "        max_tokens=kwargs.get(\"max_tokens\", 500),\n",
    "        n=kwargs.get(\"n\", 1),\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def llm(prompt, **kwargs):\n",
    "    response = openai.Completion.create(\n",
    "        model=kwargs.get(\"model\", \"text-davinci-003\"),\n",
    "        prompt=prompt,\n",
    "        temperature=kwargs.get(\"temperature\", 0),\n",
    "        top_p=kwargs.get(\"top_p\", 1),\n",
    "        frequency_penalty=kwargs.get(\"frequency_penalty\", 0.0),\n",
    "        presence_penalty=kwargs.get(\"presence_penalty\", 0.0),\n",
    "        max_tokens=kwargs.get(\"max_tokens\", 500),\n",
    "        logprobs=kwargs.get(\"logprobs\", 1),\n",
    "        n=kwargs.get(\"n\", 1),\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d9b4e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_logger(data, filename=\"nli_check\"):\n",
    "    output = json.load(open(filename + \".json\"))\n",
    "    output.append(data)\n",
    "    with open(filename + \".json\", \"w\") as file:\n",
    "        json.dump(output, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50add06b",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9f4280e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/shahules/.cache/huggingface/datasets/explodinggradients___parquet/explodinggradients--ragas-wikiqa-5b5116e5cb909aca/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|â–ˆ| 1/\n"
     ]
    }
   ],
   "source": [
    "wikiqa_ragas = load_dataset(\"explodinggradients/ragas-wikiqa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e0148e",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "eca20daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr(targets, predictions):\n",
    "    scores = [kendalltau(x, y).correlation for x, y in zip(targets, predictions)]\n",
    "    return [score if not np.isnan(score) else 0 for score in scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5563146",
   "metadata": {},
   "source": [
    "## QA-QG paradigm\n",
    "- Generate question and answer pair from `generated answer`.\n",
    "- Given `context`, ask these questions\n",
    "- Verify answer correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3e35532",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question_generation = \"\"\"Given a text, extract {} noun phrases and create questions for each based on given text.\n",
    "text: Albert Einstein was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. Best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\n",
    "A: Germany\n",
    "Q: Where was Albert Einstein born?\n",
    "A: theory of relativity\n",
    "Q: What is Albert Einstein best known for?\n",
    "text: {}\n",
    "\"\"\"\n",
    "\n",
    "Question_answering = \"\"\"Given a text and set of questions, answer the questions\n",
    "text: Albert Einstein was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. Best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.\n",
    "questions: Where was Albert Einstein born?\\n\\nWhat is Albert Einstein best known for?\n",
    "answers:Germany\\n\\ntheory of relativity\n",
    "text: {}\n",
    "questions:{}\n",
    "answers:\"\"\"\n",
    "\n",
    "Answer_verification = \"\"\"Given a set of questions, correct answer and student's answer return the number of questions incorrectly answered by student.\n",
    "Where was Albert Einstein born?\\nCorrect answer: Germany\\nStudent answer:India\\n\\n\n",
    "What is Albert Einstein best known for?\\nCorrect answer:  theory of relativity\\nStudent answer: theory of relativity\\n\\n\n",
    "Number of incorrect answers:1\n",
    "{}\n",
    "Number of incorrect answers:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "335081e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QAQG_fun(question, context, answer):\n",
    "    \"\"\"\n",
    "    returns number of factual inconsistencies.\n",
    "    \"\"\"\n",
    "\n",
    "    def answer_ver(qstn, answer, cand):\n",
    "        return f\"{qstn}\\nCorrect answer: {answer}\\nStudent answer: {cand}\"\n",
    "\n",
    "    num = len(answer.split(\".\")) - 1\n",
    "    prompt = Question_generation.format(num, answer)\n",
    "    output = llm(prompt)\n",
    "    qa_pairs = [\n",
    "        re.sub(r\"A:|Q:\", \"\", x).strip()\n",
    "        for item in output[\"choices\"][0][\"text\"].strip().split(\"\\n\\n\")\n",
    "        for x in item.split(\"\\n\")\n",
    "    ]\n",
    "    qa_pairs = [tuple(qa_pairs[i : i + 2]) for i in range(0, len(qa_pairs), 2)]\n",
    "    print(qa_pairs)\n",
    "    questions = \"\\n\\n\".join([qstn for ans, qstn in qa_pairs])\n",
    "    prompt = Question_answering.format(context, questions)\n",
    "    answers = llm(prompt)[\"choices\"][0][\"text\"].split(\"\\n\\n\")\n",
    "\n",
    "    prompt = \"\\n\\n\".join(\n",
    "        [answer_ver(qstn, ans, cand) for (ans, qstn), cand in zip(qa_pairs, answers)]\n",
    "    )\n",
    "    output = llm(Answer_verification.format(prompt))[\"choices\"][0][\"text\"].strip()\n",
    "    return int(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2642e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"The actress who played Lolita, Sue Lyon, was 14 at the time of filming.\"\n",
    "question = \"What was the age of Sue Lyon when she played Lolita?\"\n",
    "context = \"\"\"\n",
    "Lolita is a 1962 psychological comedy-drama film[5] directed by Stanley Kubrick and based on the 1955 novel of the same title by Vladimir Nabokov, who is also credited with writing the screenplay. The film follows Humbert Humbert, a middle-aged literature lecturer who becomes sexually infatuated with Dolores Haze (nicknamed \"Lolita\"), a young adolescent girl. It stars James Mason, Shelley Winters, Peter Sellers and, as the titular character, Sue Lyon.\n",
    "\n",
    "Owing to restrictions imposed by the Motion Picture Production Code, the film toned down the most provocative aspects of the novel, sometimes leaving much to the audience's imagination. The actress who played Lolita, Sue Lyon, was 14 at the time of filming.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26ca4af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sue Lyon', 'Who played the role of Lolita in the movie?')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QAQG_fun(question, context, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6bdd767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7aiRcMcfrtt4jp5AK9PaIBMqFdlIB at 0x7f9b1b66e630> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"A: Lolita\\nQ: What character did Sue Lyon play in the movie?\\nA: 14\\nQ: How old was Sue Lyon when she filmed Lolita?\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1688985008,\n",
       "  \"id\": \"chatcmpl-7aiRcMcfrtt4jp5AK9PaIBMqFdlIB\",\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 35,\n",
       "    \"prompt_tokens\": 128,\n",
       "    \"total_tokens\": 163\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2([Question_generation.format(2,answer)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2078ece",
   "metadata": {},
   "source": [
    "## G-Eval\n",
    "- Define criterions to evaluate model.\n",
    "- Normalize `score = prob(s) * s`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ca1c56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevence = \"\"\"\n",
    "Evaluation Criteria.\\n\n",
    "Relevance (1-5) - how relevant is the reply to the given question.\n",
    "1. Read the reply and compare it to the question. Check if the given reply\n",
    "actually answers the question, and if it presents them in a clear and logical order.\n",
    "2. The reply should include only required information to answer the question.\n",
    "3. Penalize replies that contain redundancies and excess information.\n",
    "4. Assign a score for Relevance on a scale of 1 to 5, where 1 is the lowest and\n",
    "5 is the highest based on the Evaluation Criteria.\n",
    "\n",
    "question:{}\n",
    "reply:{}\n",
    "score:\"\"\"\n",
    "\n",
    "faithfulness = \"\"\"\n",
    "Evaluation Criteria.\\n\n",
    "Faithfulness (1-5) - how factually consistant is the reply with the given context.\n",
    "1. Read the reply and compare it to the question. Check if the given reply\n",
    "actually answers the question correctly, and if the reply is factualy consistent with the context.\n",
    "2. Assign a score for faithfulness on a scale of 1 to 5, where 1 is the lowest and\n",
    "5 is the highest based on the Evaluation Criteria.\n",
    "\n",
    "context: {}\n",
    "question:{}\n",
    "reply:{}\n",
    "score:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "541c1423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_faithfulness(question: list, context: list, answer: list):\n",
    "    prompt = [\n",
    "        faithfulness.format(c, q, a) for c, q, a in zip(question, context, answer)\n",
    "    ]\n",
    "    output = [output for output in llm(prompt)[\"choices\"]]\n",
    "    scores = [(out[\"text\"].strip()) for out in output]\n",
    "    scores = [\n",
    "        int(score) if score in [\"1\", \"2\", \"3\", \"4\", \"5\"] else 1 for score in scores\n",
    "    ]\n",
    "    return scores\n",
    "\n",
    "\n",
    "def gpt_relevance(question: list, answer: list):\n",
    "    prompt = [relevence.format(q, a) for q, a in zip(question, answer)]\n",
    "    output = [output for output in llm(prompt)[\"choices\"]]\n",
    "    scores = [(out[\"text\"].strip()) for out in output]\n",
    "    scores = [\n",
    "        int(score) if score in [\"1\", \"2\", \"3\", \"4\", \"5\"] else 1 for score in scores\n",
    "    ]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cd7fed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_eval(question, context, answer):\n",
    "    prompt = relevence.format(question, answer)\n",
    "    output = llm(prompt)[\"choices\"][0]\n",
    "    prob = np.exp(sum(output[\"logprobs\"][\"token_logprobs\"]))\n",
    "    score = int(output[\"text\"].strip())\n",
    "    print(score)\n",
    "    return prob * score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "35113558",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which year did Lolita release?\"\n",
    "answer = \"Lolita film released in 1947.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e82d0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_relevance(question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a79b1780",
   "metadata": {},
   "outputs": [],
   "source": [
    "q, a, c = (\n",
    "    wikiqa_ragas[\"train\"][0][\"question\"],\n",
    "    wikiqa_ragas[\"train\"][0][\"generated_without_rag\"],\n",
    "    wikiqa_ragas[\"train\"][0][\"context\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f25b046f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_faithfulness([q], [c], [a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e158274f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_relevance([q], [a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce1baa",
   "metadata": {},
   "source": [
    "## Relevancy Score \n",
    "- Scores `answers` according to `prompt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aa62eb",
   "metadata": {},
   "source": [
    "### QGen scoring method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc263805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics.answer_relevance import QGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38deaf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_qgen = QGen(\"t5-base\", \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "45942810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_(examples):\n",
    "    scores = {}\n",
    "    questions = examples[\"question\"]\n",
    "    context = examples[\"context\"]\n",
    "    for col in COLUMNS:\n",
    "        passage = examples[col]\n",
    "        inputs = list(zip(questions, passage))\n",
    "        # scores[f\"{col}_relevance\"] = t5_qgen.predict(inputs, show_progress=False)\n",
    "        scores[f\"{col}_relevance\"] = gpt_faithfulness(questions, context, passage)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1410f3c",
   "metadata": {},
   "source": [
    "- We assume `generated_with_rag > correct_answer > incorrect_answer` for relevancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ab00e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\"generated_with_rag\", \"correct_answer\", \"incorrect_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e705767d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output = wikiqa_ragas[\"train\"].map(predict_relevance, batched=True, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab21cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [[item[f\"{k}_relevance\"] for k in COLUMNS] for item in output]\n",
    "target = [[2, 1, 0] for i in range(len(output))]\n",
    "np.mean(get_corr(target, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2c5e1c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "608a7ddb",
   "metadata": {},
   "source": [
    "Relevance\n",
    "\n",
    "- 0.6337284370533437 for wikiQA gpt 3.5\n",
    "\n",
    "- 0.6831823238905629 For wikiwa t5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d8ccbc",
   "metadata": {},
   "source": [
    "## Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2f26f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\"generated_with_rag\", \"correct_answer\", \"generated_without_rag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a3a8fc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "          \r"
     ]
    }
   ],
   "source": [
    "output = wikiqa_ragas[\"train\"].map(predict_relevance, batched=True, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "57f0b521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48110338184466117"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [[item[f\"{k}_facuality\"] for k in COLUMNS] for item in output]\n",
    "target = [[2, 1, 0] for i in range(len(output))]\n",
    "np.mean(get_corr(target, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10aee98",
   "metadata": {},
   "source": [
    "0.48110338184466117 for GPT3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d562351",
   "metadata": {},
   "source": [
    "### Cross encoder method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6d76ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics.context_relevance import context_relavancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcb4e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_relevance(examples):\n",
    "    scores = {}\n",
    "    questions = examples[\"question\"]\n",
    "    for col in COLUMNS:\n",
    "        passage = examples[col]\n",
    "        inputs = list(zip(questions, passage))\n",
    "        scores[f\"{col}_relevance\"] = cross_encoder.predict(inputs, show_progress=False)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36565a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = (\n",
    "    wikiqa_ragas[\"train\"]\n",
    "    .select(range(0, 10))\n",
    "    .map(predict_relevance, batched=True, batch_size=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f0571",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [[item[f\"{k}_relevance\"] for k in COLUMNS] for item in output]\n",
    "target = [[2, 1, 0] for i in range(len(output))]\n",
    "get_tau(target, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd9923",
   "metadata": {},
   "source": [
    "## Faithfulness on HotpotQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2316c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6cd24f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'experimental' (namespace)>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "reload(experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "723e662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experimental.nli import NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3f9bd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset hotpot_qa (/Users/shahules/.cache/huggingface/datasets/hotpot_qa/distractor/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5)\n"
     ]
    }
   ],
   "source": [
    "hotpot_qa = load_dataset(\n",
    "    \"hotpot_qa\",\n",
    "    \"distractor\",\n",
    "    split=\"validation\",\n",
    ").select(range(0, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2ab98cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_answer_prompt = \"\"\"Given a question and correct answer, generate an incorrect answer\n",
    "question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
    "correct answer: yes\n",
    "answer: no\n",
    "question: {}\n",
    "correct answer: {}\n",
    "answer:\"\"\"\n",
    "\n",
    "\n",
    "def generate_false_answers(question, answer):\n",
    "    answer = llm(false_answer_prompt.format(question, answer))[\"choices\"][0][\n",
    "        \"text\"\n",
    "    ].strip()\n",
    "    return {\"false_answer\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "542bdb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/shahules/.cache/huggingface/datasets/hotpot_qa/distractor/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5/cache-593e03a966a13563.arrow\n"
     ]
    }
   ],
   "source": [
    "hotpot_qa = hotpot_qa.map(lambda x: generate_false_answers(x[\"question\"], x[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f8682fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(item):\n",
    "    titles, ids = item[\"supporting_facts\"].values()\n",
    "    title_ids = [item[\"context\"][\"title\"].index(i) for i in titles]\n",
    "    sentences = [\n",
    "        item[\"context\"][\"sentences\"][i][k]\n",
    "        for i, k in zip(title_ids, item[\"supporting_facts\"][\"sent_id\"])\n",
    "    ]\n",
    "    orig_context = \" \".join(sentences)\n",
    "    return {\"answer_context\": orig_context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a94511fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    }
   ],
   "source": [
    "hotpot_qa = hotpot_qa.map(lambda x: get_context(x), batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "84f39785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_faithfulness(examples, scoring_fun=NLI.score):\n",
    "    scores = {}\n",
    "    questions = examples[\"question\"]\n",
    "    contexts = examples[\"answer_context\"]\n",
    "    for col in COLUMNS:\n",
    "        answers = examples[col]\n",
    "        while True:\n",
    "            try:\n",
    "                scores[f\"{col}_factual\"] = scoring_fun(questions, contexts, answers)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b75f9dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/shahules/.cache/huggingface/datasets/hotpot_qa/distractor/1.0.0/133b9501f892e5193babbad937bee3b4899deb4691ef4d791e6ac0111c875bb5/cache-d51f81546b2858f1.arrow\n"
     ]
    }
   ],
   "source": [
    "COLUMNS = [\"answer\", \"false_answer\"]\n",
    "hotpot_qa = hotpot_qa.map(predict_faithfulness, batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ca2cd14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.75\n"
     ]
    }
   ],
   "source": [
    "predictions = [[item[f\"{k}_factual\"] for k in COLUMNS] for item in hotpot_qa]\n",
    "target = [[1, 0] for i in range(len(hotpot_qa))]\n",
    "incorrect = [\n",
    "    idx for idx, item in enumerate(predictions) if all(np.argsort(item) != [1.0, 0.0])\n",
    "]\n",
    "print(\"Accuracy\", 1 - (len(incorrect) / len(target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f03a06",
   "metadata": {},
   "source": [
    "## Context relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c3db326",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    }
   ],
   "source": [
    "def get_all_facts(item):\n",
    "    all_facts = item['context']['sentences']\n",
    "    all_facts = [sent for para in all_facts for sent in para]\n",
    "    return {\"full_context\":''.join(all_facts)}\n",
    "hotpot_qa = hotpot_qa.map(get_all_facts, batched=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f26aec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_ex1 = \"Were Scott Derrickson and Ed Wood of the same nationality?\"\n",
    "context_ex1 = 'Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer He lives in Los Angeles, California He is best known for directing horror films such as \"Sinister\", \"The Exorcism of Emily Rose\", and \"Deliver Us From Evil\", as well as the 2016 Marvel Cinematic Universe installment, \"Doctor Strange\"Tyler Bates is an American musician, music producer, and composer for films, television, and video games. Adam Collis is an American filmmaker and actor.Conrad Brooks is an American actor.Edward Davis Wood Jr. (October 10, 1924 â€“ December 10, 1978) was an American filmmaker, actor, writer, producer, and director.'\n",
    "answer_ex1 = \"Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer. \\nEdward Davis Wood Jr. (October 10, 1924 â€“ December 10, 1978) was an American filmmaker, actor, writer, producer, and director.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f0b7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "Context_relevency = \"\"\"\n",
    "Task: Candidate sentence extraction.\n",
    "Given the question and context, extract minimum number of sentences from context required to answer the question. If the context do not contain information required to answer the question return \"No candidate sentences found\".\n",
    "\n",
    "question: Which equation is known as worlds most famous equation?\n",
    "context:\\nAlbert Einstein (14 March 1879 â€“ 18 April 1955) was a German-born theoretical physicist,[5] widely ranked among the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century.\n",
    "His massâ€“energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world's most famous equation\".\n",
    "sentences:His massâ€“energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world's most famous equation\".\n",
    "\n",
    "question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
    "context :\\nScott Derrickson (born July 16, 1966) is an American director, screenwriter and producer He lives in Los Angeles, California He is best known for directing horror films such as \"Sinister\", \"The Exorcism of Emily Rose\", and \"Deliver Us From Evil\", as well as the 2016 Marvel Cinematic Universe installment, \"Doctor Strange\"Tyler Bates is an American musician, music producer, and composer for films, television, and video games. Adam Collis is an American filmmaker and actor.Conrad Brooks is an American actor.Edward Davis Wood Jr. (October 10, 1924 â€“ December 10, 1978) was an American filmmaker, actor, writer, producer, and director.\n",
    "Now given a question and context, extract the minimum number of sentences from the given context required to answer the question completely. \n",
    "sentences:Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer. Edward Davis Wood Jr. (October 10, 1924 â€“ December 10, 1978) was an American filmmaker, actor, writer, producer, and director.\n",
    "\n",
    "question:{}\n",
    "context:\\n{}\n",
    "sentences:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f649eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=15\n",
    "q,c = hotpot_qa[i]['question'],hotpot_qa[i]['full_context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b9f5e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"A black hole is a region of spacetime where gravity is so strong that nothing, including light or other electromagnetic waves, has enough energy to escape it.[2] The theory of general relativity predicts that a sufficiently compact mass can deform spacetime to form a black hole.[3][4] The boundary of no escape is called the event horizon. Although it has a great effect on the fate and circumstances of an object crossing it, it has no locally detectable features according to general relativity.[5] In many ways, a black hole acts like an ideal black body, as it reflects no light\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "b711de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"what is general relativity?\"\n",
    "n=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "11a83f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n",
    "\n",
    "p_wiki = wiki_wiki.page(\"Black hole\")\n",
    "\n",
    "def get_page_section(page, section):\n",
    "    all_text = \"\"\n",
    "    p_wiki = wiki_wiki.page(page)\n",
    "    sections = p_wiki.sections_by_title(section)\n",
    "    for s in sections:\n",
    "        all_text += s.full_text()\n",
    "    return all_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2755ba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from itertools import combinations\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/stsb-TinyBERT-L-4\")\n",
    "\n",
    "        \n",
    "def sent_tokenize(sent):\n",
    "    return [s[:-1] if  s.endswith('.') else s for s in sent.strip().split('. ')]\n",
    "\n",
    "class SentenceAgreement:\n",
    "    \n",
    "    def __init__(self, scoring=\"bert_score\"):\n",
    "        \n",
    "        self.scoring = scoring\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def bert_score(para1, para2):\n",
    "        \n",
    "        sentences1, sentences2 = sent_tokenize(para1), sent_tokenize(para2)\n",
    "        scores = cross_encoder.predict(list(itertools.product(sentences1, sentences2)))\n",
    "        scores = scores.reshape(len(sentences1), len(sentences2))\n",
    "        return scores.max(axis=1).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def jaccard_score(para1, para2):\n",
    "        \n",
    "        sentences1, sentences2 = sent_tokenize(para1), sent_tokenize(para2)\n",
    "        intersect = len(np.intersect1d(sentences1, sentences2))\n",
    "        union = len(np.union1d(sentences1, sentences2))\n",
    "        return intersect/union\n",
    "    \n",
    "    def evaluate(self,answers:List[List[str]]):\n",
    "        \n",
    "        \"\"\"\n",
    "        eval nC2 combinations\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        groups = combinations(answers,2)\n",
    "        for group in groups:\n",
    "            if self.scoring == \"jaccard\":\n",
    "                score = self.jaccard_score(*group)\n",
    "            elif self.scoring == \"bert_score\":\n",
    "                score = self.bert_score(*group)\n",
    "            scores.append(score)\n",
    "        return np.mean(scores)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d3aa09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextRelevacy:\n",
    "    \n",
    "    def __init__(self, strictness = 2, agreement_metric=\"bert_score\"):\n",
    "        \n",
    "        self.strictness = strictness\n",
    "        self.sent_agreement = SentenceAgreement(agreement_metric)\n",
    "        \n",
    "    def score(self,question,context):\n",
    "        scores = []\n",
    "        outputs = llm(Context_relevency.format(q,c),n=self.strictness,temperature=1)\n",
    "        outputs = [outputs['choices'][i]['text'].strip() for i in range(self.strictness)]\n",
    "        context_sents = sent_tokenize(context)\n",
    "        for output in outputs:\n",
    "            indices = [context.find(sent) for sent in sent_tokenize(output) if context.find(sent)!=-1]\n",
    "            scores.append(len(indices)/len(context_sents))\n",
    "        \n",
    "        if self.strictness > 1:\n",
    "            agr_score = self.sent_agreement.evaluate(outputs)\n",
    "        else:\n",
    "            agr_score =1 \n",
    "        return agr_score * np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "6985c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = get_page_section(\"HIV/AIDS\", \"Prevention\")\n",
    "c = ' '.join(c.split(' ')[:500])\n",
    "q = \"When was the first HIV case detected?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "689e1aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm([Context_relevency.format(q,c), Context_relevency.format(\"How to prevent AIDS?\",c)],n=n,temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6aee1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/blade2blade/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import context_relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61fdab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbad39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_relevancy.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755e0c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fiqa (/Users/shahules/.cache/huggingface/datasets/explodinggradients___fiqa/ragas_eval/1.0.0/3dc7b639f5b4b16509a3299a2ceb78bf5fe98ee6b5fee25e7d5e4d290c88efb8)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 146.34it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"explodinggradients/fiqa\", \"ragas_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b9d75e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truths</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>context_relavency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to deposit a cheque issued to an associate...</td>\n",
       "      <td>[Have the check reissued to the proper payee.J...</td>\n",
       "      <td>\\nThe best way to deposit a cheque issued to a...</td>\n",
       "      <td>[Just have the associate sign the back and the...</td>\n",
       "      <td>0.220575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I send a money order from USPS as a business?</td>\n",
       "      <td>[Sure you can.  You can fill in whatever you w...</td>\n",
       "      <td>\\nYes, you can send a money order from USPS as...</td>\n",
       "      <td>[Sure you can.  You can fill in whatever you w...</td>\n",
       "      <td>0.155282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 EIN doing business under multiple business n...</td>\n",
       "      <td>[You're confusing a lot of things here. Compan...</td>\n",
       "      <td>\\nYes, it is possible to have one EIN doing bu...</td>\n",
       "      <td>[You're confusing a lot of things here. Compan...</td>\n",
       "      <td>0.347134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How to deposit a cheque issued to an associate...   \n",
       "1  Can I send a money order from USPS as a business?   \n",
       "2  1 EIN doing business under multiple business n...   \n",
       "\n",
       "                                       ground_truths  \\\n",
       "0  [Have the check reissued to the proper payee.J...   \n",
       "1  [Sure you can.  You can fill in whatever you w...   \n",
       "2  [You're confusing a lot of things here. Compan...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  \\nThe best way to deposit a cheque issued to a...   \n",
       "1  \\nYes, you can send a money order from USPS as...   \n",
       "2  \\nYes, it is possible to have one EIN doing bu...   \n",
       "\n",
       "                                            contexts  context_relavency  \n",
       "0  [Just have the associate sign the back and the...           0.220575  \n",
       "1  [Sure you can.  You can fill in whatever you w...           0.155282  \n",
       "2  [You're confusing a lot of things here. Compan...           0.347134  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_relevancy.score(dataset[\"baseline\"].select(range(0,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a4a2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truths</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>context_relavency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to deposit a cheque issued to an associate...</td>\n",
       "      <td>[Have the check reissued to the proper payee.J...</td>\n",
       "      <td>\\nThe best way to deposit a cheque issued to a...</td>\n",
       "      <td>[Just have the associate sign the back and the...</td>\n",
       "      <td>0.220575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can I send a money order from USPS as a business?</td>\n",
       "      <td>[Sure you can.  You can fill in whatever you w...</td>\n",
       "      <td>\\nYes, you can send a money order from USPS as...</td>\n",
       "      <td>[Sure you can.  You can fill in whatever you w...</td>\n",
       "      <td>0.155282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 EIN doing business under multiple business n...</td>\n",
       "      <td>[You're confusing a lot of things here. Compan...</td>\n",
       "      <td>\\nYes, it is possible to have one EIN doing bu...</td>\n",
       "      <td>[You're confusing a lot of things here. Compan...</td>\n",
       "      <td>0.347134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How to deposit a cheque issued to an associate...   \n",
       "1  Can I send a money order from USPS as a business?   \n",
       "2  1 EIN doing business under multiple business n...   \n",
       "\n",
       "                                       ground_truths  \\\n",
       "0  [Have the check reissued to the proper payee.J...   \n",
       "1  [Sure you can.  You can fill in whatever you w...   \n",
       "2  [You're confusing a lot of things here. Compan...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  \\nThe best way to deposit a cheque issued to a...   \n",
       "1  \\nYes, you can send a money order from USPS as...   \n",
       "2  \\nYes, it is possible to have one EIN doing bu...   \n",
       "\n",
       "                                            contexts  context_relavency  \n",
       "0  [Just have the associate sign the back and the...           0.220575  \n",
       "1  [Sure you can.  You can fill in whatever you w...           0.155282  \n",
       "2  [You're confusing a lot of things here. Compan...           0.347134  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_relevancy.score(dataset[\"baseline\"].select(range(0,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd76a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Context relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c3db326",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    }
   ],
   "source": [
    "def get_all_facts(item):\n",
    "    all_facts = item['context']['sentences']\n",
    "    all_facts = [sent for para in all_facts for sent in para]\n",
    "    return {\"full_context\":''.join(all_facts)}\n",
    "hotpot_qa = hotpot_qa.map(get_all_facts, batched=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f26aec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_ex1 = \"Were Scott Derrickson and Ed Wood of the same nationality?\"\n",
    "context_ex1 = 'Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer He lives in Los Angeles, California He is best known for directing horror films such as \"Sinister\", \"The Exorcism of Emily Rose\", and \"Deliver Us From Evil\", as well as the 2016 Marvel Cinematic Universe installment, \"Doctor Strange\"Tyler Bates is an American musician, music producer, and composer for films, television, and video games. Adam Collis is an American filmmaker and actor.Conrad Brooks is an American actor.Edward Davis Wood Jr. (October 10, 1924 â€“ December 10, 1978) was an American filmmaker, actor, writer, producer, and director.'\n",
    "answer_ex1 = \"Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer. \\nEdward Davis Wood Jr. (October 10, 1924 â€“ December 10, 1978) was an American filmmaker, actor, writer, producer, and director.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "5f0b7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "Context_relevency = \"\"\"\n",
    "Task: Candidate sentence extraction.\n",
    "Given the question and context, extract minimum number of sentences from context required to answer the question. If the context do not contain information required to answer the question return \"No candidate sentences found\".\n",
    "\n",
    "question: Which equation is known as worlds most famous equation?\n",
    "context:\\nAlbert Einstein (14 March 1879 â€“ 18 April 1955) was a German-born theoretical physicist,[5] widely ranked among the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century.\n",
    "His massâ€“energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world's most famous equation\".\n",
    "sentences:His massâ€“energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world's most famous equation\".\n",
    "\n",
    "question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
    "context :\\nScott Derrickson (born July 16, 1966) is an American director, screenwriter and producer He lives in Los Angeles, California He is best known for directing horror films such as \"Sinister\", \"The Exorcism of Emily Rose\", and \"Deliver Us From Evil\", as well as the 2016 Marvel Cinematic Universe installment, \"Doctor Strange\"Tyler Bates is an American musician, music producer, and composer for films, television, and video games. Adam Collis is an American filmmaker and actor.Conrad Brooks is an American actor.Edward Davis Wood Jr. (October 10, 1924 â€“ December 10, 1978) was an American filmmaker, actor, writer, producer, and director.\n",
    "Now given a question and context, extract the minimum number of sentences from the given context required to answer the question completely. \n",
    "sentences:Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer. Edward Davis Wood Jr. (October 10, 1924 â€“ December 10, 1978) was an American filmmaker, actor, writer, producer, and director.\n",
    "\n",
    "question:{}\n",
    "context:\\n{}\n",
    "sentences:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f649eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=15\n",
    "q,c = hotpot_qa[i]['question'],hotpot_qa[i]['full_context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b9f5e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"A black hole is a region of spacetime where gravity is so strong that nothing, including light or other electromagnetic waves, has enough energy to escape it.[2] The theory of general relativity predicts that a sufficiently compact mass can deform spacetime to form a black hole.[3][4] The boundary of no escape is called the event horizon. Although it has a great effect on the fate and circumstances of an object crossing it, it has no locally detectable features according to general relativity.[5] In many ways, a black hole acts like an ideal black body, as it reflects no light\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "b711de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"what is general relativity?\"\n",
    "n=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "11a83f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n",
    "\n",
    "p_wiki = wiki_wiki.page(\"Black hole\")\n",
    "\n",
    "def get_page_section(page, section):\n",
    "    all_text = \"\"\n",
    "    p_wiki = wiki_wiki.page(page)\n",
    "    sections = p_wiki.sections_by_title(section)\n",
    "    for s in sections:\n",
    "        all_text += s.full_text()\n",
    "    return all_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "2755ba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from itertools import combinations\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/stsb-TinyBERT-L-4\")\n",
    "\n",
    "        \n",
    "def sent_tokenize(sent):\n",
    "    return [s[:-1] if  s.endswith('.') else s for s in sent.strip().split('. ')]\n",
    "\n",
    "class SentenceAgreement:\n",
    "    \n",
    "    def __init__(self, scoring=\"bert_score\"):\n",
    "        \n",
    "        self.scoring = scoring\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def bert_score(para1, para2):\n",
    "        \n",
    "        sentences1, sentences2 = sent_tokenize(para1), sent_tokenize(para2)\n",
    "        scores = cross_encoder.predict(list(itertools.product(sentences1, sentences2)))\n",
    "        scores = scores.reshape(len(sentences1), len(sentences2))\n",
    "        return scores.max(axis=1).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def jaccard_score(para1, para2):\n",
    "        \n",
    "        sentences1, sentences2 = sent_tokenize(para1), sent_tokenize(para2)\n",
    "        intersect = len(np.intersect1d(sentences1, sentences2))\n",
    "        union = len(np.union1d(sentences1, sentences2))\n",
    "        return intersect/union\n",
    "    \n",
    "    def evaluate(self,answers:List[List[str]]):\n",
    "        \n",
    "        \"\"\"\n",
    "        eval nC2 combinations\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        groups = combinations(answers,2)\n",
    "        for group in groups:\n",
    "            if self.scoring == \"jaccard\":\n",
    "                score = self.jaccard_score(*group)\n",
    "            elif self.scoring == \"bert_score\":\n",
    "                score = self.bert_score(*group)\n",
    "            scores.append(score)\n",
    "        return np.mean(scores)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "8d3aa09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextRelevacy:\n",
    "    \n",
    "    def __init__(self, strictness = 2, agreement_metric=\"bert_score\"):\n",
    "        \n",
    "        self.strictness = strictness\n",
    "        self.sent_agreement = SentenceAgreement(agreement_metric)\n",
    "        \n",
    "    def score(self,question,context):\n",
    "        scores = []\n",
    "        outputs = llm(Context_relevency.format(q,c),n=self.strictness,temperature=1)\n",
    "        outputs = [outputs['choices'][i]['text'].strip() for i in range(self.strictness)]\n",
    "        context_sents = sent_tokenize(context)\n",
    "        for output in outputs:\n",
    "            indices = [context.find(sent) for sent in sent_tokenize(output) if context.find(sent)!=-1]\n",
    "            scores.append(len(indices)/len(context_sents))\n",
    "        \n",
    "        if self.strictness > 1:\n",
    "            agr_score = self.sent_agreement.evaluate(outputs)\n",
    "        else:\n",
    "            agr_score =1 \n",
    "        return agr_score * np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "6985c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = get_page_section(\"HIV/AIDS\", \"Prevention\")\n",
    "c = ' '.join(c.split(' ')[:500])\n",
    "q = \"When was the first HIV case detected?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "689e1aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm([Context_relevency.format(q,c), Context_relevency.format(\"How to prevent AIDS?\",c)],n=n,temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "5b2bc77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "cf822217",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [510]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "46dd037b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Consistent condom use reduces the risk of HIV transmission by approximately 80% over the long term. Pre-exposure prophylaxis (PrEP) with a daily dose of the medications tenofovir, with or without emtricitabine, is effective in people at high risk including men who have sex with men, couples where one is HIV-positive, and young heterosexuals in Africa. A course of antiretrovirals administered within 48 to 72 hours after exposure to HIV-positive blood or genital secretions is referred to as post-exposure prophylaxis (PEP).'"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['choices'][3]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "692aa0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "[2, 3]\n"
     ]
    }
   ],
   "source": [
    "prev=0\n",
    "for i in range(2,5,2):\n",
    "    print([idx for idx in range(prev,i)])\n",
    "    prev = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "c48decf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, ['No candidate sentences found.', 'No candidate sentences found.'])"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ContextRelevacy(2).score(q, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "7fbe8a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((Context_relevency.format(q,c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "d4d94e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C o n s i s t e n t   c o n d o m   u s e   r e d u c e s   t h e   r i s k   o f   H I V   t r a n s m i s s i o n   b y   a p p r o x i m a t e l y   8 0 %   o v e r   t h e   l o n g   t e r m ',\n",
       " '  P r e - e x p o s u r e   p r o p h y l a x i s   ( P r E P )   w i t h   a   d a i l y   d o s e   o f   t h e   m e d i c a t i o n s   t e n o f o v i r ,   w i t h   o r   w i t h o u t   e m t r i c i t a b i n e ,   i s   e f f e c t i v e   i n   p e o p l e   a t   h i g h   r i s k   i n c l u d i n g   m e n   w h o   h a v e   s e x   w i t h   m e n ,   c o u p l e s   w h e r e   o n e   i s   H I V - p o s i t i v e ,   a n d   y o u n g   h e t e r o s e x u a l s   i n   A f r i c a ',\n",
       " '  A   c o u r s e   o f   a n t i r e t r o v i r a l s   a d m i n i s t e r e d   w i t h i n   4 8   t o   7 2   h o u r s   a f t e r   e x p o s u r e   t o   H I V - p o s i t i v e   b l o o d   o r   g e n i t a l   s e c r e t i o n s   i s   r e f e r r e d   t o   a s   p o s t - e x p o s u r e   p r o p h y l a x i s   ( P E P ) ']"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(\" \".join(paras[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "9dc590bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88868356"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentenceAgreement().evaluate(paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "2e5979b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.047619047619047616"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentenceAgreement(\"jaccard_score\").jaccard_score(paras[0],paras[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "705da870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c.split('. '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "d04330ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "9e7b8030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Consistent condom use reduces the risk of HIV transmission by approximately 80% over the long term. Pre-exposure prophylaxis (PrEP) with a daily dose of the medications tenofovir, with or without emtricitabine, is effective in people at high risk including men who have sex with men, couples where one is HIV-positive, and young heterosexuals in Africa. A course of antiretrovirals administered within 48 to 72 hours after exposure to HIV-positive blood or genital secretions is referred to as post-exposure prophylaxis (PEP).'"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paras[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "1bc7ce6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26, 2286, 3055]"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c.find(s) for s in sent_tokenize(paras[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "ffecc23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Consistent condom use reduces the risk of HIV transmission by approximately 80% over the long term',\n",
       " 'Pre-exposure prophylaxis (PrEP) with a daily dose of the medications tenofovir, with or without emtricitabine, is effective in people at high risk',\n",
       " 'A course of antiretrovirals administered within 48 to 72 hours after exposure to HIV-positive blood or genital secretions is referred to as post-exposure prophylaxis (PEP)']"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(paras[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "89c5d822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Prevention\\nSexual contact\\nConsistent condom use reduces the risk of HIV transmission by approximately 80% over the long term.',\n",
       " 'When condoms are used consistently by a couple in which one person is infected, the rate of HIV infection is less than 1% per year.',\n",
       " 'There is some evidence to suggest that female condoms may provide an equivalent level of protection.',\n",
       " 'Application of a vaginal gel containing tenofovir (a reverse transcriptase inhibitor) immediately before sex seems to reduce infection rates by approximately 40% among African women.',\n",
       " 'By contrast, use of the spermicide nonoxynol-9 may increase the risk of transmission due to its tendency to cause vaginal and rectal irritation.Circumcision in sub-Saharan Africa \"reduces the acquisition of HIV by heterosexual men by between 38% and 66% over 24 months\".',\n",
       " 'Owing to these studies, both the World Health Organization and UNAIDS recommended male circumcision in 2007 as a method of preventing female-to-male HIV transmission in areas with high rates of HIV.',\n",
       " 'However, whether it protects against male-to-female transmission is disputed, and whether it is of benefit in developed countries and among men who have sex with men is undetermined.Programs encouraging sexual abstinence do not appear to affect subsequent HIV risk.',\n",
       " 'Evidence of any benefit from peer education is equally poor.',\n",
       " 'Comprehensive sexual education provided at school may decrease high-risk behavior.',\n",
       " 'A substantial minority of young people continues to engage in high-risk practices despite knowing about HIV/AIDS, underestimating their own risk of becoming infected with HIV.',\n",
       " 'Voluntary counseling and testing people for HIV does not affect risky behavior in those who test negative but does increase condom use in those who test positive.',\n",
       " 'Enhanced family planning services appear to increase the likelihood of women with HIV using contraception, compared to basic services.',\n",
       " 'It is not known whether treating other sexually transmitted infections is effective in preventing HIV.\\n\\nPre-exposure\\nAntiretroviral treatment among people with HIV whose CD4 count â‰¤ 550 cells/ÂµL is a very effective way to prevent HIV infection of their partner (a strategy known as treatment as prevention, or TASP).',\n",
       " 'TASP is associated with a 10- to 20-fold reduction in transmission risk.',\n",
       " 'Pre-exposure prophylaxis (PrEP) with a daily dose of the medications tenofovir, with or without emtricitabine, is effective in people at high risk including men who have sex with men, couples where one is HIV-positive, and young heterosexuals in Africa.',\n",
       " 'It may also be effective in intravenous drug users, with a study finding a decrease in risk of 0.7 to 0.4 per 100 person years.',\n",
       " 'The USPSTF, in 2019, recommended PrEP in those who are at high risk.Universal precautions within the health care environment are believed to be effective in decreasing the risk of HIV.',\n",
       " 'Intravenous drug use is an important risk factor, and harm reduction strategies such as needle-exchange programs and opioid substitution therapy appear effective in decreasing this risk.\\n\\nPost-exposure\\nA course of antiretrovirals administered within 48 to 72 hours after exposure to HIV-positive blood or genital secretions is referred to as post-exposure prophylaxis (PEP).',\n",
       " 'The use of the single agent zidovudine reduces the risk of.']"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "eed5d95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10526315789473684"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2/19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "e30d88c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['History\\nDiscovery\\nThe first news story on the disease appeared on May 18, 1981, in the gay newspaper New York Native.',\n",
       " 'AIDS was first clinically reported on June 5, 1981, with five cases in the United States.',\n",
       " 'The initial cases were a cluster of injecting drug users and gay men with no known cause of impaired immunity who showed symptoms of Pneumocystis carinii pneumonia (PCP), a rare opportunistic infection that was known to occur in people with very compromised immune systems.',\n",
       " \"Soon thereafter, a large number of homosexual men developed a generally rare skin cancer called Kaposi's sarcoma (KS).\",\n",
       " 'Many more cases of PCP and KS emerged, alerting U.S.',\n",
       " 'Centers for Disease Control and Prevention (CDC) and a CDC task force was formed to monitor the outbreak.In the early days, the CDC did not have an official name for the disease, often referring to it by way of diseases associated with it, such as lymphadenopathy, the disease after which the discoverers of HIV originally named the virus.',\n",
       " \"They also used Kaposi's sarcoma and opportunistic infections, the name by which a task force had been set up in 1981.\",\n",
       " 'At one point the CDC referred to it as the \"4H disease\", as the syndrome seemed to affect heroin users, homosexuals, hemophiliacs, and Haitians.',\n",
       " 'The term GRID, which stood for gay-related immune deficiency, had also been coined.',\n",
       " 'However, after determining that AIDS was not isolated to the gay community, it was realized that the term GRID was misleading, and the term AIDS was introduced at a meeting in July 1982.',\n",
       " 'By September 1982 the CDC started referring to the disease as AIDS.In 1983, two separate research groups led by Robert Gallo and Luc Montagnier declared that a novel retrovirus may have been infecting people with AIDS, and published their findings in the same issue of the journal Science.',\n",
       " 'Gallo claimed a virus which his group had isolated from a person with AIDS was strikingly similar in shape to other human T-lymphotropic viruses (HTLVs) that his group had been the first to isolate.',\n",
       " \"Gallo's group called their newly isolated virus HTLV-III.\",\n",
       " \"At the same time, Montagnier's group isolated a virus from a person presenting with swelling of the lymph nodes of the neck and physical weakness, two characteristic symptoms of AIDS.\",\n",
       " \"Contradicting the report from Gallo's group, Montagnier and his colleagues showed that core proteins of this virus were immunologically different from those of HTLV-I.\",\n",
       " \"Montagnier's group named their isolated virus lymphadenopathy-associated virus (LAV).\",\n",
       " 'As these two viruses turned out to be the same, in 1986, LAV and HTLV-III were renamed HIV.\\n\\nOrigins\\nThe origin of HIV / AIDS and the circumstances that led to its emergence remain unsolved.Both HIV-1 and HIV-2 are believed to have originated in non-human primates in West-central Africa and were transferred to humans in the early 20th century.',\n",
       " 'HIV-1 appears to have originated in southern Cameroon through the evolution of SIV(cpz), a simian immunodeficiency virus (SIV) that infects wild chimpanzees (HIV-1 descends from the SIVcpz endemic in the chimpanzee subspecies Pan troglodytes.']"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "842c8093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The first news story on the disease appeared on May 18, 1981, in the gay newspaper New York Native.',\n",
       " 'AIDS was first clinically reported on June 5, 1981, with five cases in the United States.',\n",
       " 'HIV-1 appears to have originated in southern Cameroon through the evolution of SIV(cpz), a simian immunodeficiency virus (SIV) that infects wild chimpanzees.']"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(paras[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "b06c0ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Word1', ' word2', ' word3', '']"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Word1. word2. word3.\".split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "8ce7a110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05555555555555555"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "839b546b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31127658, 0.7124167 , 0.38770333, 0.36687687, 0.3265456 ,\n",
       "       0.97304463], dtype=float32)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "scores = cross_encoder.predict(list(itertools.product(paras[0].split('. '),paras[1].split('. '))))\n",
    "scores.reshape(len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "faf2ddd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe theory of general relativity predicts that a sufficiently compact mass can deform spacetime to form a black hole.'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b3582dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brown State Fishing Lake (sometimes also known as Brown State Fishing Lake And Wildlife Area) is a protected area in Brown County, Kansas in the United States.  As of the 2010 census, the county population was 9,984.'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotpot_qa[i]['answer_context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "997097ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer. \n",
      "Edward Davis Wood Jr. (October 10, 1924 â€“ December 10, 1978) was an American filmmaker, actor, writer, producer, and director.\n"
     ]
    }
   ],
   "source": [
    "print(answer_ex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "34d620a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "148c4dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"In 2020, the city had an estimated population density of 29,302.37 inhabitants per square mile (11,313.71/km2), rendering it the nation's most densely populated of all larger municipalities (those with more than 100,000 residents).\",\n",
       " \"The borough of Manhattan's 2017 population density of 72,918 inhabitants per square mile (28,154/km2) makes it the highest of any county in the United States.\"]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d9627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "ragas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
